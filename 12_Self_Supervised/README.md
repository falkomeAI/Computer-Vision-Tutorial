<div align="center">

# ğŸ”„ Self-Supervised Learning

### *Contrastive Learning, MAE & DINO*

| Level | Time | Prerequisites |
|:-----:|:----:|:-------------:|
| ğŸ”´ Advanced | 2.5 hours | Deep Learning, Vision Transformers |

</div>

---

**Navigation:** [â† Vision Transformers](../11_Vision_Transformers/) | [ğŸ  Home](../README.md) | [Video & Temporal â†’](../13_Video_Temporal/)

---

## ğŸ“– Table of Contents
- [Key Concepts](#-key-concepts)
- [Mathematical Foundations](#-mathematical-foundations)
- [Algorithms](#-algorithms)
- [Visual Overview](#-visual-overview)
- [Practice](#-practice)
- [Interview Q&A](#-interview-questions--answers)

---

## ğŸ¯ Key Concepts

| Method | Key Idea | Architecture |
|:-------|:---------|:-------------|
| **SimCLR** | Contrastive + strong augmentation | Dual encoder + projector |
| **MoCo** | Momentum encoder + queue | Encoder + momentum encoder |
| **BYOL** | No negatives, EMA target | Online + target network |
| **MAE** | Masked autoencoding | Encoder + decoder |
| **DINO** | Self-distillation | Student + teacher (EMA) |

---

## ğŸ¨ Visual Overview

<div align="center">
<img src="./svg_figs/ssl_methods.svg" alt="Self-Supervised Methods" width="100%"/>
</div>

---

## ğŸ”¢ Mathematical Foundations

### 1. Contrastive Loss (InfoNCE / NT-Xent)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INFONCE LOSS                                       â”‚
â”‚                                                     â”‚
â”‚  L = -log [ exp(sim(záµ¢, zâ±¼)/Ï„) ]                   â”‚
â”‚           [ â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€• ]                â”‚
â”‚           [ Î£â‚– exp(sim(záµ¢, zâ‚–)/Ï„) ]                â”‚
â”‚                                                     â”‚
â”‚  záµ¢, zâ±¼: positive pair (same image, diff augment)  â”‚
â”‚  zâ‚–: negatives (other images in batch)             â”‚
â”‚  Ï„: temperature (0.07-0.5 typical)                 â”‚
â”‚  sim: cosine similarity                            â”‚
â”‚                                                     â”‚
â”‚  NT-XENT (SimCLR):                                 â”‚
â”‚  L = Î£áµ¢ -log [ exp(záµ¢Â·zâ±¼/Ï„) / Î£â‚–â‰ áµ¢ exp(záµ¢Â·zâ‚–/Ï„) ] â”‚
â”‚                                                     â”‚
â”‚  Intuition: Pull positives together,               â”‚
â”‚             push negatives apart                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. BYOL Loss (No Negatives)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BYOL ARCHITECTURE                                  â”‚
â”‚                                                     â”‚
â”‚  Online: encoder fÎ¸ â†’ projector gÎ¸ â†’ predictor qÎ¸  â”‚
â”‚  Target: encoder fÎ¾ â†’ projector gÎ¾ (EMA of online) â”‚
â”‚                                                     â”‚
â”‚  LOSS (MSE after normalization):                   â”‚
â”‚                                                     â”‚
â”‚  L = 2 - 2 Â· <qÎ¸(zÎ¸), sg(zÎ¾)>                      â”‚
â”‚        â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•                      â”‚
â”‚        ||qÎ¸(zÎ¸)|| Â· ||sg(zÎ¾)||                     â”‚
â”‚                                                     â”‚
â”‚  where sg = stop gradient                          â”‚
â”‚                                                     â”‚
â”‚  EMA UPDATE:                                        â”‚
â”‚  Î¾ â† Ï„Î¾ + (1-Ï„)Î¸   (Ï„ = 0.996 â†’ 1.0)              â”‚
â”‚                                                     â”‚
â”‚  Why no collapse?                                   â”‚
â”‚  - Asymmetric architecture (predictor)             â”‚
â”‚  - EMA target (slowly moving)                      â”‚
â”‚  - BatchNorm plays role                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. Masked Autoencoder (MAE) Loss

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MASKING STRATEGY                                   â”‚
â”‚                                                     â”‚
â”‚  1. Divide image into patches (16Ã—16)              â”‚
â”‚  2. Randomly mask 75% of patches                   â”‚
â”‚  3. Encode ONLY visible patches                    â”‚
â”‚                                                     â”‚
â”‚  RECONSTRUCTION LOSS (MSE on pixels):              â”‚
â”‚                                                     â”‚
â”‚  L = (1/|M|) Î£áµ¢âˆˆM ||xáµ¢ - xÌ‚áµ¢||Â²                    â”‚
â”‚                                                     â”‚
â”‚  M = set of masked patches                         â”‚
â”‚  xáµ¢ = original patch pixels                        â”‚
â”‚  xÌ‚áµ¢ = reconstructed patch pixels                   â”‚
â”‚                                                     â”‚
â”‚  ALTERNATIVE: Predict normalized pixels            â”‚
â”‚  L = ||norm(x) - norm(xÌ‚)||Â²                       â”‚
â”‚                                                     â”‚
â”‚  Key insight: High masking ratio forces            â”‚
â”‚  learning meaningful representations               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. DINO Loss (Self-Distillation)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STUDENT-TEACHER SETUP                              â”‚
â”‚                                                     â”‚
â”‚  Student: gÎ¸â‚› (trained)                            â”‚
â”‚  Teacher: gÎ¸â‚œ (EMA of student)                     â”‚
â”‚                                                     â”‚
â”‚  MULTI-CROP STRATEGY:                              â”‚
â”‚  - 2 global views (224Ã—224)                        â”‚
â”‚  - 8 local views (96Ã—96)                          â”‚
â”‚  - Student sees all, teacher sees only global     â”‚
â”‚                                                     â”‚
â”‚  CENTERING (prevent collapse):                     â”‚
â”‚  teacher_out = teacher_out - center               â”‚
â”‚  center â† mÂ·center + (1-m)Â·mean(teacher_out)      â”‚
â”‚                                                     â”‚
â”‚  LOSS (cross-entropy with soft targets):           â”‚
â”‚                                                     â”‚
â”‚  L = -Î£áµ¥ Pâ‚œ(v) log Pâ‚›(v)                           â”‚
â”‚                                                     â”‚
â”‚  Pâ‚œ = softmax(teacher_out / Ï„â‚œ)                   â”‚
â”‚  Pâ‚› = softmax(student_out / Ï„â‚›)                   â”‚
â”‚  Ï„â‚œ < Ï„â‚› (sharper teacher targets)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Algorithms

### Algorithm 1: SimCLR Training

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FOR each batch of N images:                       â”‚
â”‚                                                     â”‚
â”‚  1. AUGMENT:                                        â”‚
â”‚     xÌƒâ‚, xÌƒâ‚‚ = Tâ‚(x), Tâ‚‚(x) for each image         â”‚
â”‚     (Crop, flip, color, blur, grayscale)          â”‚
â”‚                                                     â”‚
â”‚  2. ENCODE:                                         â”‚
â”‚     háµ¢ = f(xÌƒáµ¢)  (backbone)                        â”‚
â”‚     záµ¢ = g(háµ¢)  (projector MLP)                   â”‚
â”‚                                                     â”‚
â”‚  3. COMPUTE SIMILARITY:                             â”‚
â”‚     sáµ¢â±¼ = záµ¢áµ€zâ±¼ / (||záµ¢|| ||zâ±¼|| Ï„)              â”‚
â”‚                                                     â”‚
â”‚  4. LOSS:                                           â”‚
â”‚     L = -(1/2N) Î£áµ¢ [log exp(sáµ¢,áµ¢â‚Šâ‚™)/Î£â‚– exp(sáµ¢,â‚–)] â”‚
â”‚     for positive pair (i, i+N)                    â”‚
â”‚                                                     â”‚
â”‚  5. UPDATE encoder and projector                   â”‚
â”‚                                                     â”‚
â”‚  Large batch (4096+) is critical                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 2: MoCo Training

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INIT: Queue Q of size K (e.g., 65536)             â”‚
â”‚        Momentum encoder fâ‚– = copy(f)               â”‚
â”‚                                                     â”‚
â”‚  FOR each batch:                                    â”‚
â”‚                                                     â”‚
â”‚  1. ENCODE:                                         â”‚
â”‚     q = f(xâ‚)   (query encoder)                   â”‚
â”‚     k = fâ‚–(xâ‚‚)  (momentum encoder, no grad)       â”‚
â”‚                                                     â”‚
â”‚  2. POSITIVE: kâº = k from same image              â”‚
â”‚     NEGATIVES: all keys in queue Q                â”‚
â”‚                                                     â”‚
â”‚  3. LOSS:                                           â”‚
â”‚     L = -log [ exp(qÂ·kâº/Ï„) ]                      â”‚
â”‚              [ â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€• ]                   â”‚
â”‚              [ exp(qÂ·kâº/Ï„) + Î£ exp(qÂ·kâ¿/Ï„) ]      â”‚
â”‚                                                     â”‚
â”‚  4. UPDATE QUEUE:                                   â”‚
â”‚     Enqueue current keys                          â”‚
â”‚     Dequeue oldest keys                           â”‚
â”‚                                                     â”‚
â”‚  5. MOMENTUM UPDATE:                               â”‚
â”‚     fâ‚– â† mÂ·fâ‚– + (1-m)Â·f   (m=0.999)              â”‚
â”‚                                                     â”‚
â”‚  Advantage: Large negative set without big batch  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 3: MAE Pre-training

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FOR each image:                                    â”‚
â”‚                                                     â”‚
â”‚  1. PATCHIFY:                                       â”‚
â”‚     patches = split(image, 16Ã—16)  (N patches)    â”‚
â”‚                                                     â”‚
â”‚  2. RANDOM MASKING:                                 â”‚
â”‚     mask = random_sample(N, ratio=0.75)           â”‚
â”‚     visible = patches[~mask]                      â”‚
â”‚                                                     â”‚
â”‚  3. ENCODER (ViT):                                 â”‚
â”‚     - Add positional embeddings                   â”‚
â”‚     - Process ONLY visible patches                â”‚
â”‚     - Output: encoded visible tokens              â”‚
â”‚                                                     â”‚
â”‚  4. DECODER:                                        â”‚
â”‚     - Add mask tokens at masked positions         â”‚
â”‚     - Add positional embeddings                   â”‚
â”‚     - Light transformer decoder                   â”‚
â”‚     - Predict pixel values for masked patches     â”‚
â”‚                                                     â”‚
â”‚  5. LOSS:                                           â”‚
â”‚     L = MSE(pred_masked, true_masked)             â”‚
â”‚     Only on masked patches                        â”‚
â”‚                                                     â”‚
â”‚  Key: Encoder never sees mask tokens (efficient)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ““ Practice

See the Colab notebook for hands-on coding: [`colab_tutorial.ipynb`](./colab_tutorial.ipynb)

---

## â“ Interview Questions & Answers

<details>
<summary><b>Q1: Why does contrastive learning need large batch sizes?</b></summary>

**Reason:** More negatives â†’ better gradient signal

**SimCLR:** Needs 4096+ batch for good performance
- 2N-2 negatives per positive
- More negatives = harder task = better features

**MoCo solution:** Memory bank/queue of 65K negatives
- Decouples batch size from negative count

</details>

<details>
<summary><b>Q2: How does BYOL avoid collapse without negatives?</b></summary>

**Naive approach would collapse:** Everything maps to same point

**BYOL prevents this via:**
1. **Asymmetric architecture:** Predictor only on online network
2. **EMA target:** Teacher moves slowly, provides stable target
3. **BatchNorm:** Implicit contrastive effect (debated)

**Key insight:** Predictor must predict something the target doesn't have

</details>

<details>
<summary><b>Q3: Why does MAE use 75% masking ratio?</b></summary>

**High masking forces semantic understanding:**
- Can't just copy from neighbors
- Must understand global context
- Information-dense task

**Comparison:**
- BERT: 15% masking (language is discrete)
- MAE: 75% masking (images have spatial redundancy)

**Benefit:** Also makes training faster (process fewer patches)

</details>

<details>
<summary><b>Q4: What is the InfoNCE loss?</b></summary>

**Formula:**

L = -log [ exp(sim(záµ¢,zâ±¼)/Ï„) / Î£â‚– exp(sim(záµ¢,zâ‚–)/Ï„) ]

**Interpretation:**
- Numerator: positive pair similarity
- Denominator: sum over positives + negatives
- Maximize positive, minimize negatives

**Relation to mutual information:** Lower bound on I(x;z)

</details>

<details>
<summary><b>Q5: How does DINO create good features without labels?</b></summary>

**Self-distillation:**
1. Student learns from teacher (EMA of student)
2. Multi-crop: Teacher sees global, student sees local+global
3. Centering: Prevents mode collapse

**Emergent properties:**
- Attention maps highlight objects
- Features support k-NN classification
- No explicit supervision needed

</details>

<details>
<summary><b>Q6: Compare data augmentation requirements.</b></summary>

| Method | Augmentation |
|:-------|:-------------|
| SimCLR | Strong (crop, color, blur) - critical |
| MoCo | Moderate |
| BYOL | Strong (similar to SimCLR) |
| MAE | Minimal (just crop) |
| DINO | Multi-crop strategy |

**SimCLR insight:** Stronger augmentations = better invariances

</details>

---

## ğŸ“š Key Formulas Reference

| Formula | Description |
|:--------|:------------|
| L = -log[exp(sâº/Ï„)/Î£exp(s/Ï„)] | InfoNCE/NT-Xent |
| Î¾ â† Ï„Î¾ + (1-Ï„)Î¸ | EMA update |
| L = \|\|q(z) - sg(z')\|\|Â² | BYOL loss |
| L = Î£\|\|xáµ¢ - xÌ‚áµ¢\|\|Â² | MAE reconstruction |
| L = -Î£ Pâ‚œ log Pâ‚› | DINO cross-entropy |

---

<div align="center">

**[â† Vision Transformers](../11_Vision_Transformers/) | [ğŸ  Home](../README.md) | [Video & Temporal â†’](../13_Video_Temporal/)**

</div>
