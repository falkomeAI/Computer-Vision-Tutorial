<div align="center">

<br/>

<a href="../04_Low_Level_Processing/README.md"><img src="https://img.shields.io/badge/â—€__Processing-0f172a?style=for-the-badge&labelColor=1e293b" height="35"/></a>
&nbsp;&nbsp;&nbsp;&nbsp;
<a href="../README.md"><img src="https://img.shields.io/badge/ğŸ __HOME-FBBF24?style=for-the-badge&labelColor=0f172a" height="35"/></a>
&nbsp;&nbsp;&nbsp;&nbsp;
<a href="../06_Geometry_MultiView/README.md"><img src="https://img.shields.io/badge/Geometry__â–¶-0f172a?style=for-the-badge&labelColor=1e293b" height="35"/></a>

<br/><br/>

---

<br/>

# ğŸ¯ FEATURE DETECTION

### ğŸŒ™ *Finding What Matters*

<br/>

<img src="https://img.shields.io/badge/ğŸ“š__MODULE__05/20-FBBF24?style=for-the-badge&labelColor=0f172a" height="40"/>
&nbsp;&nbsp;
<img src="https://img.shields.io/badge/â±ï¸__2_HOURS-FBBF24?style=for-the-badge&labelColor=0f172a" height="40"/>
&nbsp;&nbsp;
<img src="https://img.shields.io/badge/ğŸ““__NOTEBOOK_READY-34D399?style=for-the-badge&labelColor=0f172a" height="40"/>

<br/><br/>

---

</div>

<br/>

## ğŸ“– Overview

> **Feature detection and description enable matching and tracking across images.** This module covers corner detection (Harris), scale-invariant features (SIFT), binary descriptors (ORB), and matching algorithmsâ€”essential for panorama stitching, 3D reconstruction, and SLAM.

<br/>

---

## ğŸ¯ What You'll Learn

<table>
<tr>
<td width="50%">

### ğŸ” **Keypoint Detection**
- Harris corner detection
- FAST keypoints
- Scale-space (SIFT)
- Blob detection (LoG/DoG)

</td>
<td width="50%">

### ğŸ“ **Descriptors**
- SIFT (128-D)
- ORB (256-bit binary)
- BRIEF
- HOG features

</td>
</tr>
<tr>
<td width="50%">

### ğŸ”— **Matching**
- Ratio test (Lowe)
- Hamming distance
- RANSAC verification
- Homography estimation

</td>
<td width="50%">

### ğŸ¯ **Applications**
- Panorama stitching
- Object tracking
- 3D reconstruction
- Visual SLAM

</td>
</tr>
</table>

<br/>

---

## ğŸ¯ Key Concepts

| Concept | Description | Use Case |
| :--- | :--- | :--- |
| **Corner** | Point with strong gradients in 2 directions | Tracking, matching |
| **Blob** | Region different from surroundings | Object detection |
| **Keypoint** | Interesting point with location, scale, orientation | Feature matching |
| **Descriptor** | Vector describing local patch around keypoint | Matching across images |
| **Feature Matching** | Finding corresponding keypoints between images | Panorama, 3D reconstruction |

---

## ğŸ¨ Visual Overview

<div align="center">
<img src="./svg_figs/feature_pipeline.svg" alt="Feature Pipeline" width="100%"/>
</div>

---

## ğŸ”¢ Mathematical Foundations

### 1. Harris Corner Detection

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STRUCTURE TENSOR (Second Moment Matrix)            â”‚
â”‚                                                     â”‚
â”‚       â”Œ                    â”                        â”‚
â”‚  M =  â”‚  Î£ IxÂ²    Î£ IxIy  â”‚  weighted by Gaussian   â”‚
â”‚       â”‚  Î£ IxIy   Î£ IyÂ²   â”‚                         â”‚
â”‚       â””                    â”˜                        â”‚
â”‚                                                     â”‚
â”‚  CORNER RESPONSE                                    â”‚
â”‚                                                     â”‚
â”‚  R = det(M) - kÂ·trace(M)Â²                           â”‚
â”‚  R = Î»â‚Î»â‚‚ - k(Î»â‚ + Î»â‚‚)Â²                             â”‚
â”‚                                                     â”‚
â”‚  k â‰ˆ 0.04 - 0.06 (empirical)                        â”‚
â”‚                                                     â”‚
â”‚  R > threshold â†’ CORNER                             â”‚
â”‚  R < 0 â†’ EDGE                                       â”‚
â”‚  |R| small â†’ FLAT                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Scale-Space for Blob Detection

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAPLACIAN OF GAUSSIAN (LoG)                        â”‚
â”‚                                                     â”‚
â”‚  LoG(x,y,Ïƒ) = âˆ‚Â²G/âˆ‚xÂ² + âˆ‚Â²G/âˆ‚yÂ²                     â”‚
â”‚                                                     â”‚
â”‚  Scale-normalized: ÏƒÂ² Â· LoG                         â”‚
â”‚                                                     â”‚
â”‚  DIFFERENCE OF GAUSSIAN (DoG) - Approximation       â”‚
â”‚                                                     â”‚
â”‚  DoG â‰ˆ (k-1)ÏƒÂ² âˆ‡Â²G                                  â”‚
â”‚  DoG(x,y,Ïƒ) = G(x,y,kÏƒ) - G(x,y,Ïƒ)                  â”‚
â”‚                                                     â”‚
â”‚  Used in SIFT: k = âˆš2                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. SIFT Descriptor

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SIFT DESCRIPTOR (128-D)                            â”‚
â”‚                                                     â”‚
â”‚  1. Take 16Ã—16 patch around keypoint                â”‚
â”‚  2. Divide into 4Ã—4 grid of cells                   â”‚
â”‚  3. Compute 8-bin gradient histogram per cell       â”‚
â”‚  4. Concatenate: 4Ã—4Ã—8 = 128 dimensions             â”‚
â”‚  5. Normalize to unit length                        â”‚
â”‚                                                     â”‚
â”‚  Gradient magnitude: m = âˆš(LxÂ² + LyÂ²)               â”‚
â”‚  Gradient orientation: Î¸ = atan2(Ly, Lx)            â”‚
â”‚                                                     â”‚
â”‚  Properties:                                        â”‚
â”‚  - Scale invariant (normalized to keypoint scale)   â”‚
â”‚  - Rotation invariant (aligned to dominant orient)  â”‚
â”‚  - Illumination robust (normalized descriptor)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. ORB (Oriented FAST and Rotated BRIEF)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FAST Keypoint Detection                            â”‚
â”‚                                                     â”‚
â”‚  - Check 16 pixels on circle of radius 3            â”‚
â”‚  - If N contiguous pixels brighter/darker than      â”‚
â”‚    center by threshold â†’ corner                     â”‚
â”‚  - Very fast: uses decision tree                    â”‚
â”‚                                                     â”‚
â”‚  BRIEF Descriptor (Binary)                          â”‚
â”‚                                                     â”‚
â”‚  - 256 pairs of pixel locations                     â”‚
â”‚  - Compare intensities: Ï„(p,q) = 1 if I(p) < I(q)   â”‚
â”‚  - Result: 256-bit binary string                    â”‚
â”‚                                                     â”‚
â”‚  ORB adds:                                          â”‚
â”‚  - Orientation from intensity centroid              â”‚
â”‚  - Steered BRIEF for rotation invariance            â”‚
â”‚                                                     â”‚
â”‚  Matching: Hamming distance (XOR + popcount)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5. HOG (Histogram of Oriented Gradients)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HOG FEATURE COMPUTATION                            â”‚
â”‚                                                     â”‚
â”‚  1. Divide image into cells (8Ã—8 pixels)            â”‚
â”‚  2. Compute gradient magnitude & orientation        â”‚
â”‚  3. Create 9-bin histogram per cell                 â”‚
â”‚  4. Group cells into blocks (2Ã—2 cells)             â”‚
â”‚  5. L2-normalize each block                         â”‚
â”‚                                                     â”‚
â”‚  For 64Ã—128 detection window:                       â”‚
â”‚  - 8Ã—16 cells                                       â”‚
â”‚  - 7Ã—15 blocks (overlapping)                        â”‚
â”‚  - 7Ã—15Ã—4Ã—9 = 3780 dimensions                       â”‚
â”‚                                                     â”‚
â”‚  Bin interpolation:                                 â”‚
â”‚  - Trilinear: spatial (x,y) + orientation (Î¸)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Algorithms

### Algorithm 1: Harris Corner Detection

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Image I, k, threshold                       â”‚
â”‚  OUTPUT: Corner locations                           â”‚
â”‚                                                     â”‚
â”‚  1. Compute gradients: Ix = âˆ‚I/âˆ‚x, Iy = âˆ‚I/âˆ‚y       â”‚
â”‚  2. Compute products: IxÂ², IyÂ², IxIy                â”‚
â”‚  3. Apply Gaussian window w to each product         â”‚
â”‚  4. For each pixel (x,y):                           â”‚
â”‚     a. M = [Î£wÂ·IxÂ²  Î£wÂ·IxIy]                        â”‚
â”‚            [Î£wÂ·IxIy Î£wÂ·IyÂ² ]                        â”‚
â”‚     b. R = det(M) - kÂ·trace(M)Â²                     â”‚
â”‚  5. Non-maximum suppression on R                    â”‚
â”‚  6. Return pixels where R > threshold               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 2: SIFT Keypoint Detection

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Image I                                     â”‚
â”‚  OUTPUT: Keypoints with (x, y, scale, orientation)  â”‚
â”‚                                                     â”‚
â”‚  1. BUILD SCALE SPACE:                              â”‚
â”‚     - Multiple octaves (each half resolution)       â”‚
â”‚     - 5 scales per octave (Ïƒ, kÏƒ, kÂ²Ïƒ, ...)         â”‚
â”‚                                                     â”‚
â”‚  2. COMPUTE DoG:                                    â”‚
â”‚     - DoG = G(kÏƒ) - G(Ïƒ) between adjacent scales    â”‚
â”‚                                                     â”‚
â”‚  3. FIND EXTREMA:                                   â”‚
â”‚     - Compare each pixel to 26 neighbors            â”‚
â”‚       (8 spatial + 9 above + 9 below)               â”‚
â”‚                                                     â”‚
â”‚  4. REFINE LOCATION:                                â”‚
â”‚     - Sub-pixel via Taylor expansion                â”‚
â”‚     - Reject low contrast & edge responses          â”‚
â”‚                                                     â”‚
â”‚  5. ASSIGN ORIENTATION:                             â”‚
â”‚     - 36-bin histogram in local region              â”‚
â”‚     - Dominant peak(s) â†’ keypoint orientation(s)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 3: Feature Matching with Ratio Test

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Descriptors D1, D2                          â”‚
â”‚  OUTPUT: Matched pairs                              â”‚
â”‚                                                     â”‚
â”‚  FOR each descriptor d1 in D1:                      â”‚
â”‚    1. Compute distance to all d2 in D2              â”‚
â”‚    2. Find nearest (dist1) and 2nd nearest (dist2)  â”‚
â”‚    3. IF dist1/dist2 < ratio (0.75):                â”‚
â”‚         Accept match (d1, nearest d2)               â”‚
â”‚       ELSE:                                         â”‚
â”‚         Reject as ambiguous                         â”‚
â”‚                                                     â”‚
â”‚  For binary descriptors (ORB):                      â”‚
â”‚    - Distance = Hamming = popcount(d1 XOR d2)       â”‚
â”‚                                                     â”‚
â”‚  For float descriptors (SIFT):                      â”‚
â”‚    - Distance = L2 = ||d1 - d2||                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 4: RANSAC for Geometric Verification

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Matched points, threshold, iterations       â”‚
â”‚  OUTPUT: Homography H, inlier mask                  â”‚
â”‚                                                     â”‚
â”‚  best_H = None                                      â”‚
â”‚  best_inliers = 0                                   â”‚
â”‚                                                     â”‚
â”‚  FOR i = 1 to iterations:                           â”‚
â”‚    1. Random sample 4 point pairs                   â”‚
â”‚    2. Compute H from 4 correspondences (DLT)        â”‚
â”‚    3. FOR each match:                               â”‚
â”‚         error = ||p2 - HÂ·p1||                       â”‚
â”‚         IF error < threshold: count as inlier       â”‚
â”‚    4. IF inliers > best_inliers:                    â”‚
â”‚         best_H = H                                  â”‚
â”‚         best_inliers = inliers                      â”‚
â”‚                                                     â”‚
â”‚  Refine best_H using all inliers                    â”‚
â”‚  RETURN best_H, inlier_mask                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

---

## ğŸ¨ Visual Diagrams

<div align="center">
<img src="./svg_figs/sift_pipeline.svg" alt="SIFT Pipeline" width="90%"/>
</div>

<br/>

---

## âš ï¸ Common Pitfalls

| âŒ Pitfall | âœ… Solution |
| --- | --- |
| Not using ratio test | Always use ratio test (0.75) to reject ambiguous matches |
| Wrong distance metric | Use Hamming for binary (ORB), L2 for float (SIFT) |
| Insufficient RANSAC iterations | Compute N = log(1-p)/log(1-w^n) for confidence |
| Ignoring scale/rotation | Use scale/rotation invariant descriptors (SIFT, ORB) |
| Too many keypoints | Use non-maximum suppression and response thresholding |

<br/>

---

## ğŸ› ï¸ Mini Projects

<details>
<summary><b>Project 1: Panorama Stitching</b></summary>

- Capture overlapping images
- Detect keypoints (SIFT/ORB)
- Match features with ratio test
- Estimate homography with RANSAC
- Warp and blend images
- Create seamless panorama

</details>

<details>
<summary><b>Project 2: Feature Matching Visualization</b></summary>

- Implement Harris corner detection
- Extract SIFT descriptors
- Match features between image pairs
- Visualize matches with lines
- Compare SIFT vs ORB matching quality
- Measure matching accuracy

</details>

<details>
<summary><b>Project 3: Object Tracking</b></summary>

- Detect keypoints in first frame
- Track across video using Lucas-Kanade
- Handle occlusion and re-detection
- Compare with feature matching approach
- Evaluate tracking accuracy

</details>

<br/>

---

## â“ Interview Questions & Answers

<details>
<summary><b>Q1: SIFT vs ORB - when to use which?</b></summary>

| SIFT | ORB |
| :--- | :--- |
| 128D float descriptor | 256-bit binary |
| Scale + rotation invariant | Rotation invariant only |
| Slower (~10 ms/image) | ~100x faster (~0.1 ms) |
| More accurate matching | Good enough for real-time |
| Patented (was, now free) | Free from start |

**Use SIFT:** When accuracy matters (3D reconstruction)
**Use ORB:** Real-time applications (AR, SLAM)

</details>

<details>
<summary><b>Q2: How does Harris corner detection work?</b></summary>

**Steps:**
1. Compute image gradients Ix, Iy (Sobel)
2. Build structure tensor M (second moment matrix)
3. Compute corner response R = det(M) - kÂ·trace(M)Â²
4. Non-maximum suppression
5. Threshold to get corners

**Intuition:**
- **R > 0 (large):** Corner (Î»â‚, Î»â‚‚ both large)
- **R < 0:** Edge (one Î» large, one small)
- **R â‰ˆ 0:** Flat region (both Î» small)

</details>

<details>
<summary><b>Q3: What is the ratio test in feature matching?</b></summary>

**Lowe's Ratio Test:**
- Find best match (distance d1) and second-best match (d2)
- Accept match if d1/d2 < 0.75
- Rejects ambiguous matches where multiple features are similar

**Why it works:** Good matches have unique nearest neighbor; bad matches have multiple similar candidates.

</details>

<details>
<summary><b>Q4: How does RANSAC work?</b></summary>

**Algorithm:**
1. **Sample:** Pick minimal set (4 for homography)
2. **Fit:** Compute model from sample
3. **Score:** Count inliers (points fitting model within threshold)
4. **Repeat:** N iterations, keep best model
5. **Refine:** Re-estimate using all inliers

**Key parameters:**
- N iterations: `log(1-p) / log(1-wâ¿)` where p=success prob, w=inlier ratio, n=sample size
- Threshold: typically 3-5 pixels for geometric models

</details>

<details>
<summary><b>Q5: Why is HOG good for pedestrian detection?</b></summary>

**Reasons:**
1. **Captures shape:** Gradients encode edges and contours
2. **Robust to illumination:** Gradient-based, block normalization
3. **Local + global:** Cells capture local, blocks capture spatial arrangement
4. **Proven:** Dalal & Triggs showed state-of-art results on INRIA dataset

**Limitations:** Fixed window size, sensitive to occlusion, replaced by CNNs

</details>

<details>
<summary><b>Q6: What is scale-space and why is it important?</b></summary>

**Scale-space:** Family of smoothed images at multiple scales (Gaussian pyramid)

**Importance:**
- Objects appear at different sizes
- Features should be detected at their natural scale
- SIFT finds keypoints as extrema across scale

**Mathematical basis:** Gaussian is the only kernel satisfying scale-space axioms (no spurious details at coarser scales)

</details>

<details>
<summary><b>Q7: How is rotation invariance achieved in SIFT?</b></summary>

**Steps:**
1. Compute gradient orientations in region around keypoint
2. Build 36-bin orientation histogram
3. Find dominant orientation (peak)
4. Assign orientation to keypoint
5. Rotate descriptor coordinate system to canonical orientation

**Result:** Same feature produces same descriptor regardless of image rotation

</details>

---

## ğŸ“š Resources

**Papers:**
- [SIFT (2004)](https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf) - Lowe
- [ORB (2011)](https://ieeexplore.ieee.org/document/6126544) - Rublee et al.
- [Harris Corner (1988)](https://www.cs.cmu.edu/~16385/s17/Slides/6.1_Harris_Corner_Detector.pdf)

**Videos:**
- [SIFT Explained](https://www.youtube.com/watch?v=NPcMS49V5hg)
- [Feature Matching Tutorial](https://docs.opencv.org/master/dc/dc3/tutorial_py_matcher.html)

<br/>

---

## ğŸ“š Key Formulas Reference

| Formula | Description |
| :--- | :--- |
| R = det(M) - kÂ·trace(M)Â² | Harris corner response |
| DoG â‰ˆ G(kÏƒ) - G(Ïƒ) | Difference of Gaussian |
| m = âˆš(IxÂ² + IyÂ²) | Gradient magnitude |
| Î¸ = atan2(Iy, Ix) | Gradient orientation |
| d = popcount(d1 XOR d2) | Hamming distance |


---

<br/>

<div align="center">

## ğŸ““ PRACTICE

<br/>

### ğŸš€ Click to Open Directly in Google Colab

<br/>

<a href="https://colab.research.google.com/github/USERNAME/computer_vision_complete/blob/main/05_Features_Detection/colab_tutorial.ipynb">
<img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" height="50"/>
</a>

<br/><br/>

> âš ï¸ **First time?** Push this repo to GitHub, then replace `USERNAME` in the link above with your GitHub username.

<br/>

**Or manually:** [ğŸ“¥ Download](./colab_tutorial.ipynb) â†’ [ğŸŒ Colab](https://colab.research.google.com) â†’ Upload

</div>

<br/>




---

<br/>

<div align="center">

| | | |
|:---|:---:|---:|
| **[â—€ Processing](../04_Low_Level_Processing/README.md)** | **[ğŸ  HOME](../README.md)** | **[Geometry â–¶](../06_Geometry_MultiView/README.md)** |

<br/>

---

ğŸŒ™ Part of **[Computer Vision Complete](../README.md)** Â· Made with â¤ï¸

<br/>

</div>
