<div align="center">

<br/>

<a href="../15_Generative_Vision/README.md"><img src="https://img.shields.io/badge/â—€__Generative-0f172a?style=for-the-badge&labelColor=1e293b" height="35"/></a>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="../README.md"><img src="https://img.shields.io/badge/ğŸ __HOME-34D399?style=for-the-badge&labelColor=0f172a" height="35"/></a>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="../17_Computational_Photography/README.md"><img src="https://img.shields.io/badge/Photo__â–¶-0f172a?style=for-the-badge&labelColor=1e293b" height="35"/></a>

<br/><br/>

---

<br/>

# ğŸ’¬ VISION-LANGUAGE

### ğŸŒ™ *Images Meet Words*

<br/>

<img src="https://img.shields.io/badge/ğŸ“š__MODULE__16/20-34D399?style=for-the-badge&labelColor=0f172a" height="40"/>
&nbsp;&nbsp;
<img src="https://img.shields.io/badge/â±ï¸__2_HOURS-FBBF24?style=for-the-badge&labelColor=0f172a" height="40"/>
&nbsp;&nbsp;
<img src="https://img.shields.io/badge/ğŸ““__NOTEBOOK_READY-34D399?style=for-the-badge&labelColor=0f172a" height="40"/>

<br/><br/>

---

</div>

<br/>

## ğŸ¯ Key Concepts

| Model | Task | Architecture | Training |
| :--- | :--- | :--- | :--- |
| **CLIP** | Zero-shot classification | Dual encoder | Contrastive |
| **BLIP** | Captioning, VQA | Encoder-decoder | Unified |
| **Flamingo** | Few-shot multimodal | Cross-attention | Interleaved |
| **LLaVA** | Visual chat | Vision + LLM | Instruction tuning |
| **GPT-4V** | General vision | Unified | Massive scale |

---

## ğŸ¨ Visual Overview

<div align="center">
<img src="./svg_figs/clip_architecture.svg" alt="CLIP Architecture" width="100%"/>
</div>

---

## ğŸ”¢ Mathematical Foundations

### 1. CLIP - Contrastive Language-Image Pre-training

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ARCHITECTURE                                       â”‚
â”‚                                                     â”‚
â”‚  Image Encoder: f_img(I) â†’ z_img âˆˆ â„áµˆ               â”‚
â”‚  Text Encoder:  f_txt(T) â†’ z_txt âˆˆ â„áµˆ               â”‚
â”‚                                                     â”‚
â”‚  CONTRASTIVE LOSS (InfoNCE)                         â”‚
â”‚                                                     â”‚
â”‚  L = -log[exp(sim(I,T)/Ï„) / Î£â±¼exp(sim(I,Tâ±¼)/Ï„)]     â”‚
â”‚                                                     â”‚
â”‚  Where:                                             â”‚
â”‚    sim(I,T) = z_imgáµ€ z_txt / (||z_img|| ||z_txt||)  â”‚
â”‚    Ï„ = temperature (learned or fixed ~0.07)         â”‚
â”‚                                                     â”‚
â”‚  Symmetrized: L = (L_imgâ†’txt + L_txtâ†’img) / 2       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Zero-Shot Classification

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INFERENCE (No training on target classes!)         â”‚
â”‚                                                     â”‚
â”‚  1. Encode image: z_img = f_img(I)                  â”‚
â”‚                                                     â”‚
â”‚  2. Create text prompts: "a photo of a {class}"     â”‚
â”‚     Tâ‚ = "a photo of a cat"                         â”‚
â”‚     Tâ‚‚ = "a photo of a dog"                         â”‚
â”‚     ...                                             â”‚
â”‚                                                     â”‚
â”‚  3. Encode texts: z_tâ‚– = f_txt(Tâ‚–)                  â”‚
â”‚                                                     â”‚
â”‚  4. Compute similarities:                           â”‚
â”‚     sâ‚– = cosine(z_img, z_tâ‚–)                        â”‚
â”‚                                                     â”‚
â”‚  5. Predict: Å· = argmax softmax(s/Ï„)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. Image Captioning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ENCODER-DECODER ARCHITECTURE                       â”‚
â”‚                                                     â”‚
â”‚  Encoder: Extract visual features                   â”‚
â”‚    V = CNN(I) or ViT(I)  â†’ [vâ‚, ..., vâ‚™]            â”‚
â”‚                                                     â”‚
â”‚  Decoder: Generate caption autoregressively         â”‚
â”‚    P(wâ‚œ|wâ‚:â‚œâ‚‹â‚, V) = Decoder(wâ‚:â‚œâ‚‹â‚, V)             â”‚
â”‚                                                     â”‚
â”‚  Cross-attention:                                   â”‚
â”‚    Attn(Q, K, V) where Q=text, K,V=image            â”‚
â”‚                                                     â”‚
â”‚  Training loss (cross-entropy):                     â”‚
â”‚    L = -Î£â‚œ log P(wâ‚œ|wâ‚:â‚œâ‚‹â‚, V)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. Visual Question Answering (VQA)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Image I, Question Q                         â”‚
â”‚  OUTPUT: Answer A                                   â”‚
â”‚                                                     â”‚
â”‚  APPROACHES:                                        â”‚
â”‚                                                     â”‚
â”‚  1. Classification (closed-set):                    â”‚
â”‚     A = argmax P(a|I, Q)                            â”‚
â”‚     where a âˆˆ {yes, no, color, number, ...}         â”‚
â”‚                                                     â”‚
â”‚  2. Generation (open-set):                          â”‚
â”‚     A = Decoder(I, Q)                               â”‚
â”‚     Autoregressive text generation                  â”‚
â”‚                                                     â”‚
â”‚  FUSION METHODS:                                    â”‚
â”‚     - Early: concat features                        â”‚
â”‚     - Attention: cross-attention                    â”‚
â”‚     - Late: separate encoders, merge                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5. Cross-Modal Attention

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CROSS-ATTENTION MECHANISM                          â”‚
â”‚                                                     â”‚
â”‚  Q = W_q Ã— text_tokens                              â”‚
â”‚  K = W_k Ã— image_tokens                             â”‚
â”‚  V = W_v Ã— image_tokens                             â”‚
â”‚                                                     â”‚
â”‚  Attn(Q, K, V) = softmax(QKáµ€/âˆšd) V                  â”‚
â”‚                                                     â”‚
â”‚  Allows text to attend to relevant image regions    â”‚
â”‚                                                     â”‚
â”‚  GATED CROSS-ATTENTION (Flamingo):                  â”‚
â”‚  output = text + tanh(Î±) Ã— CrossAttn(text, image)   â”‚
â”‚  Î± = learnable gate (initialized to 0)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6. Visual Instruction Tuning (LLaVA)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ARCHITECTURE                                       â”‚
â”‚                                                     â”‚
â”‚  1. Visual Encoder: CLIP ViT â†’ image features       â”‚
â”‚  2. Projection: Linear layer to LLM dimension       â”‚
â”‚  3. LLM: Process [image tokens, text tokens]        â”‚
â”‚                                                     â”‚
â”‚  INPUT FORMAT:                                      â”‚
â”‚  "<image> [img_tokens] </image> User: {question}    â”‚
â”‚   Assistant: {answer}"                              â”‚
â”‚                                                     â”‚
â”‚  TRAINING STAGES:                                   â”‚
â”‚  Stage 1: Pretrain projection (frozen encoders)     â”‚
â”‚  Stage 2: Instruction tuning (unfreeze all)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Algorithms

### Algorithm 1: CLIP Training

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Image-text pairs (I, T)                     â”‚
â”‚  OUTPUT: Trained encoders f_img, f_txt              â”‚
â”‚                                                     â”‚
â”‚  FOR each mini-batch of N pairs:                    â”‚
â”‚    1. Encode: z_img = f_img(I), z_txt = f_txt(T)    â”‚
â”‚    2. L2 normalize embeddings                       â”‚
â”‚    3. Compute NxN similarity matrix:                â”‚
â”‚       S[i,j] = z_img[i]áµ€ z_txt[j] / Ï„               â”‚
â”‚    4. Labels: y = [0, 1, ..., N-1] (diagonal)       â”‚
â”‚    5. Loss (symmetric):                             â”‚
â”‚       L_i2t = CrossEntropy(S, y)    (rows)          â”‚
â”‚       L_t2i = CrossEntropy(Sáµ€, y)   (columns)       â”‚
â”‚       L = (L_i2t + L_t2i) / 2                       â”‚
â”‚    6. Backprop and update                           â”‚
â”‚                                                     â”‚
â”‚  Key: Diagonal elements are positive pairs          â”‚
â”‚       Off-diagonal are negative pairs               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 2: Beam Search for Captioning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Image features V, beam width k              â”‚
â”‚  OUTPUT: Best caption                               â”‚
â”‚                                                     â”‚
â”‚  1. Initialize: beams = {("<start>", 0)}            â”‚
â”‚  2. FOR t = 1 to max_length:                        â”‚
â”‚     3. candidates = []                              â”‚
â”‚     4. FOR each (seq, score) in beams:              â”‚
â”‚        5. Get P(w|seq, V) for all words w           â”‚
â”‚        6. FOR top-k words w:                        â”‚
â”‚           7. new_score = score + log P(w|seq, V)    â”‚
â”‚           8. candidates.add((seq + w, new_score))   â”‚
â”‚     9. beams = top-k candidates by score            â”‚
â”‚    10. IF all beams end with "<end>": break         â”‚
â”‚  11. RETURN beam with highest score                 â”‚
â”‚                                                     â”‚
â”‚  Beam search balances quality vs diversity          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 3: Zero-Shot CLIP Inference

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Image I, class names [câ‚, ..., câ‚™]          â”‚
â”‚  OUTPUT: Predicted class                            â”‚
â”‚                                                     â”‚
â”‚  1. Encode image: z_img = normalize(f_img(I))       â”‚
â”‚  2. FOR each class c:                               â”‚
â”‚     3. Create prompt: T = "a photo of a {c}"        â”‚
â”‚     4. Encode: z_txt = normalize(f_txt(T))          â”‚
â”‚  3. Compute similarities: s = z_img @ Z_txtáµ€        â”‚
â”‚  4. Probabilities: P = softmax(s / Ï„)               â”‚
â”‚  5. Predict: Å· = argmax(P)                          â”‚
â”‚                                                     â”‚
â”‚  PROMPT ENGINEERING (improves accuracy):            â”‚
â”‚  - "a photo of a {c}"                               â”‚
â”‚  - "a {c} in the wild"                              â”‚
â”‚  - Ensemble multiple prompts                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## â“ Interview Questions & Answers

<details>
<summary><b>Q1: How does CLIP enable zero-shot classification?</b></summary>

**Answer:**

**Training:** Learn joint image-text embedding space with 400M+ pairs

**Zero-shot inference:**
1. No training on target classes
2. Create text prompts: "a photo of a {class}"
3. Compare image embedding with text embeddings
4. Highest similarity = predicted class

**Why it works:**
- Learned general visual-semantic alignment
- Text describes visual concepts
- New classes = new text prompts

</details>

<details>
<summary><b>Q2: What is the difference between CLIP and BLIP?</b></summary>

**Answer:**

| Aspect | CLIP | BLIP |
| :--- | :--- | :--- |
| Training | Contrastive only | Contrastive + Generative |
| Architecture | Dual encoder | Unified encoder-decoder |
| Tasks | Classification, retrieval | + Captioning, VQA |
| Data cleaning | None | Bootstrap filtering |
| Generation | No | Yes |

**BLIP innovations:**
- Multimodal mixture of encoder-decoder
- Caption filtering to remove noisy web data
- Can both understand and generate

</details>

<details>
<summary><b>Q3: What is visual grounding?</b></summary>

**Answer:**

**Task:** Localize image regions described by text

**Input:** Image + text query ("the red car on the left")
**Output:** Bounding box or segmentation mask

**Methods:**
- Two-stage: Propose regions, match with text
- One-stage: Predict box directly from text
- Transformer: Cross-attention between text and image patches

**Applications:** Robotics, image editing, accessibility

</details>

<details>
<summary><b>Q4: How does cross-attention work in multimodal models?</b></summary>

**Answer:**

**Mechanism:**
- Query: from one modality (e.g., text)
- Key, Value: from other modality (e.g., image)
- Allows text to "look at" image regions

**Formula:** Attn(Q_text, K_image, V_image)

**Benefits:**
- Dynamic attention based on query
- Learns alignment between modalities
- No fixed pooling

**Gated version (Flamingo):**
- Gate controls how much vision influences text
- Initialized to 0 for stable training

</details>

<details>
<summary><b>Q5: What are the challenges in vision-language models?</b></summary>

**Answer:**

1. **Data:** Need aligned image-text pairs (noisy web data)
2. **Compute:** Very large models (billions of params)
3. **Grounding:** Text about visual details
4. **Hallucination:** Generating text not in image
5. **Compositionality:** "red cube on blue table" vs "blue cube on red table"
6. **Evaluation:** Hard to evaluate open-ended generation

</details>

<details>
<summary><b>Q6: Explain the InfoNCE loss.</b></summary>

**Answer:**

**Formula:**
L = -log[exp(sim(I,T)/Ï„) / Î£â±¼exp(sim(I,Tâ±¼)/Ï„)]

**Interpretation:**
- Numerator: Score of positive pair (matching I, T)
- Denominator: Scores of all pairs in batch
- Maximize ratio â†’ pull positives, push negatives

**Temperature Ï„:**
- Small Ï„: Sharper distribution, harder negatives
- Large Ï„: Softer distribution
- Typically 0.05-0.1 or learned

</details>

<details>
<summary><b>Q7: What is instruction tuning for VLMs?</b></summary>

**Answer:**

**Concept:** Fine-tune on (instruction, response) pairs

**Example:**
- Instruction: "Describe what's happening in this image"
- Response: "A dog is playing fetch in the park..."

**Benefits:**
- Follow user instructions
- Generalize to new tasks
- More controllable outputs

**LLaVA approach:**
- Stage 1: Align vision encoder to LLM
- Stage 2: Instruction tuning on 150K examples

</details>

<details>
<summary><b>Q8: How do you evaluate VQA models?</b></summary>

**Answer:**

**Metrics:**
- Accuracy: Exact match with ground truth
- Soft accuracy: min(#humans who gave that answer / 3, 1)
- F1: For open-ended answers

**VQA v2 dataset:** ~1M questions, ~200K images

**Challenges:**
- Multiple valid answers
- Language bias (can answer without image)
- Compositionality

**VQA-v2 innovation:** Complementary pairs to reduce bias

</details>

---

## ğŸ“š Key Formulas Reference

| Formula | Description |
| :--- | :--- |
| L = -log[exp(sâº/Ï„) / Î£exp(sâ±¼/Ï„)] | InfoNCE loss |
| sim(I,T) = (z_I Â· z_T) / (\|\|z_I\|\| \|\|z_T\|\|) | Cosine similarity |
| Attn(Q,K,V) = softmax(QKáµ€/âˆšd)V | Cross-attention |
| P(wâ‚œ\|wâ‚:â‚œâ‚‹â‚, V) | Autoregressive captioning |


---

<br/>

<div align="center">

## ğŸ““ PRACTICE

### ğŸš€ *Ready to code? Let's get started!*

<br/>

### ğŸš€ Open in Google Colab

<br/>

<p align="center">
  <a href="https://colab.research.google.com/github/falkomeAI/computer_vision_complete/blob/main/16_Vision_Language/colab_tutorial.ipynb">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" height="60"/>
  </a>
</p>

<br/>

<p align="center">
  <strong>âœ¨ Click the badge above to open this notebook directly in Google Colab!</strong>
</p>

<br/>


</div>

<br/>


---

<br/>

<div align="center">

| | | |
| :--- |:---:|---:|
| **[â—€ Generative](../15_Generative_Vision/README.md)** | **[ğŸ  HOME](../README.md)** | **[Photo â–¶](../17_Computational_Photography/README.md)** |

<br/>

---

ğŸŒ™ Part of **[Computer Vision Complete](../README.md)**

<p align="center">
  Made with â¤ï¸ by <a href="https://github.com/falkomeAI">falkomeAI</a>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/â­_Star_this_repo_if_helpful-60A5FA?style=for-the-badge&logo=github&logoColor=white" alt="Star"/>
</p>

<br/>

</div>
