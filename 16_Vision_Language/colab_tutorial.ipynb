{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udde3\ufe0f Vision-Language Models\n",
        "\n",
        "**Topics:** CLIP, Image-Text Similarity, Zero-Shot Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "!pip install torch torchvision transformers pillow -q\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "print('\u2705 Setup complete!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLIP-style Contrastive Loss\n",
        "def clip_loss(image_embeds, text_embeds, temperature=0.07):\n",
        "    \"\"\"Compute CLIP contrastive loss\"\"\"\n",
        "    # Normalize embeddings\n",
        "    image_embeds = F.normalize(image_embeds, dim=-1)\n",
        "    text_embeds = F.normalize(text_embeds, dim=-1)\n",
        "    \n",
        "    # Compute similarity matrix\n",
        "    logits = image_embeds @ text_embeds.T / temperature\n",
        "    \n",
        "    # Labels are diagonal (image_i matches text_i)\n",
        "    labels = torch.arange(len(image_embeds))\n",
        "    \n",
        "    # Cross-entropy both ways\n",
        "    loss_i2t = F.cross_entropy(logits, labels)\n",
        "    loss_t2i = F.cross_entropy(logits.T, labels)\n",
        "    \n",
        "    return (loss_i2t + loss_t2i) / 2\n",
        "\n",
        "# Demo\n",
        "batch_size, embed_dim = 4, 512\n",
        "img_emb = torch.randn(batch_size, embed_dim)\n",
        "txt_emb = torch.randn(batch_size, embed_dim)\n",
        "\n",
        "loss = clip_loss(img_emb, txt_emb)\n",
        "print(f'CLIP Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cosine Similarity Visualization\n",
        "# Simulate image and text embeddings\n",
        "np.random.seed(42)\n",
        "images = ['cat', 'dog', 'car', 'tree']\n",
        "texts = ['a photo of a cat', 'a photo of a dog', 'a photo of a car', 'a photo of a tree']\n",
        "\n",
        "# Create mock embeddings (in real CLIP, these come from encoders)\n",
        "img_emb = np.random.randn(4, 128)\n",
        "txt_emb = img_emb + np.random.randn(4, 128) * 0.3  # Similar but noisy\n",
        "\n",
        "# Normalize\n",
        "img_emb = img_emb / np.linalg.norm(img_emb, axis=1, keepdims=True)\n",
        "txt_emb = txt_emb / np.linalg.norm(txt_emb, axis=1, keepdims=True)\n",
        "\n",
        "# Compute similarity\n",
        "similarity = img_emb @ txt_emb.T\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.imshow(similarity, cmap='Blues')\n",
        "plt.xticks(range(4), texts, rotation=45, ha='right')\n",
        "plt.yticks(range(4), images)\n",
        "plt.colorbar(label='Cosine Similarity')\n",
        "plt.title('Image-Text Similarity Matrix')\n",
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        plt.text(j, i, f'{similarity[i,j]:.2f}', ha='center', va='center')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}