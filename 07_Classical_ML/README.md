<div align="center">

<br/>

<a href="../06_Geometry_MultiView/README.md"><img src="https://img.shields.io/badge/â—€__Geometry-0f172a?style=for-the-badge&labelColor=1e293b" height="35"/></a>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="../README.md"><img src="https://img.shields.io/badge/ğŸ __HOME-A78BFA?style=for-the-badge&labelColor=0f172a" height="35"/></a>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="../08_Neural_Networks/README.md"><img src="https://img.shields.io/badge/Neural Nets__â–¶-0f172a?style=for-the-badge&labelColor=1e293b" height="35"/></a>

<br/><br/>

---

<br/>

# ğŸ“Š CLASSICAL ML

### ğŸŒ™ *Before Deep Learning*

<br/>

<img src="https://img.shields.io/badge/ğŸ“š__MODULE__07/20-A78BFA?style=for-the-badge&labelColor=0f172a" height="40"/>
&nbsp;&nbsp;
<img src="https://img.shields.io/badge/â±ï¸__2_HOURS-FBBF24?style=for-the-badge&labelColor=0f172a" height="40"/>
&nbsp;&nbsp;
<img src="https://img.shields.io/badge/ğŸ““__NOTEBOOK_READY-34D399?style=for-the-badge&labelColor=0f172a" height="40"/>

<br/><br/>

---

</div>

<br/>

## ğŸ¯ Key Concepts

| Method | Type | Objective | Use Case |
| :--- | :--- | :--- | :--- |
| **PCA** | Unsupervised | max Var(Xw), \|\|w\|\|=1 | Dimensionality reduction |
| **SVM** | Supervised | min \|\|w\|\|Â² + CÎ£Î¾ | Classification |
| **K-Means** | Unsupervised | min Î£\|\|x-Î¼â‚–\|\|Â² | Clustering |
| **KNN** | Supervised | Majority vote of k neighbors | Classification |
| **Random Forest** | Supervised | Ensemble of trees | Classification/Regression |

---

## ğŸ¨ Visual Overview

<div align="center">
<img src="./svg_figs/svm_kernel.svg" alt="SVM Kernel" width="100%"/>
</div>

---

## ğŸ”¢ Mathematical Foundations

### 1. Principal Component Analysis (PCA)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GOAL: Find directions of maximum variance          â”‚
â”‚                                                     â”‚
â”‚  1. Center data: XÌ„ = X - mean(X)                   â”‚
â”‚                                                     â”‚
â”‚  2. Covariance matrix: C = (1/n)XÌ„áµ€XÌ„               â”‚
â”‚                                                     â”‚
â”‚  3. Eigendecomposition: C = VÎ›Váµ€                    â”‚
â”‚     - V: eigenvectors (principal components)        â”‚
â”‚     - Î›: eigenvalues (variance explained)           â”‚
â”‚                                                     â”‚
â”‚  4. Project: X_pca = XÌ„V[:,:k]                      â”‚
â”‚                                                     â”‚
â”‚  Variance explained: Î»áµ¢ / Î£Î»â±¼                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Properties:**
- Principal components are orthogonal
- First PC captures maximum variance
- Used for visualization, denoising, compression

### 2. Support Vector Machine (SVM)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HARD MARGIN (linearly separable)                   â”‚
â”‚                                                     â”‚
â”‚  min  (1/2)||w||Â²                                   â”‚
â”‚  s.t. yáµ¢(wáµ€xáµ¢ + b) â‰¥ 1  âˆ€i                          â”‚
â”‚                                                     â”‚
â”‚  SOFT MARGIN (with slack variables)                 â”‚
â”‚                                                     â”‚
â”‚  min  (1/2)||w||Â² + C Î£Î¾áµ¢                           â”‚
â”‚  s.t. yáµ¢(wáµ€xáµ¢ + b) â‰¥ 1 - Î¾áµ¢                         â”‚
â”‚       Î¾áµ¢ â‰¥ 0                                        â”‚
â”‚                                                     â”‚
â”‚  Margin = 2 / ||w||                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Kernel Trick:**
| Kernel | Formula | Use Case |
| :--- | :--- | :--- |
| Linear | K(x,y) = xáµ€y | Linearly separable |
| RBF | K(x,y) = exp(-Î³\|\|x-y\|\|Â²) | Non-linear, default |
| Polynomial | K(x,y) = (Î³xáµ€y + r)^d | Polynomial boundary |

### 3. K-Means Clustering

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OBJECTIVE: min Î£â‚– Î£â‚“âˆˆCâ‚– ||x - Î¼â‚–||Â²                â”‚
â”‚                                                     â”‚
â”‚  Where:                                             â”‚
â”‚    Câ‚– = cluster k                                   â”‚
â”‚    Î¼â‚– = centroid of cluster k                       â”‚
â”‚                                                     â”‚
â”‚  Update rules:                                      â”‚
â”‚    Assignment: cáµ¢ = argmin_k ||xáµ¢ - Î¼â‚–||Â²           â”‚
â”‚    Centroid:   Î¼â‚– = (1/|Câ‚–|) Î£â‚“âˆˆCâ‚– x                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. K-Nearest Neighbors (KNN)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLASSIFICATION:                                    â”‚
â”‚    Å· = mode({yâ±¼ : xâ±¼ âˆˆ Nâ‚–(x)})                      â”‚
â”‚                                                     â”‚
â”‚  REGRESSION:                                        â”‚
â”‚    Å· = (1/k) Î£â±¼âˆˆNâ‚–(x) yâ±¼                            â”‚
â”‚                                                     â”‚
â”‚  Distance metrics:                                  â”‚
â”‚    Euclidean: d(x,y) = âˆš(Î£(xáµ¢-yáµ¢)Â²)                 â”‚
â”‚    Manhattan: d(x,y) = Î£|xáµ¢-yáµ¢|                     â”‚
â”‚    Cosine:    d(x,y) = 1 - (xáµ€y)/(||x||||y||)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5. Decision Trees

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SPLITTING CRITERIA                                 â”‚
â”‚                                                     â”‚
â”‚  Entropy: H(S) = -Î£páµ¢logâ‚‚(páµ¢)                       â”‚
â”‚                                                     â”‚
â”‚  Information Gain: IG = H(S) - Î£(|Sáµ¥|/|S|)H(Sáµ¥)     â”‚
â”‚                                                     â”‚
â”‚  Gini Impurity: G = 1 - Î£páµ¢Â²                        â”‚
â”‚                                                     â”‚
â”‚  Choose split that maximizes IG or minimizes Gini   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6. Ensemble Methods

| Method | Technique | Formula |
| :--- | :--- | :--- |
| **Bagging** | Bootstrap + Aggregate | Å· = (1/B)Î£fáµ¦(x) |
| **Random Forest** | Bagging + random features | Å· = mode(tree predictions) |
| **Boosting** | Sequential weighted | Å· = Î£Î±â‚˜hâ‚˜(x) |
| **AdaBoost** | Exponential loss | Î±â‚˜ = (1/2)ln((1-Îµâ‚˜)/Îµâ‚˜) |

---

## âš™ï¸ Algorithms

### Algorithm 1: K-Means

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Data X, number of clusters K                â”‚
â”‚  OUTPUT: Cluster assignments, centroids             â”‚
â”‚                                                     â”‚
â”‚  1. Initialize centroids Î¼â‚,...,Î¼â‚– randomly         â”‚
â”‚  2. REPEAT until convergence:                       â”‚
â”‚     3. Assignment step:                             â”‚
â”‚        FOR each xáµ¢:                                 â”‚
â”‚          cáµ¢ = argmin_k ||xáµ¢ - Î¼â‚–||Â²                 â”‚
â”‚     4. Update step:                                 â”‚
â”‚        FOR each k:                                  â”‚
â”‚          Î¼â‚– = mean({xáµ¢ : cáµ¢ = k})                   â”‚
â”‚  5. RETURN clusters, centroids                      â”‚
â”‚                                                     â”‚
â”‚  Convergence: centroids don't change                â”‚
â”‚  Complexity: O(nKd) per iteration                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 2: PCA

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Data X âˆˆ â„â¿Ë£áµˆ, target dimensions k          â”‚
â”‚  OUTPUT: Projected data X_pca âˆˆ â„â¿Ë£áµ                â”‚
â”‚                                                     â”‚
â”‚  1. Center: XÌ„ = X - mean(X, axis=0)                â”‚
â”‚  2. Covariance: C = (1/n)XÌ„áµ€XÌ„                      â”‚
â”‚  3. Eigendecomposition: C = VÎ›Váµ€                    â”‚
â”‚  4. Sort eigenvectors by eigenvalue (descending)    â”‚
â”‚  5. Select top k eigenvectors: Vâ‚–                   â”‚
â”‚  6. Project: X_pca = XÌ„Vâ‚–                           â”‚
â”‚  7. RETURN X_pca                                    â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 3: SVM (SMO sketch)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Data (xáµ¢, yáµ¢), kernel K, C                  â”‚
â”‚  OUTPUT: Support vectors, weights                   â”‚
â”‚                                                     â”‚
â”‚  Dual problem:                                      â”‚
â”‚  max Î£Î±áµ¢ - (1/2)Î£Î£Î±áµ¢Î±â±¼yáµ¢yâ±¼K(xáµ¢,xâ±¼)                  â”‚
â”‚  s.t. 0 â‰¤ Î±áµ¢ â‰¤ C, Î£Î±áµ¢yáµ¢ = 0                         â”‚
â”‚                                                     â”‚
â”‚  Decision function:                                 â”‚
â”‚  f(x) = sign(Î£Î±áµ¢yáµ¢ K(xáµ¢,x) + b)                     â”‚
â”‚                                                     â”‚
â”‚  Support vectors: points where 0 < Î±áµ¢ â‰¤ C           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## â“ Interview Questions & Answers

<details>
<summary><b>Q1: How does PCA work? What are its limitations?</b></summary>

**Answer:**

**How it works:**
1. Find directions of maximum variance
2. Project data onto these directions
3. Keeps most information with fewer dimensions

**Limitations:**
- Only linear transformations
- Sensitive to scaling (standardize first!)
- May not capture class-discriminative features
- Outliers affect results significantly

</details>

<details>
<summary><b>Q2: Explain the kernel trick in SVM.</b></summary>

**Answer:**

**Problem:** Data not linearly separable in original space

**Solution:** Map to higher dimension where it becomes separable

**Kernel trick:** Never explicitly compute Ï†(x), only K(x,y) = Ï†(x)áµ€Ï†(y)

**Example - RBF kernel:**
- Implicitly maps to infinite dimensions
- K(x,y) = exp(-Î³||x-y||Â²)
- Î³ controls decision boundary complexity

**Key insight:** Dual formulation only uses dot products â†’ can kernelize

</details>

<details>
<summary><b>Q3: How to choose K in K-Means?</b></summary>

**Answer:**

**Methods:**
1. **Elbow method:** Plot inertia vs K, find "elbow"
2. **Silhouette score:** Measures cluster separation, maximize
3. **Gap statistic:** Compare to null reference distribution
4. **Domain knowledge:** Sometimes K is known

**Inertia formula:** Î£â‚– Î£â‚“âˆˆCâ‚– ||x - Î¼â‚–||Â²

**Silhouette:** s = (b-a) / max(a,b)
- a = mean intra-cluster distance
- b = mean nearest-cluster distance

</details>

<details>
<summary><b>Q4: Random Forest vs single Decision Tree?</b></summary>

**Answer:**

| Aspect | Single Tree | Random Forest |
| :--- | :--- | :--- |
| Variance | High (overfit) | Low (averaged) |
| Bias | Low | Low |
| Interpretability | High | Low |
| Training time | Fast | Slower |
| Feature importance | Yes | Yes (averaged) |

**Why RF works:**
- Bagging reduces variance
- Random feature selection decorrelates trees
- Ensemble averages out individual errors

</details>

<details>
<summary><b>Q5: What is the bias-variance tradeoff?</b></summary>

**Answer:**

```
Total Error = BiasÂ² + Variance + Noise
```

| Model | Bias | Variance | Example |
| :--- | :--- | :--- | :--- |
| Simple | High | Low | Linear regression |
| Complex | Low | High | Deep tree |

**Goal:** Find sweet spot

**Solutions:**
- Cross-validation to tune complexity
- Regularization (increase bias, decrease variance)
- Ensemble methods (decrease variance)

</details>

<details>
<summary><b>Q6: KNN - how to choose K?</b></summary>

**Answer:**

**Guidelines:**
- Small K: Low bias, high variance (noisy)
- Large K: High bias, low variance (smooth)
- Odd K for binary classification (avoid ties)
- Rule of thumb: K = âˆšn

**Cross-validation:** Try different K, pick best

**Distance weighting:** Give closer neighbors more weight

</details>

<details>
<summary><b>Q7: How does AdaBoost work?</b></summary>

**Answer:**

1. **Initialize** weights wáµ¢ = 1/n
2. **For each round m:**
   - Train weak learner hâ‚˜ on weighted data
   - Compute error: Îµâ‚˜ = Î£wáµ¢ğŸ™[hâ‚˜(xáµ¢)â‰ yáµ¢]
   - Compute weight: Î±â‚˜ = (1/2)ln((1-Îµâ‚˜)/Îµâ‚˜)
   - Update weights: wáµ¢ â† wáµ¢ exp(-Î±â‚˜yáµ¢hâ‚˜(xáµ¢))
   - Normalize weights
3. **Final prediction:** H(x) = sign(Î£Î±â‚˜hâ‚˜(x))

**Key:** Focuses on misclassified samples each round

</details>

<details>
<summary><b>Q8: LDA vs PCA?</b></summary>

**Answer:**

| Aspect | PCA | LDA |
| :--- | :--- | :--- |
| Type | Unsupervised | Supervised |
| Goal | Max variance | Max class separation |
| Uses labels | No | Yes |
| Max components | min(n,d) | C-1 (C=classes) |

**LDA objective:** Maximize between-class / within-class variance

**When to use:**
- PCA: General dimensionality reduction
- LDA: When you have labels and want classification

</details>

---

## ğŸ“š Key Formulas Reference

| Formula | Description |
| :--- | :--- |
| C = (1/n)Xáµ€X | Covariance matrix |
| K(x,y) = exp(-Î³\|\|x-y\|\|Â²) | RBF kernel |
| J = Î£â‚– Î£â‚“âˆˆCâ‚– \|\|x - Î¼â‚–\|\|Â² | K-means objective |
| H(S) = -Î£páµ¢logâ‚‚(páµ¢) | Entropy |
| G = 1 - Î£páµ¢Â² | Gini impurity |
| s = (b-a) / max(a,b) | Silhouette score |


---

<br/>

<div align="center">

## ğŸ““ PRACTICE

### ğŸš€ *Ready to code? Let's get started!*

<br/>

### ğŸš€ Open in Google Colab

<br/>

<p align="center">
  <a href="https://colab.research.google.com/github/falkomeAI/computer_vision_complete/blob/main/07_Classical_ML/colab_tutorial.ipynb">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" height="60"/>
  </a>
</p>

<br/>

<p align="center">
  <strong>âœ¨ Click the badge above to open this notebook directly in Google Colab!</strong>
</p>

<br/>


</div>

<br/>


---

<br/>

<div align="center">

| | | |
| :--- |:---:|---:|
| **[â—€ Geometry](../06_Geometry_MultiView/README.md)** | **[ğŸ  HOME](../README.md)** | **[Neural Nets â–¶](../08_Neural_Networks/README.md)** |

<br/>

---

ğŸŒ™ Part of **[Computer Vision Complete](../README.md)**

<p align="center">
  Made with â¤ï¸ by <a href="https://github.com/falkomeAI">falkomeAI</a>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/â­_Star_this_repo_if_helpful-60A5FA?style=for-the-badge&logo=github&logoColor=white" alt="Star"/>
</p>

<br/>

</div>
