<div align="center">

<br/>

<a href="../09_CNN_Architectures/README.md"><img src="https://img.shields.io/badge/â—€__CNNs-0f172a?style=for-the-badge&labelColor=1e293b" height="35"/></a>
&nbsp;&nbsp;&nbsp;&nbsp;
<a href="../README.md"><img src="https://img.shields.io/badge/ğŸ __HOME-F472B6?style=for-the-badge&labelColor=0f172a" height="35"/></a>
&nbsp;&nbsp;&nbsp;&nbsp;
<a href="../11_Vision_Transformers/README.md"><img src="https://img.shields.io/badge/ViTs__â–¶-0f172a?style=for-the-badge&labelColor=1e293b" height="35"/></a>

<br/><br/>

---

<br/>

# ğŸ“‹ VISION TASKS

### ğŸŒ™ *What Models Do*

<br/>

<img src="https://img.shields.io/badge/ğŸ“š__MODULE__10/20-F472B6?style=for-the-badge&labelColor=0f172a" height="40"/>
&nbsp;&nbsp;
<img src="https://img.shields.io/badge/â±ï¸__2_HOURS-FBBF24?style=for-the-badge&labelColor=0f172a" height="40"/>
&nbsp;&nbsp;
<img src="https://img.shields.io/badge/ğŸ““__NOTEBOOK_READY-34D399?style=for-the-badge&labelColor=0f172a" height="40"/>

<br/><br/>

---

</div>

<br/>

## ğŸ“– Overview

> **Vision tasks define what we want computers to see.** This module covers classification, object detection, segmentation (semantic, instance, panoptic), and trackingâ€”the core problems that deep learning models solve, along with evaluation metrics and architectures.

<br/>

---

## ğŸ¯ What You'll Learn

<table>
<tr>
<td width="50%">

### ğŸ·ï¸ **Classification**
- Image classification
- Multi-label classification
- Top-k accuracy
- Transfer learning

</td>
<td width="50%">

### ğŸ“¦ **Detection**
- Two-stage (R-CNN family)
- One-stage (YOLO, SSD)
- Anchor-based vs anchor-free
- mAP evaluation

</td>
</tr>
<tr>
<td width="50%">

### ğŸ¨ **Segmentation**
- Semantic segmentation (FCN, U-Net)
- Instance segmentation (Mask R-CNN)
- Panoptic segmentation
- mIoU, AP metrics

</td>
<td width="50%">

### ğŸ¯ **Tracking**
- Object tracking
- Multi-object tracking (MOT)
- Tracking-by-detection
- Association algorithms

</td>
</tr>
</table>

<br/>

---

## ğŸ¯ Key Concepts

| Task | Input | Output | Metric |
| :--- | :--- | :--- | :--- |
| **Classification** | Image | Class label | Accuracy, Top-5 |
| **Detection** | Image | Boxes + labels | mAP@IoU |
| **Semantic Seg** | Image | Pixel-wise labels | mIoU |
| **Instance Seg** | Image | Masks + labels | AP |
| **Panoptic Seg** | Image | Things + Stuff | PQ |

---

## ğŸ¨ Visual Overview

<div align="center">
<img src="./svg_figs/detection_architectures.svg" alt="Detection Architectures" width="100%"/>
</div>

---

## ğŸ”¢ Mathematical Foundations

### 1. Intersection over Union (IoU)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  IoU = |A âˆ© B| / |A âˆª B|                            â”‚
â”‚                                                     â”‚
â”‚  For boxes:                                         â”‚
â”‚  A = (x1, y1, x2, y2)                               â”‚
â”‚  B = (x1', y1', x2', y2')                           â”‚
â”‚                                                     â”‚
â”‚  Intersection:                                      â”‚
â”‚  xi1 = max(x1, x1')                                 â”‚
â”‚  yi1 = max(y1, y1')                                 â”‚
â”‚  xi2 = min(x2, x2')                                 â”‚
â”‚  yi2 = min(y2, y2')                                 â”‚
â”‚  area_i = max(0, xi2-xi1) Ã— max(0, yi2-yi1)         â”‚
â”‚                                                     â”‚
â”‚  Union:                                             â”‚
â”‚  area_u = area_A + area_B - area_i                  â”‚
â”‚                                                     â”‚
â”‚  IoU = area_i / area_u                              â”‚
â”‚                                                     â”‚
â”‚  Range: [0, 1], higher is better                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Mean Average Precision (mAP)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PRECISION-RECALL CURVE                             â”‚
â”‚                                                     â”‚
â”‚  Precision = TP / (TP + FP)                         â”‚
â”‚  Recall = TP / (TP + FN)                            â”‚
â”‚                                                     â”‚
â”‚  AVERAGE PRECISION (per class)                      â”‚
â”‚                                                     â”‚
â”‚  AP = âˆ«â‚€Â¹ p(r) dr  â‰ˆ Î£ (ráµ¢ - ráµ¢â‚‹â‚) Ã— páµ¢â‚™â‚œâ‚‘áµ£â‚š        â”‚
â”‚                                                     â”‚
â”‚  11-point interpolation (PASCAL VOC):               â”‚
â”‚  AP = (1/11) Î£ max(p(r')) for r' â‰¥ r                â”‚
â”‚       râˆˆ{0,0.1,...,1}                               â”‚
â”‚                                                     â”‚
â”‚  MEAN AVERAGE PRECISION                             â”‚
â”‚                                                     â”‚
â”‚  mAP = (1/C) Î£ APc                                  â”‚
â”‚                                                     â”‚
â”‚  mAP@0.5: IoU threshold = 0.5                       â”‚
â”‚  mAP@0.5:0.95: average over IoU thresholds          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. Non-Maximum Suppression (NMS)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Boxes B, Scores S, IoU threshold Ï„          â”‚
â”‚  OUTPUT: Filtered boxes                             â”‚
â”‚                                                     â”‚
â”‚  1. Sort boxes by score (descending)                â”‚
â”‚  2. Select box with highest score â†’ D               â”‚
â”‚  3. Remove all boxes with IoU(box, D) > Ï„           â”‚
â”‚  4. Repeat until no boxes remain                    â”‚
â”‚                                                     â”‚
â”‚  SOFT-NMS (alternative):                            â”‚
â”‚  Instead of hard removal, decay scores:             â”‚
â”‚  sáµ¢ = sáµ¢ Ã— exp(-IoUÂ²/Ïƒ)                             â”‚
â”‚                                                     â”‚
â”‚  Typical Ï„ = 0.5 for detection                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. Focal Loss (for class imbalance)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CROSS-ENTROPY LOSS                                 â”‚
â”‚                                                     â”‚
â”‚  CE(p, y) = -log(pâ‚œ)                                â”‚
â”‚  where pâ‚œ = p if y=1, else (1-p)                    â”‚
â”‚                                                     â”‚
â”‚  FOCAL LOSS                                         â”‚
â”‚                                                     â”‚
â”‚  FL(p, y) = -Î±â‚œ(1-pâ‚œ)áµ log(pâ‚œ)                      â”‚
â”‚                                                     â”‚
â”‚  (1-pâ‚œ)áµ: modulating factor                         â”‚
â”‚  - Easy examples (pâ‚œ â†’ 1): factor â†’ 0               â”‚
â”‚  - Hard examples (pâ‚œ â†’ 0): factor â†’ 1               â”‚
â”‚                                                     â”‚
â”‚  Typical: Î³ = 2, Î± = 0.25                           â”‚
â”‚                                                     â”‚
â”‚  Addresses: foreground-background imbalance         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5. Segmentation Losses

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CROSS-ENTROPY (per pixel)                          â”‚
â”‚                                                     â”‚
â”‚  L = -(1/N) Î£áµ¢ Î£c yáµ¢c log(páµ¢c)                      â”‚
â”‚                                                     â”‚
â”‚  DICE LOSS                                          â”‚
â”‚                                                     â”‚
â”‚  Dice = 2|X âˆ© Y| / (|X| + |Y|)                      â”‚
â”‚  L_dice = 1 - Dice                                  â”‚
â”‚                                                     â”‚
â”‚  For soft predictions:                              â”‚
â”‚  L_dice = 1 - (2Î£páµ¢yáµ¢ + Îµ) / (Î£páµ¢ + Î£yáµ¢ + Îµ)        â”‚
â”‚                                                     â”‚
â”‚  IoU LOSS                                           â”‚
â”‚                                                     â”‚
â”‚  L_iou = 1 - IoU(pred, target)                      â”‚
â”‚                                                     â”‚
â”‚  Dice good for: imbalanced classes                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Algorithms

### Algorithm 1: Two-Stage Detection (Faster R-CNN)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. BACKBONE: Extract feature maps                  â”‚
â”‚     - ResNet/VGG â†’ multi-scale features             â”‚
â”‚                                                     â”‚
â”‚  2. REGION PROPOSAL NETWORK (RPN):                  â”‚
â”‚     - Slide 3Ã—3 window over feature map             â”‚
â”‚     - At each location: k anchors (scales/ratios)   â”‚
â”‚     - Predict: objectness + box regression          â”‚
â”‚     - Output: ~2000 region proposals                â”‚
â”‚                                                     â”‚
â”‚  3. ROI POOLING/ALIGN:                              â”‚
â”‚     - Extract fixed-size features from proposals    â”‚
â”‚     - ROI Align: bilinear interpolation             â”‚
â”‚                                                     â”‚
â”‚  4. DETECTION HEAD:                                 â”‚
â”‚     - Classification: C+1 classes (+ background)    â”‚
â”‚     - Bounding box regression: 4C outputs           â”‚
â”‚                                                     â”‚
â”‚  5. POST-PROCESSING:                                â”‚
â”‚     - Apply NMS per class                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 2: One-Stage Detection (YOLO)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. BACKBONE + NECK:                                â”‚
â”‚     - CSPDarknet + PANet (YOLOv4)                   â”‚
â”‚     - Multi-scale feature pyramids                  â”‚
â”‚                                                     â”‚
â”‚  2. DETECTION HEAD (at each scale):                 â”‚
â”‚     - Grid: SÃ—S cells                               â”‚
â”‚     - Each cell predicts B boxes:                   â”‚
â”‚       * 4 coordinates (x, y, w, h)                  â”‚
â”‚       * 1 objectness score                          â”‚
â”‚       * C class probabilities                       â”‚
â”‚                                                     â”‚
â”‚  3. OUTPUT ENCODING:                                â”‚
â”‚     - (x, y): offset from cell corner               â”‚
â”‚     - (w, h): relative to anchor size               â”‚
â”‚                                                     â”‚
â”‚  4. LOSS FUNCTION:                                  â”‚
â”‚     L = Î»_coord Ã— L_box                             â”‚
â”‚       + Î»_obj Ã— L_obj                               â”‚
â”‚       + Î»_cls Ã— L_cls                               â”‚
â”‚                                                     â”‚
â”‚  5. NMS on all predictions                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 3: Semantic Segmentation (DeepLab)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. ENCODER (with dilated/atrous convolutions):     â”‚
â”‚     - Preserve resolution with dilation             â”‚
â”‚     - Output stride = 8 or 16                       â”‚
â”‚                                                     â”‚
â”‚  2. ATROUS SPATIAL PYRAMID POOLING (ASPP):          â”‚
â”‚     - Parallel dilated convs at rates (6,12,18)     â”‚
â”‚     - 1Ã—1 conv (global features)                    â”‚
â”‚     - Concatenate all                               â”‚
â”‚                                                     â”‚
â”‚  3. DECODER:                                        â”‚
â”‚     - Upsample ASPP output                          â”‚
â”‚     - Concatenate with low-level features           â”‚
â”‚     - Refine boundaries                             â”‚
â”‚                                                     â”‚
â”‚  4. OUTPUT:                                         â”‚
â”‚     - 1Ã—1 conv â†’ C channels                         â”‚
â”‚     - Bilinear upsample to input resolution         â”‚
â”‚     - Softmax per pixel                             â”‚
â”‚                                                     â”‚
â”‚  Key: Dilated convs capture multi-scale context     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

---

## âš ï¸ Common Pitfalls

| âŒ Pitfall | âœ… Solution |
| --- | --- |
| Wrong IoU threshold | Use 0.5 for COCO, 0.5:0.95 for mAP@[0.5:0.95] |
| Not using NMS | Always apply Non-Maximum Suppression after detection |
| Confusing mAP vs AP | AP = single class, mAP = average across classes |
| Wrong anchor sizes | Match anchors to object scales in your dataset |
| Forgetting to normalize boxes | Normalize coordinates to [0,1] for training |
| Using wrong loss | Use focal loss for imbalanced, CE for balanced |

<br/>

---

## ğŸ› ï¸ Mini Projects

<details>
<summary><b>Project 1: Object Detection Pipeline</b></summary>

- Load pretrained YOLO or Faster R-CNN
- Run inference on custom images
- Visualize bounding boxes and scores
- Filter by confidence threshold
- Apply NMS to remove duplicates
- Evaluate on test set

</details>

<details>
<summary><b>Project 2: Semantic Segmentation</b></summary>

- Implement U-Net architecture
- Train on Cityscapes or Pascal VOC
- Visualize segmentation masks
- Compute mIoU metric
- Compare with pretrained models
- Apply to custom images

</details>

<details>
<summary><b>Project 3: Multi-Object Tracking</b></summary>

- Use detection model (YOLO)
- Implement tracking algorithm (SORT/DeepSORT)
- Track objects across video frames
- Handle occlusions and re-identification
- Visualize tracks with unique IDs
- Evaluate tracking accuracy

</details>

<br/>

---

## â“ Interview Questions & Answers

<details>
<summary><b>Q1: One-stage vs two-stage detectors?</b></summary>

| Two-Stage (Faster R-CNN) | One-Stage (YOLO, SSD) |
| :--- | :--- |
| Region proposals first | Direct prediction |
| Higher accuracy | Faster inference |
| Slower (~5 FPS) | Real-time (30+ FPS) |
| Better for small objects | May miss small objects |

**Modern:** One-stage closing accuracy gap (YOLOv5, FCOS)

</details>

<details>
<summary><b>Q2: What is Feature Pyramid Network (FPN)?</b></summary>

**Problem:** Objects at different scales

**Solution:** Multi-scale feature maps with top-down pathway

**Architecture:**
1. Bottom-up: standard backbone
2. Top-down: upsample + lateral connections
3. Output: pyramid of feature maps

**Benefit:** Strong features at all scales for detection

</details>

<details>
<summary><b>Q3: How does anchor-free detection work?</b></summary>

**Anchor-based:** Predefined box sizes/ratios
**Anchor-free:** Directly predict box coordinates

**Methods:**
- **FCOS:** Predict distance to edges at each point
- **CenterNet:** Predict center heatmap + size
- **CornerNet:** Predict top-left, bottom-right corners

**Advantage:** No anchor hyperparameter tuning

</details>

<details>
<summary><b>Q4: Explain dilated/atrous convolution.</b></summary>

**Standard conv:** Adjacent pixels
**Dilated conv:** Insert gaps (dilation rate r)

**Effective receptive field:** k' = k + (k-1)(r-1)

**Use in segmentation:**
- Large receptive field
- No resolution loss (no pooling)
- Multi-scale via different dilation rates (ASPP)

</details>

<details>
<summary><b>Q5: What is panoptic segmentation?</b></summary>

**Combines:**
- **Instance seg:** Things (countable: person, car)
- **Semantic seg:** Stuff (uncountable: sky, road)

**Metric:** Panoptic Quality (PQ) = SQ Ã— RQ
- SQ: Segmentation Quality (IoU of matched)
- RQ: Recognition Quality (like F1)

**Challenge:** Unified handling of things + stuff

</details>

<details>
<summary><b>Q6: How to handle class imbalance in detection?</b></summary>

**Problem:** Many more background than foreground

**Solutions:**
1. **Hard negative mining:** Sample hard negatives
2. **Focal loss:** Down-weight easy examples
3. **OHEM:** Online hard example mining
4. **Class weights:** Higher weight for rare classes

**Focal loss:** FL = -Î±â‚œ(1-pâ‚œ)áµ log(pâ‚œ), Î³=2 typical

</details>

---

## ğŸ“š Resources

**Papers:**
- [R-CNN (2014)](https://arxiv.org/abs/1311.2524) - Girshick et al.
- [YOLO (2016)](https://arxiv.org/abs/1506.02640) - Redmon et al.
- [Mask R-CNN (2017)](https://arxiv.org/abs/1703.06870) - He et al.
- [U-Net (2015)](https://arxiv.org/abs/1505.04597) - Ronneberger et al.

**Datasets:**
- [COCO](https://cocodataset.org/) - Detection & segmentation
- [Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/) - Classification & detection
- [Cityscapes](https://www.cityscapes-dataset.com/) - Semantic segmentation

**Videos:**
- [Object Detection Explained](https://www.youtube.com/watch?v=nDPWywWRIRo)
- [YOLO Algorithm](https://www.youtube.com/watch?v=MPU2HistivI)

<br/>

---

## ğŸ“š Key Formulas Reference

| Formula | Description |
| :--- | :--- |
| IoU = \|Aâˆ©B\| / \|AâˆªB\| | Intersection over Union |
| mAP = (1/C)Î£APc | Mean Average Precision |
| FL = -Î±â‚œ(1-pâ‚œ)áµlog(pâ‚œ) | Focal Loss |
| Dice = 2\|Xâˆ©Y\|/(\|X\|+\|Y\|) | Dice coefficient |
| k' = k + (k-1)(r-1) | Dilated conv receptive field |


---

<br/>

<div align="center">

## ğŸ““ PRACTICE

<br/>

### ğŸš€ Click to Open Directly in Google Colab

<br/>

<a href="https://colab.research.google.com/github/USERNAME/computer_vision_complete/blob/main/10_Vision_Tasks/colab_tutorial.ipynb">
<img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" height="50"/>
</a>

<br/><br/>

> âš ï¸ **First time?** Push this repo to GitHub, then replace `USERNAME` in the link above with your GitHub username.

<br/>

**Or manually:** [ğŸ“¥ Download](./colab_tutorial.ipynb) â†’ [ğŸŒ Colab](https://colab.research.google.com) â†’ Upload

</div>

<br/>




---

<br/>

<div align="center">

| | | |
|:---|:---:|---:|
| **[â—€ CNNs](../09_CNN_Architectures/README.md)** | **[ğŸ  HOME](../README.md)** | **[ViTs â–¶](../11_Vision_Transformers/README.md)** |

<br/>

---

ğŸŒ™ Part of **[Computer Vision Complete](../README.md)** Â· Made with â¤ï¸

<br/>

</div>
