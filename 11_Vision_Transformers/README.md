<div align="center">

<br/>

<a href="../10_Vision_Tasks/README.md"><img src="https://img.shields.io/badge/â—€__Tasks-0f172a?style=for-the-badge&labelColor=1e293b" height="35"/></a>
&nbsp;&nbsp;&nbsp;&nbsp;
<a href="../README.md"><img src="https://img.shields.io/badge/ğŸ __HOME-60A5FA?style=for-the-badge&labelColor=0f172a" height="35"/></a>
&nbsp;&nbsp;&nbsp;&nbsp;
<a href="../12_Self_Supervised/README.md"><img src="https://img.shields.io/badge/Self-Supervised__â–¶-0f172a?style=for-the-badge&labelColor=1e293b" height="35"/></a>

<br/><br/>

---

<br/>

# ğŸ‘ï¸ VISION TRANSFORMERS

### ğŸŒ™ *Attention is All You Need*

<br/>

<img src="https://img.shields.io/badge/ğŸ“š__MODULE__11/20-60A5FA?style=for-the-badge&labelColor=0f172a" height="40"/>
&nbsp;&nbsp;
<img src="https://img.shields.io/badge/â±ï¸__2_HOURS-FBBF24?style=for-the-badge&labelColor=0f172a" height="40"/>
&nbsp;&nbsp;
<img src="https://img.shields.io/badge/ğŸ““__NOTEBOOK_READY-34D399?style=for-the-badge&labelColor=0f172a" height="40"/>

<br/><br/>

---

</div>

<br/>

## ğŸ“– Overview

> **Vision Transformers treat images as sequences of patches.** No convolutionsâ€”just attention! This module covers ViT fundamentals, Swin Transformer's window attention, and self-supervised methods like DINO and MAE.

<br/>

---

## ğŸ¯ Key Concepts

| Concept | Description | Used In |
| :--- | :--- | :--- |
| **Patch Embedding** | Split image into patches, project to tokens | ViT, DeiT, Swin |
| **Self-Attention** | Compute pairwise relationships between tokens | All Transformers |
| **Position Embedding** | Add spatial information (learned or sinusoidal) | All ViTs |
| **CLS Token** | Learnable token for classification | ViT, DeiT |
| **Window Attention** | Attention within local windows | Swin Transformer |
| **Shifted Windows** | Cross-window connections | Swin Transformer |

<br/>

---

## ğŸ—ï¸ ViT Architecture

```
Image â†’ Patch Split â†’ Linear Projection â†’ + Position â†’ Transformer â†’ Class Token
224Ã—224    14Ã—14Ã—196      196Ã—768          Embedding    12 blocks      â†’ MLP Head
```

<br/>

---

## ğŸ“Š Model Comparison

| Model | Year | Key Innovation | ImageNet | Speed |
| :--- | :---: | :--- | :---: | :---: |
| **ViT-B/16** | 2020 | Patch tokens + Transformer | 77.9% | Medium |
| **DeiT-S** | 2021 | Knowledge distillation | 79.8% | Fast |
| **Swin-T** | 2021 | Window + shifted attention | 81.3% | Fast |
| **BEiT** | 2021 | BERT-style pretraining | 83.2% | Medium |
| **MAE** | 2022 | Masked autoencoder | 83.6% | Slow |
| **DINOv2** | 2023 | Self-supervised foundation | 86.5% | Medium |

<br/>

---

## ğŸ”¢ Key Formulas

<table>
<tr>
<td>

### Patch Embedding
```
patches = split(image, PÃ—P)  # N = HÃ—W / PÂ²
tokens = Linear(flatten(patches))
zâ‚€ = [CLS] + tokens + pos_embed
```

</td>
<td>

### Self-Attention
```
Q, K, V = Linear(x)
Attn = softmax(QKáµ€ / âˆšd) Ã— V
```

</td>
</tr>
<tr>
<td>

### Multi-Head Attention
```
MultiHead = Concat(headâ‚...headâ‚•)Wá´¼
headáµ¢ = Attention(QWáµ¢Q, KWáµ¢K, VWáµ¢V)
```

</td>
<td>

### Transformer Block
```
x' = x + MSA(LayerNorm(x))
x'' = x' + MLP(LayerNorm(x'))
```

</td>
</tr>
</table>

<br/>

---

## âš™ï¸ Algorithms

### Algorithm 1: ViT Forward Pass

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Image x (HÃ—WÃ—3)                             â”‚
â”‚  OUTPUT: Class probabilities                        â”‚
â”‚                                                     â”‚
â”‚  1. PATCH EMBEDDING:                                â”‚
â”‚     patches = split(x, 16Ã—16) â†’ N patches           â”‚
â”‚     tokens = Linear(flatten(patches)) â†’ (N, D)      â”‚
â”‚                                                     â”‚
â”‚  2. ADD CLS TOKEN + POSITION:                       â”‚
â”‚     zâ‚€ = [CLS; tokens] + pos_embed â†’ (N+1, D)       â”‚
â”‚                                                     â”‚
â”‚  3. TRANSFORMER ENCODER (L layers):                 â”‚
â”‚     FOR l = 1 to L:                                 â”‚
â”‚       z' = z + MSA(LayerNorm(z))                    â”‚
â”‚       z = z' + MLP(LayerNorm(z'))                   â”‚
â”‚                                                     â”‚
â”‚  4. CLASSIFICATION:                                 â”‚
â”‚     output = MLP_head(z[0])  # CLS token only       â”‚
â”‚     probs = softmax(output)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 2: Multi-Head Self-Attention

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Tokens z (NÃ—D)                              â”‚
â”‚  OUTPUT: Attended tokens (NÃ—D)                      â”‚
â”‚                                                     â”‚
â”‚  1. PROJECT to Q, K, V:                             â”‚
â”‚     Q = z @ Wq  (N Ã— d_k)                           â”‚
â”‚     K = z @ Wk  (N Ã— d_k)                           â”‚
â”‚     V = z @ Wv  (N Ã— d_v)                           â”‚
â”‚                                                     â”‚
â”‚  2. COMPUTE ATTENTION:                              â”‚
â”‚     scores = Q @ K.T / âˆšd_k   (N Ã— N)               â”‚
â”‚     attn = softmax(scores)                          â”‚
â”‚     output = attn @ V          (N Ã— d_v)            â”‚
â”‚                                                     â”‚
â”‚  3. MULTI-HEAD (h heads):                           â”‚
â”‚     Split Q,K,V into h heads                        â”‚
â”‚     Compute attention per head                      â”‚
â”‚     Concat and project: out @ Wo                    â”‚
â”‚                                                     â”‚
â”‚  Complexity: O(NÂ²Â·D)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 3: Swin Window Attention

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Feature map (HÃ—WÃ—C)                         â”‚
â”‚  OUTPUT: Transformed features                       â”‚
â”‚                                                     â”‚
â”‚  1. PARTITION into windows:                         â”‚
â”‚     windows = split_into_windows(x, MÃ—M)            â”‚
â”‚     # Each window: MÃ—M tokens                       â”‚
â”‚                                                     â”‚
â”‚  2. WINDOW ATTENTION (per window):                  â”‚
â”‚     Q, K, V = project(window)                       â”‚
â”‚     attn = softmax(Q @ K.T / âˆšd + bias)             â”‚
â”‚     out = attn @ V                                  â”‚
â”‚                                                     â”‚
â”‚  3. MERGE windows back                              â”‚
â”‚                                                     â”‚
â”‚  4. SHIFT (alternate layers):                       â”‚
â”‚     Shift by (M/2, M/2) before windowing            â”‚
â”‚     Allows cross-window information                 â”‚
â”‚                                                     â”‚
â”‚  Complexity: O(NÂ·MÂ²) instead of O(NÂ²)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

<br/>

---

## ğŸ—ï¸ Architecture Diagrams

<div align="center">
<img src="./svg_figs/vit_architecture.svg" alt="ViT Architecture" width="90%"/>
</div>

<br/>

<div align="center">
<img src="./svg_figs/attention_mechanism.svg" alt="Attention" width="80%"/>
</div>

<br/>

<div align="center">
<img src="./svg_figs/swin_transformer.svg" alt="Swin Transformer" width="80%"/>
</div>

<br/>

---

## ğŸ’» Complete Code

> **Copy this entire code block and paste into Google Colab!**

*See Colab notebook for implementation*

<br/>

---

## âš ï¸ Common Pitfalls

| âŒ Pitfall | âœ… Solution |
| --- | --- |
| ViT needs huge data | Use pretrained or DeiT distillation |
| Wrong image size | ViT-B/16 expects 224Ã—224 |
| Forgetting CLS token | Classification uses CLS, not avg |
| Position embedding size | Must match number of patches |
| Slow attention | Use window attention (Swin) |

<br/>

---

## â“ Interview Q&A

<details>
<summary><b>Q1: Why does ViT need more data than CNN?</b></summary>

CNNs have **inductive biases**:
- **Locality**: Conv kernels look at local regions
- **Translation equivariance**: Same kernel everywhere

ViT has **no such biases**â€”must learn everything from data!

**Solutions:**
- Pretrain on large datasets (JFT-300M)
- Knowledge distillation (DeiT)
- Self-supervised pretraining (MAE, DINO)
</details>

<details>
<summary><b>Q2: What is the CLS token?</b></summary>

A **learnable embedding** prepended to patch tokens:
```
[CLS, patch1, patch2, ..., patch196]
```

- Attends to all patches through transformer layers
- Aggregates global information
- Used for final classification
- Similar to BERT's [CLS] token
</details>

<details>
<summary><b>Q3: How does Swin Transformer reduce complexity?</b></summary>

| Standard ViT | Swin Transformer |
| --- | --- |
| Global attention | Window attention |
| O(NÂ²) | O(N) per window |
| All patches | 7Ã—7 windows |
| No hierarchy | Hierarchical (merge patches) |

**Shifted windows** allow cross-window information flow.
</details>

<details>
<summary><b>Q4: ViT vs CNN - when to use which?</b></summary>

| Use CNN | Use ViT |
| --- | --- |
| Small datasets | Large datasets |
| Need locality | Global context matters |
| Edge deployment | Server inference |
| Real-time | Accuracy priority |
</details>

<br/>

---

## ğŸ“š Resources

**Papers:**
- [ViT (2020)](https://arxiv.org/abs/2010.11929) - Original Vision Transformer
- [DeiT (2021)](https://arxiv.org/abs/2012.12877) - Data-efficient training
- [Swin (2021)](https://arxiv.org/abs/2103.14030) - Hierarchical ViT
- [MAE (2022)](https://arxiv.org/abs/2111.06377) - Masked Autoencoder

**Videos:**
- [Yannic Kilcher - ViT](https://www.youtube.com/watch?v=TrdevFK_am4)

<br/>

---

<br/>

<div align="center">

## ğŸ““ PRACTICE

<br/>

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                                                                               â”ƒ
â”ƒ   ğŸ“¥ Download .ipynb  â†’  ğŸŒ Open colab.google  â†’  ğŸ“¤ Upload  â†’  â–¶ï¸ Run All   â”ƒ
â”ƒ                                                                               â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
```

<br/>

<a href="./colab_tutorial.ipynb"><img src="https://img.shields.io/badge/ğŸ“¥__DOWNLOAD_NOTEBOOK-0f172a?style=for-the-badge&labelColor=1e293b" height="40"/></a>
&nbsp;&nbsp;&nbsp;&nbsp;
<a href="https://colab.research.google.com"><img src="https://img.shields.io/badge/ğŸŒ__OPEN_COLAB-0f172a?style=for-the-badge&labelColor=1e293b" height="40"/></a>

</div>

<br/>



---

<br/>

<div align="center">

| | | |
|:---|:---:|---:|
| **[â—€ Tasks](../10_Vision_Tasks/README.md)** | **[ğŸ  HOME](../README.md)** | **[Self-Supervised â–¶](../12_Self_Supervised/README.md)** |

<br/>

---

ğŸŒ™ Part of **[Computer Vision Complete](../README.md)** Â· Made with â¤ï¸

<br/>

</div>
