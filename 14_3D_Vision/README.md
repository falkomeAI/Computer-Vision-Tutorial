<div align="center">

<br/>

<a href="../13_Video_Temporal/README.md"><img src="https://img.shields.io/badge/â—€__Video-0f172a?style=for-the-badge&labelColor=1e293b" height="35"/></a>
&nbsp;&nbsp;&nbsp;&nbsp;
<a href="../README.md"><img src="https://img.shields.io/badge/ğŸ __HOME-F87171?style=for-the-badge&labelColor=0f172a" height="35"/></a>
&nbsp;&nbsp;&nbsp;&nbsp;
<a href="../15_Generative_Vision/README.md"><img src="https://img.shields.io/badge/Generative__â–¶-0f172a?style=for-the-badge&labelColor=1e293b" height="35"/></a>

<br/><br/>

---

<br/>

# ğŸ² 3D VISION

### ğŸŒ™ *Three Dimensions*

<br/>

<img src="https://img.shields.io/badge/ğŸ“š__MODULE__14/20-F87171?style=for-the-badge&labelColor=0f172a" height="40"/>
&nbsp;&nbsp;
<img src="https://img.shields.io/badge/â±ï¸__2_HOURS-FBBF24?style=for-the-badge&labelColor=0f172a" height="40"/>
&nbsp;&nbsp;
<img src="https://img.shields.io/badge/ğŸ““__NOTEBOOK_READY-34D399?style=for-the-badge&labelColor=0f172a" height="40"/>

<br/><br/>

---

</div>

<br/>

## ğŸ¯ Key Concepts

| Concept | Description | Application |
| :--- | :--- | :--- |
| **Depth Estimation** | Per-pixel distance | AR, robotics |
| **Stereo Vision** | Depth from disparity | Autonomous driving |
| **Point Cloud** | 3D point set (x,y,z) | LiDAR processing |
| **NeRF** | Neural implicit 3D | Novel view synthesis |
| **Gaussian Splatting** | 3D Gaussians for rendering | Real-time 3D |

---

## ğŸ¨ Visual Overview

<div align="center">
<img src="./svg_figs/nerf_architecture.svg" alt="NeRF Architecture" width="100%"/>
</div>

---

## ğŸ”¢ Mathematical Foundations

### 1. Stereo Depth Estimation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DISPARITY-DEPTH RELATIONSHIP                       â”‚
â”‚                                                     â”‚
â”‚  Z = f Ã— B / d                                      â”‚
â”‚                                                     â”‚
â”‚  Z: depth                                           â”‚
â”‚  f: focal length (pixels)                           â”‚
â”‚  B: baseline (distance between cameras)             â”‚
â”‚  d: disparity (pixel difference between views)      â”‚
â”‚                                                     â”‚
â”‚  TRIANGULATION                                      â”‚
â”‚                                                     â”‚
â”‚  d = xâ‚— - xáµ£  (corresponding points)                â”‚
â”‚                                                     â”‚
â”‚  Inverse depth relationship:                        â”‚
â”‚  - Large disparity â†’ close object                   â”‚
â”‚  - Small disparity â†’ far object                     â”‚
â”‚  - d=0 â†’ infinity                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Point Cloud Representation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  POINT CLOUD: P = {(xáµ¢, yáµ¢, záµ¢)}áµ¢â‚Œâ‚á´º                â”‚
â”‚                                                     â”‚
â”‚  Optional attributes per point:                     â”‚
â”‚  - Color (RGB)                                      â”‚
â”‚  - Normal vector (nx, ny, nz)                       â”‚
â”‚  - Intensity (from LiDAR)                           â”‚
â”‚  - Semantic label                                   â”‚
â”‚                                                     â”‚
â”‚  PROCESSING CHALLENGES:                             â”‚
â”‚  - Unordered: permutation invariant needed          â”‚
â”‚  - Unstructured: no grid                            â”‚
â”‚  - Variable size                                    â”‚
â”‚                                                     â”‚
â”‚  POINTNET:                                          â”‚
â”‚  f(P) = g(MAX{h(xáµ¢)})                               â”‚
â”‚  - h: per-point MLP                                 â”‚
â”‚  - MAX: symmetric function (permutation invariant)  â”‚
â”‚  - g: output MLP                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. NeRF (Neural Radiance Fields)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCENE REPRESENTATION                               â”‚
â”‚                                                     â”‚
â”‚  F: (x, y, z, Î¸, Ï†) â†’ (c, Ïƒ)                        â”‚
â”‚                                                     â”‚
â”‚  Input: 3D position (x,y,z) + view direction (Î¸,Ï†)  â”‚
â”‚  Output: color c = (r,g,b) + density Ïƒ              â”‚
â”‚                                                     â”‚
â”‚  VOLUME RENDERING                                   â”‚
â”‚                                                     â”‚
â”‚  C(r) = âˆ« T(t) Ïƒ(r(t)) c(r(t), d) dt                â”‚
â”‚                                                     â”‚
â”‚  T(t) = exp(-âˆ«â‚€áµ— Ïƒ(r(s)) ds)  (transmittance)       â”‚
â”‚                                                     â”‚
â”‚  Discrete approximation:                            â”‚
â”‚  C = Î£áµ¢ Táµ¢ (1 - exp(-Ïƒáµ¢Î´áµ¢)) cáµ¢                      â”‚
â”‚  Táµ¢ = exp(-Î£â±¼â‚Œâ‚â±â»Â¹ Ïƒâ±¼Î´â±¼)                            â”‚
â”‚  Î´áµ¢ = táµ¢â‚Šâ‚ - táµ¢  (distance between samples)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. 3D Gaussian Splatting

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  REPRESENTATION: Set of 3D Gaussians                â”‚
â”‚                                                     â”‚
â”‚  Each Gaussian:                                     â”‚
â”‚  - Position Î¼ âˆˆ RÂ³                                  â”‚
â”‚  - Covariance Î£ (3Ã—3, for shape/orientation)        â”‚
â”‚  - Color (spherical harmonics for view-dependent)   â”‚
â”‚  - Opacity Î±                                        â”‚
â”‚                                                     â”‚
â”‚  GAUSSIAN FUNCTION:                                 â”‚
â”‚                                                     â”‚
â”‚  G(x) = exp(-Â½(x-Î¼)áµ€ Î£â»Â¹ (x-Î¼))                     â”‚
â”‚                                                     â”‚
â”‚  RENDERING (differentiable rasterization):          â”‚
â”‚                                                     â”‚
â”‚  1. Project 3D Gaussians to 2D                      â”‚
â”‚  2. Sort by depth                                   â”‚
â”‚  3. Alpha-blend front-to-back:                      â”‚
â”‚     C = Î£áµ¢ cáµ¢ Î±áµ¢ Gáµ¢(x) âˆâ±¼<áµ¢ (1 - Î±â±¼Gâ±¼(x))           â”‚
â”‚                                                     â”‚
â”‚  Advantage: Real-time rendering (100+ FPS)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5. Depth Estimation Loss

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  L1/L2 LOSS                                         â”‚
â”‚                                                     â”‚
â”‚  Lâ‚ = (1/n) Î£áµ¢ |dáµ¢ - dÌ‚áµ¢|                           â”‚
â”‚  Lâ‚‚ = (1/n) Î£áµ¢ (dáµ¢ - dÌ‚áµ¢)Â²                          â”‚
â”‚                                                     â”‚
â”‚  SCALE-INVARIANT LOG LOSS                           â”‚
â”‚                                                     â”‚
â”‚  L = (1/n) Î£(log dáµ¢ - log dÌ‚áµ¢)Â²                     â”‚
â”‚    - (1/nÂ²) (Î£(log dáµ¢ - log dÌ‚áµ¢))Â²                  â”‚
â”‚                                                     â”‚
â”‚  Handles scale ambiguity in monocular depth         â”‚
â”‚                                                     â”‚
â”‚  GRADIENT MATCHING LOSS                             â”‚
â”‚                                                     â”‚
â”‚  L_grad = |âˆ‡â‚“d - âˆ‡â‚“dÌ‚| + |âˆ‡áµ§d - âˆ‡áµ§dÌ‚|               â”‚
â”‚                                                     â”‚
â”‚  Encourages smooth depth with sharp edges           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Algorithms

### Algorithm 1: Stereo Matching (SGM)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SEMI-GLOBAL MATCHING                               â”‚
â”‚                                                     â”‚
â”‚  1. COMPUTE COST VOLUME:                            â”‚
â”‚     C(x,y,d) = matching_cost(Iâ‚—(x,y), Iáµ£(x-d,y))    â”‚
â”‚     for all disparities d âˆˆ [0, D_max]              â”‚
â”‚                                                     â”‚
â”‚  2. AGGREGATE ALONG PATHS (8 or 16 directions):     â”‚
â”‚     Láµ£(p,d) = C(p,d) + min(                         â”‚
â”‚       Láµ£(p-r, d),                                   â”‚
â”‚       Láµ£(p-r, dÂ±1) + Pâ‚,                            â”‚
â”‚       min_i Láµ£(p-r, i) + Pâ‚‚                         â”‚
â”‚     )                                               â”‚
â”‚                                                     â”‚
â”‚  3. SUM ALL DIRECTIONS:                             â”‚
â”‚     S(p,d) = Î£áµ£ Láµ£(p,d)                             â”‚
â”‚                                                     â”‚
â”‚  4. WINNER-TAKE-ALL:                                â”‚
â”‚     d*(p) = argmin_d S(p,d)                         â”‚
â”‚                                                     â”‚
â”‚  5. LEFT-RIGHT CONSISTENCY CHECK                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 2: NeRF Training

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Multi-view images with camera poses         â”‚
â”‚  OUTPUT: Trained MLP F_Î¸                            â”‚
â”‚                                                     â”‚
â”‚  FOR each iteration:                                â”‚
â”‚                                                     â”‚
â”‚  1. SAMPLE RAY:                                     â”‚
â”‚     - Pick random image, random pixel               â”‚
â”‚     - Ray r(t) = o + td through pixel               â”‚
â”‚                                                     â”‚
â”‚  2. SAMPLE POINTS ALONG RAY:                        â”‚
â”‚     - Stratified: divide [near,far] into bins       â”‚
â”‚     - Uniform random within each bin                â”‚
â”‚                                                     â”‚
â”‚  3. QUERY NETWORK:                                  â”‚
â”‚     - Positional encoding: Î³(x) = [sin,cos] at      â”‚
â”‚       multiple frequencies                          â”‚
â”‚     - (cáµ¢, Ïƒáµ¢) = F_Î¸(Î³(xáµ¢), Î³(d))                   â”‚
â”‚                                                     â”‚
â”‚  4. VOLUME RENDER:                                  â”‚
â”‚     Äˆ = Î£áµ¢ Táµ¢ (1-exp(-Ïƒáµ¢Î´áµ¢)) cáµ¢                     â”‚
â”‚                                                     â”‚
â”‚  5. LOSS:                                           â”‚
â”‚     L = ||Äˆ - C_gt||Â²                               â”‚
â”‚                                                     â”‚
â”‚  HIERARCHICAL SAMPLING: Coarse + fine networks      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 3: PointNet Classification

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Point cloud P = {xâ‚, ..., xâ‚™}, xáµ¢ âˆˆ RÂ³      â”‚
â”‚  OUTPUT: Class label                                â”‚
â”‚                                                     â”‚
â”‚  1. INPUT TRANSFORM (optional T-Net):               â”‚
â”‚     T = predict_transform(P)  (3Ã—3 matrix)          â”‚
â”‚     P' = P Ã— T                                      â”‚
â”‚                                                     â”‚
â”‚  2. PER-POINT MLP:                                  â”‚
â”‚     háµ¢ = MLP(xáµ¢)  for each point                    â”‚
â”‚     MLP: 64 â†’ 64 â†’ 64 â†’ 128 â†’ 1024                  â”‚
â”‚                                                     â”‚
â”‚  3. SYMMETRIC AGGREGATION:                          â”‚
â”‚     g = MAX_POOL({háµ¢})  (permutation invariant)     â”‚
â”‚                                                     â”‚
â”‚  4. CLASSIFICATION MLP:                             â”‚
â”‚     output = MLP(g)  â†’ K classes                    â”‚
â”‚     MLP: 1024 â†’ 512 â†’ 256 â†’ K                       â”‚
â”‚                                                     â”‚
â”‚  Key: MAX_POOL makes it order-invariant             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

---

## â“ Interview Questions & Answers

<details>
<summary><b>Q1: How does NeRF achieve view synthesis?</b></summary>

**Key ideas:**
1. **Implicit representation:** MLP maps (x,y,z,Î¸,Ï†) â†’ (color, density)
2. **Volume rendering:** Integrate color along rays
3. **Positional encoding:** High-frequency details via sin/cos

**Training:**
- Supervise rendered pixels with ground truth
- Learn continuous 3D representation
- Novel views via querying any camera pose

</details>

<details>
<summary><b>Q2: What makes point clouds challenging?</b></summary>

**Challenges:**
1. **Unordered:** No natural ordering (unlike images)
2. **Irregular:** No grid structure
3. **Variable size:** Different scenes have different point counts
4. **Sparse:** Points don't cover all surfaces

**Solutions:**
- PointNet: Symmetric functions (max-pool)
- Voxelization: Convert to regular grid
- Graph networks: KNN for local structure

</details>

<details>
<summary><b>Q3: Monocular vs stereo depth estimation?</b></summary>

| Monocular | Stereo |
| :--- | :--- |
| Single image | Two cameras |
| Learning-based (CNN) | Geometry-based + learning |
| Scale ambiguous | Metric depth (known baseline) |
| Works everywhere | Needs texture |
| Fails on novel scenes | Generalizes well |

**Monocular cues:** Size, occlusion, texture gradient, linear perspective

</details>

<details>
<summary><b>Q4: How does 3D Gaussian Splatting differ from NeRF?</b></summary>

| NeRF | 3D Gaussian Splatting |
| :--- | :--- |
| Implicit (MLP) | Explicit (point-based) |
| Ray marching | Rasterization |
| Slow render (~30s) | Real-time (100+ FPS) |
| Hard to edit | Easy to edit |
| Memory efficient | More memory |

**Key insight:** Gaussians are differentiable and fast to render

</details>

<details>
<summary><b>Q5: What is the disparity-depth relationship?</b></summary>

**Formula:** Z = f Ã— B / d

- Z: depth
- f: focal length
- B: baseline
- d: disparity

**Key points:**
- Inverse relationship: larger d â†’ smaller Z
- Depth resolution decreases with distance
- Zero disparity = infinite distance

</details>

<details>
<summary><b>Q6: Why does NeRF use positional encoding?</b></summary>

**Problem:** MLPs have spectral bias toward low frequencies

**Solution:** Positional encoding Î³(x) = [sin(2â¿Ï€x), cos(2â¿Ï€x)]â‚™

**Effect:**
- Maps low-dim input to high-dim
- Enables learning high-frequency details
- Without it: blurry reconstructions

</details>

---

## ğŸ“š Key Formulas Reference

| Formula | Description |
| :--- | :--- |
| Z = fB/d | Stereo depth |
| C(r) = âˆ« T(t)Ïƒ(t)c(t)dt | NeRF volume rendering |
| T(t) = exp(-âˆ«Ïƒ(s)ds) | Transmittance |
| f(P) = g(MAX{h(xáµ¢)}) | PointNet |
| Î³(x) = [sin(2â¿Ï€x), cos(2â¿Ï€x)] | Positional encoding |


---

<br/>

<div align="center">

## ğŸ““ PRACTICE

<br/>

### ğŸš€ Click to Open Directly in Google Colab

<br/>

<a href="https://colab.research.google.com/github/USERNAME/computer_vision_complete/blob/main/14_3D_Vision/colab_tutorial.ipynb">
<img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" height="50"/>
</a>

<br/><br/>

> âš ï¸ **First time?** Push this repo to GitHub, then replace `USERNAME` in the link above with your GitHub username.

<br/>

**Or manually:** [ğŸ“¥ Download](./colab_tutorial.ipynb) â†’ [ğŸŒ Colab](https://colab.research.google.com) â†’ Upload

</div>

<br/>




---

<br/>

<div align="center">

| | | |
|:---|:---:|---:|
| **[â—€ Video](../13_Video_Temporal/README.md)** | **[ğŸ  HOME](../README.md)** | **[Generative â–¶](../15_Generative_Vision/README.md)** |

<br/>

---

ğŸŒ™ Part of **[Computer Vision Complete](../README.md)** Â· Made with â¤ï¸

<br/>

</div>
