<div align="center">

<br/>

<a href="../18_Deployment_Systems/README.md"><img src="https://img.shields.io/badge/â—€__Deploy-0f172a?style=for-the-badge&labelColor=1e293b" height="35"/></a>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="../README.md"><img src="https://img.shields.io/badge/ğŸ __HOME-A78BFA?style=for-the-badge&labelColor=0f172a" height="35"/></a>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="../20_Research_Frontiers/README.md"><img src="https://img.shields.io/badge/Research__â–¶-0f172a?style=for-the-badge&labelColor=1e293b" height="35"/></a>

<br/><br/>

---

<br/>

# ğŸ›¡ï¸ ETHICS & SAFETY

### ğŸŒ™ *Responsible AI*

<br/>

<img src="https://img.shields.io/badge/ğŸ“š__MODULE__19/20-A78BFA?style=for-the-badge&labelColor=0f172a" height="40"/>
&nbsp;&nbsp;
<img src="https://img.shields.io/badge/â±ï¸__2_HOURS-FBBF24?style=for-the-badge&labelColor=0f172a" height="40"/>
&nbsp;&nbsp;
<img src="https://img.shields.io/badge/ğŸ““__NOTEBOOK_READY-34D399?style=for-the-badge&labelColor=0f172a" height="40"/>

<br/><br/>

---

</div>

<br/>

## ğŸ¯ Key Concepts

| Topic | Issue | Mitigation |
| :--- | :--- | :--- |
| **Adversarial** | Small perturbations fool models | Adversarial training |
| **Bias** | Unfair performance across groups | Dataset balancing |
| **Privacy** | Models memorize data | Differential privacy |
| **Explainability** | Black-box decisions | Saliency maps, LIME |
| **Robustness** | Distribution shift | Domain adaptation |

---

## ğŸ¨ Visual Overview

<div align="center">
<img src="./svg_figs/adversarial_attacks.svg" alt="Adversarial Attacks" width="100%"/>
</div>

---

## ğŸ”¢ Mathematical Foundations

### 1. Adversarial Examples

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DEFINITION                                         â”‚
â”‚                                                     â”‚
â”‚  x_adv = x + Î´                                      â”‚
â”‚                                                     â”‚
â”‚  Such that:                                         â”‚
â”‚  - ||Î´|| â‰¤ Îµ (imperceptible)                        â”‚
â”‚  - f(x_adv) â‰  f(x) (misclassification)              â”‚
â”‚                                                     â”‚
â”‚  ATTACK TYPES                                       â”‚
â”‚                                                     â”‚
â”‚  Untargeted: f(x_adv) â‰  y_true                      â”‚
â”‚  Targeted:   f(x_adv) = y_target                    â”‚
â”‚  White-box:  Attacker knows model                   â”‚
â”‚  Black-box:  Attacker queries model                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. FGSM (Fast Gradient Sign Method)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ATTACK                                             â”‚
â”‚                                                     â”‚
â”‚  x_adv = x + Îµ Â· sign(âˆ‡â‚“ L(Î¸, x, y))                â”‚
â”‚                                                     â”‚
â”‚  - One step, fast                                   â”‚
â”‚  - Uses gradient direction to maximize loss         â”‚
â”‚  - Îµ controls perturbation magnitude                â”‚
â”‚                                                     â”‚
â”‚  INTUITION                                          â”‚
â”‚                                                     â”‚
â”‚  Move in direction that increases loss most         â”‚
â”‚  (steepest ascent in Lâˆ ball)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. PGD (Projected Gradient Descent)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ITERATIVE ATTACK                                   â”‚
â”‚                                                     â”‚
â”‚  xâ‚€ = x + noise  (random start)                     â”‚
â”‚                                                     â”‚
â”‚  FOR t = 1 to T:                                    â”‚
â”‚    xâ‚œ = xâ‚œâ‚‹â‚ + Î± Â· sign(âˆ‡â‚“ L(Î¸, xâ‚œâ‚‹â‚, y))           â”‚
â”‚    xâ‚œ = Î _BÎµ(x)(xâ‚œ)  (project back to Îµ-ball)       â”‚
â”‚                                                     â”‚
â”‚  Stronger than FGSM but slower                      â”‚
â”‚                                                     â”‚
â”‚  PROJECTION (Lâˆ ball)                               â”‚
â”‚  Î (x') = clip(x', x-Îµ, x+Îµ)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. Adversarial Training

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ROBUST OPTIMIZATION                                â”‚
â”‚                                                     â”‚
â”‚  min_Î¸ E[(x,y)~D] [max_{||Î´||â‰¤Îµ} L(Î¸, x+Î´, y)]      â”‚
â”‚                                                     â”‚
â”‚  Inner max: Find worst-case perturbation (PGD)      â”‚
â”‚  Outer min: Minimize loss on adversarial examples   â”‚
â”‚                                                     â”‚
â”‚  TRAINING LOOP                                      â”‚
â”‚                                                     â”‚
â”‚  1. Sample batch (x, y)                             â”‚
â”‚  2. Generate x_adv using PGD                        â”‚
â”‚  3. Compute loss on x_adv                           â”‚
â”‚  4. Update model                                    â”‚
â”‚                                                     â”‚
â”‚  Trade-off: Clean accuracy vs robust accuracy       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5. Fairness Metrics

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DEMOGRAPHIC PARITY                                 â”‚
â”‚                                                     â”‚
â”‚  P(Å· = 1 | A = 0) = P(Å· = 1 | A = 1)                â”‚
â”‚                                                     â”‚
â”‚  Prediction rate same across groups                 â”‚
â”‚                                                     â”‚
â”‚  EQUALIZED ODDS                                     â”‚
â”‚                                                     â”‚
â”‚  P(Å· = 1 | Y = y, A = 0) = P(Å· = 1 | Y = y, A = 1)  â”‚
â”‚                                                     â”‚
â”‚  TPR and FPR same across groups                     â”‚
â”‚                                                     â”‚
â”‚  CALIBRATION                                        â”‚
â”‚                                                     â”‚
â”‚  P(Y = 1 | Å· = p, A = a) = p  âˆ€a                    â”‚
â”‚                                                     â”‚
â”‚  Probability estimates are accurate per group       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6. Explainability Methods

| Method | Type | Output |
| :--- | :--- | :--- |
| **Gradient** | White-box | âˆ‚f/âˆ‚x (pixel importance) |
| **Grad-CAM** | White-box | Class activation map |
| **LIME** | Black-box | Local linear model |
| **SHAP** | Black-box | Shapley values |
| **Attention** | Model-based | Attention weights |

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GRAD-CAM                                           â”‚
â”‚                                                     â”‚
â”‚  1. Get feature maps Aáµ from last conv layer        â”‚
â”‚  2. Compute gradients: âˆ‚yá¶œ/âˆ‚Aáµ                      â”‚
â”‚  3. Global average pool: Î±â‚–á¶œ = GAP(âˆ‚yá¶œ/âˆ‚Aáµ)         â”‚
â”‚  4. Weighted combination: L = ReLU(Î£â‚– Î±â‚–á¶œ Aáµ)       â”‚
â”‚                                                     â”‚
â”‚  Result: Heatmap of important regions               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7. Differential Privacy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  (Îµ, Î´)-DIFFERENTIAL PRIVACY                        â”‚
â”‚                                                     â”‚
â”‚  For neighboring datasets D, D':                    â”‚
â”‚  P(M(D) âˆˆ S) â‰¤ eáµ‹ P(M(D') âˆˆ S) + Î´                  â”‚
â”‚                                                     â”‚
â”‚  Îµ: Privacy budget (lower = more private)           â”‚
â”‚  Î´: Probability of failure                          â”‚
â”‚                                                     â”‚
â”‚  DP-SGD                                             â”‚
â”‚                                                     â”‚
â”‚  1. Clip gradients: gÌƒ = g / max(1, ||g||/C)        â”‚
â”‚  2. Add noise: gÌƒâ‚™â‚’áµ¢â‚›â‚‘ = gÌƒ + N(0, ÏƒÂ²CÂ²I)           â”‚
â”‚  3. Average and update                              â”‚
â”‚                                                     â”‚
â”‚  Privacy amplification via subsampling              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Algorithms

### Algorithm 1: FGSM Attack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Model f, image x, label y, epsilon Îµ        â”‚
â”‚  OUTPUT: Adversarial image x_adv                    â”‚
â”‚                                                     â”‚
â”‚  1. x.requires_grad = True                          â”‚
â”‚  2. output = f(x)                                   â”‚
â”‚  3. loss = CrossEntropy(output, y)                  â”‚
â”‚  4. loss.backward()                                 â”‚
â”‚  5. perturbation = Îµ Ã— sign(x.grad)                 â”‚
â”‚  6. x_adv = clip(x + perturbation, 0, 1)            â”‚
â”‚  7. RETURN x_adv                                    â”‚
â”‚                                                     â”‚
â”‚  Note: For targeted attack, use -gradient           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 2: PGD Attack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Model f, image x, label y, Îµ, Î±, T          â”‚
â”‚  OUTPUT: Adversarial image x_adv                    â”‚
â”‚                                                     â”‚
â”‚  1. x_adv = x + uniform(-Îµ, Îµ)  (random start)      â”‚
â”‚  2. FOR t = 1 to T:                                 â”‚
â”‚     3. x_adv.requires_grad = True                   â”‚
â”‚     4. loss = CrossEntropy(f(x_adv), y)             â”‚
â”‚     5. loss.backward()                              â”‚
â”‚     6. x_adv = x_adv + Î± Ã— sign(x_adv.grad)         â”‚
â”‚     7. x_adv = clip(x_adv, x-Îµ, x+Îµ)  (project)     â”‚
â”‚     8. x_adv = clip(x_adv, 0, 1)      (valid)       â”‚
â”‚  9. RETURN x_adv                                    â”‚
â”‚                                                     â”‚
â”‚  Typical: T=20, Î±=Îµ/T Ã— 2.5                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 3: Adversarial Training

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Dataset D, model f, epochs, Îµ               â”‚
â”‚  OUTPUT: Robust model f                             â”‚
â”‚                                                     â”‚
â”‚  FOR epoch = 1 to epochs:                           â”‚
â”‚    FOR batch (x, y) in D:                           â”‚
â”‚      1. Generate x_adv = PGD(f, x, y, Îµ)            â”‚
â”‚      2. Compute loss = L(f(x_adv), y)               â”‚
â”‚      3. Optionally add clean loss:                  â”‚
â”‚         loss += Î» Ã— L(f(x), y)                      â”‚
â”‚      4. Backprop and update f                       â”‚
â”‚                                                     â”‚
â”‚  TRADES (improved):                                 â”‚
â”‚  L = CE(f(x), y) + Î² Ã— KL(f(x), f(x_adv))           â”‚
â”‚                                                     â”‚
â”‚  Balances clean and robust accuracy                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 4: LIME (Local Interpretable Model-agnostic Explanations)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Model f, image x, num_samples N             â”‚
â”‚  OUTPUT: Feature importance weights                 â”‚
â”‚                                                     â”‚
â”‚  1. Segment image into superpixels S = {sâ‚,...,sâ‚–}  â”‚
â”‚  2. FOR i = 1 to N:                                 â”‚
â”‚     3. z'áµ¢ = random binary vector (turn off parts)  â”‚
â”‚     4. xáµ¢ = apply z'áµ¢ mask to x (gray out)          â”‚
â”‚     5. yáµ¢ = f(xáµ¢) (model prediction)                â”‚
â”‚     6. wáµ¢ = exp(-d(x, xáµ¢)Â²/ÏƒÂ²) (locality weight)    â”‚
â”‚  3. Fit weighted linear model:                      â”‚
â”‚     g = argmin_g Î£áµ¢ wáµ¢(f(xáµ¢) - g(z'áµ¢))Â²             â”‚
â”‚  4. RETURN coefficients of g as importance          â”‚
â”‚                                                     â”‚
â”‚  Linear model g explains f locally around x         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## â“ Interview Questions & Answers

<details>
<summary><b>Q1: What are adversarial examples and why do they exist?</b></summary>

**Answer:**

**What:** Inputs with small perturbations that cause misclassification

**Why they exist:**
1. **High dimensionality:** Small perturbations in many dimensions can have large effects
2. **Linear nature:** Deep networks are locally linear, vulnerable to gradient direction
3. **Decision boundaries:** Models have near-linear boundaries near training data

**Example:** Adding ||Î´||âˆ â‰¤ 8/255 noise can flip predictions

</details>

<details>
<summary><b>Q2: Difference between FGSM and PGD?</b></summary>

**Answer:**

| Aspect | FGSM | PGD |
| :--- | :--- | :--- |
| Steps | 1 | Multiple (T) |
| Strength | Weaker | Stronger |
| Speed | Fast | Slower |
| Random start | No | Yes |
| Use | Quick test | Adversarial training |

**PGD** is considered the strongest first-order attack

</details>

<details>
<summary><b>Q3: What is the trade-off in adversarial training?</b></summary>

**Answer:**

**Problem:** Adversarial training reduces clean accuracy

**Typical:** ~5-10% drop in clean accuracy for robust models

**Why:**
- Robust features may differ from most predictive features
- Model capacity split between clean and robust performance

**Solutions:**
- Larger models (more capacity)
- TRADES: Explicitly balance clean and robust

</details>

<details>
<summary><b>Q4: What are different types of fairness?</b></summary>

**Answer:**

| Type | Definition | Issue |
| :--- | :--- | :--- |
| **Demographic Parity** | Equal positive rate | May violate if base rates differ |
| **Equalized Odds** | Equal TPR and FPR | Harder to achieve |
| **Predictive Parity** | Equal precision | Can conflict with others |
| **Calibration** | Accurate probabilities per group | May not ensure equal outcomes |

**Impossibility theorem:** Can't satisfy all fairness criteria simultaneously

</details>

<details>
<summary><b>Q5: How does Grad-CAM work?</b></summary>

**Answer:**

**Goal:** Visualize which image regions influence prediction

**Steps:**
1. Forward pass to get feature maps A from last conv layer
2. Compute gradient of class score w.r.t feature maps
3. Global average pool gradients to get importance weights Î±
4. Weighted sum: L = ReLU(Î£ Î±â‚–Aáµ)

**Result:** Coarse localization heatmap

**Limitation:** Low resolution (from conv layer size)

</details>

<details>
<summary><b>Q6: What is differential privacy in ML?</b></summary>

**Answer:**

**Goal:** Limit information leakage about individual training examples

**DP-SGD:**
1. Clip per-sample gradients (bound sensitivity)
2. Add Gaussian noise to gradient
3. Privacy budget Îµ tracks total leakage

**Trade-off:** More noise = more privacy = less accuracy

**Applications:** Medical data, user data training

</details>

<details>
<summary><b>Q7: What are common sources of bias in vision models?</b></summary>

**Answer:**

**Data bias:**
- Imbalanced representation
- Skewed geography/demographics
- Historical biases in labels

**Model bias:**
- Learns shortcuts (spurious correlations)
- Amplifies training biases

**Evaluation bias:**
- Benchmarks not representative
- Single aggregate metrics hide disparities

**Mitigation:** Diverse data, fairness constraints, disaggregated evaluation

</details>

<details>
<summary><b>Q8: LIME vs SHAP for explainability?</b></summary>

**Answer:**

| Aspect | LIME | SHAP |
| :--- | :--- | :--- |
| Basis | Local linear model | Shapley values |
| Consistency | May vary with sampling | Mathematically consistent |
| Speed | Fast | Can be slow |
| Features | Superpixels | Any features |
| Theory | Heuristic | Game theory |

**SHAP** provides theoretical guarantees but LIME is more practical for images

</details>

---

## ğŸ“š Key Formulas Reference

| Formula | Description |
| :--- | :--- |
| x_adv = x + ÎµÂ·sign(âˆ‡L) | FGSM attack |
| xâ‚œ = Î (xâ‚œâ‚‹â‚ + Î±Â·sign(âˆ‡L)) | PGD step |
| L = ReLU(Î£ Î±â‚–Aáµ) | Grad-CAM |
| P(M(D)âˆˆS) â‰¤ eáµ‹P(M(D')âˆˆS)+Î´ | Differential privacy |


---

<br/>

<div align="center">

## ğŸ““ PRACTICE

<br/>

### ğŸš€ Open in Google Colab

<br/>

**Option 1: Direct Link (After pushing to GitHub)**
```
Replace YOUR_USERNAME with your GitHub username:
https://colab.research.google.com/github/YOUR_USERNAME/computer_vision_complete/blob/main/19_Ethics_Safety/colab_tutorial.ipynb
```

**Option 2: Manual Upload (Works Immediately!)**
1. [ğŸ“¥ Download this notebook](./colab_tutorial.ipynb)
2. Go to [Google Colab](https://colab.research.google.com)
3. Click "Upload" â†’ Select the downloaded `.ipynb` file
4. Run all cells!

**Option 3: Open from GitHub (if already pushed)**
- Click the notebook file on GitHub
- Click "Open in Colab" button (if available)
- Or copy the GitHub URL and paste it into Colab's "File â†’ Open notebook â†’ GitHub" option

<br/>

<a href="https://colab.research.google.com/">
<img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" height="50"/>
</a>

</div>

<br/>


---

<br/>

<div align="center">

| | | |
| :--- |:---:|---:|
| **[â—€ Deploy](../18_Deployment_Systems/README.md)** | **[ğŸ  HOME](../README.md)** | **[Research â–¶](../20_Research_Frontiers/README.md)** |

<br/>

---

ğŸŒ™ Part of **[Computer Vision Complete](../README.md)** Â· Made with â¤ï¸

<br/>

</div>
