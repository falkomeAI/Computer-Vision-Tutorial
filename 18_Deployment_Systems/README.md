<div align="center">

<br/>

<a href="../17_Computational_Photography/README.md"><img src="https://img.shields.io/badge/â—€__Photo-0f172a?style=for-the-badge&labelColor=1e293b" height="35"/></a>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="../README.md"><img src="https://img.shields.io/badge/ğŸ __HOME-FBBF24?style=for-the-badge&labelColor=0f172a" height="35"/></a>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="../19_Ethics_Safety/README.md"><img src="https://img.shields.io/badge/Ethics__â–¶-0f172a?style=for-the-badge&labelColor=1e293b" height="35"/></a>

<br/><br/>

---

<br/>

# âš¡ DEPLOYMENT

### ğŸŒ™ *Lab to Production*

<br/>

<img src="https://img.shields.io/badge/ğŸ“š__MODULE__18/20-FBBF24?style=for-the-badge&labelColor=0f172a" height="40"/>
&nbsp;&nbsp;
<img src="https://img.shields.io/badge/â±ï¸__2_HOURS-FBBF24?style=for-the-badge&labelColor=0f172a" height="40"/>
&nbsp;&nbsp;
<img src="https://img.shields.io/badge/ğŸ““__NOTEBOOK_READY-34D399?style=for-the-badge&labelColor=0f172a" height="40"/>

<br/><br/>

---

</div>

<br/>

## ğŸ¯ Key Concepts

| Technique | Size Reduction | Speed | Accuracy |
| :--- | :--- | :--- | :--- |
| **Quantization** | 4Ã— (FP32â†’INT8) | 2-4Ã— | ~1% drop |
| **Pruning** | 2-10Ã— | 1.5-3Ã— | 1-3% drop |
| **Distillation** | Student smaller | Varies | 1-2% drop |
| **Architecture** | Design efficient | Native | Varies |

---

## ğŸ¨ Visual Overview

<div align="center">
<img src="./svg_figs/model_optimization.svg" alt="Model Optimization" width="100%"/>
</div>

---

## ğŸ”¢ Mathematical Foundations

### 1. Quantization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LINEAR QUANTIZATION                                â”‚
â”‚                                                     â”‚
â”‚  Quantize: q = round(x / scale) + zero_point        â”‚
â”‚  Dequantize: x' = (q - zero_point) Ã— scale          â”‚
â”‚                                                     â”‚
â”‚  SYMMETRIC (signed)                                 â”‚
â”‚  scale = max(|x|) / 127                             â”‚
â”‚  zero_point = 0                                     â”‚
â”‚                                                     â”‚
â”‚  ASYMMETRIC (unsigned)                              â”‚
â”‚  scale = (max - min) / 255                          â”‚
â”‚  zero_point = round(-min / scale)                   â”‚
â”‚                                                     â”‚
â”‚  INT8 GEMM:                                         â”‚
â”‚  Y = scale_a Ã— scale_b Ã— (Qâ‚ Ã— Qáµ¦) + bias           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Quantization-Aware Training (QAT)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FAKE QUANTIZATION (differentiable)                 â”‚
â”‚                                                     â”‚
â”‚  Forward: xÌ‚ = dequant(quant(x))                    â”‚
â”‚  Backward: âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚xÌ‚ (straight-through)        â”‚
â”‚                                                     â”‚
â”‚  Simulates quantization during training             â”‚
â”‚  Allows network to adapt to quantization noise      â”‚
â”‚                                                     â”‚
â”‚  POST-TRAINING vs QAT                               â”‚
â”‚  PTQ: Faster, slight accuracy drop                  â”‚
â”‚  QAT: Requires retraining, better accuracy          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. Pruning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  UNSTRUCTURED PRUNING                               â”‚
â”‚                                                     â”‚
â”‚  Remove individual weights: W' = W âŠ™ M              â”‚
â”‚  M[i,j] = 1 if |W[i,j]| > threshold, else 0         â”‚
â”‚                                                     â”‚
â”‚  STRUCTURED PRUNING                                 â”‚
â”‚                                                     â”‚
â”‚  Remove entire filters/channels/layers              â”‚
â”‚  More hardware-friendly                             â”‚
â”‚                                                     â”‚
â”‚  MAGNITUDE PRUNING                                  â”‚
â”‚  Score = |weight|                                   â”‚
â”‚  Prune lowest k% by magnitude                       â”‚
â”‚                                                     â”‚
â”‚  LOTTERY TICKET HYPOTHESIS                          â”‚
â”‚  Sparse subnetworks exist that train well alone     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. Knowledge Distillation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HINTON'S DISTILLATION                              â”‚
â”‚                                                     â”‚
â”‚  L = Î± Ã— L_hard + (1-Î±) Ã— L_soft                    â”‚
â”‚                                                     â”‚
â”‚  L_hard = CE(student, labels)                       â”‚
â”‚  L_soft = KL(softmax(student/T), softmax(teacher/T))â”‚
â”‚                                                     â”‚
â”‚  Temperature T softens distributions                â”‚
â”‚  Higher T â†’ more information from teacher           â”‚
â”‚                                                     â”‚
â”‚  FEATURE DISTILLATION                               â”‚
â”‚                                                     â”‚
â”‚  L_feat = ||f_student - f_teacher||Â²                â”‚
â”‚  Match intermediate feature maps                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5. Efficient Architectures

| Model | Key Innovation | MAdds | Top-1 |
| :--- | :--- | :--- | :--- |
| **MobileNetV1** | Depthwise separable conv | 569M | 70.6% |
| **MobileNetV2** | Inverted residual | 300M | 72.0% |
| **EfficientNet** | Compound scaling | 390M | 77.1% |
| **ShuffleNet** | Channel shuffle | 140M | 69.4% |

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DEPTHWISE SEPARABLE CONVOLUTION                    â”‚
â”‚                                                     â”‚
â”‚  Standard: KÃ—KÃ—Cáµ¢â‚™Ã—Câ‚’áµ¤â‚œ                             â”‚
â”‚                                                     â”‚
â”‚  Depthwise: KÃ—KÃ—1Ã—Cáµ¢â‚™ (spatial)                     â”‚
â”‚  Pointwise: 1Ã—1Ã—Cáµ¢â‚™Ã—Câ‚’áµ¤â‚œ (channel mixing)           â”‚
â”‚                                                     â”‚
â”‚  Reduction: (KÂ² + Câ‚’áµ¤â‚œ) / (KÂ² Ã— Câ‚’áµ¤â‚œ)               â”‚
â”‚  For 3Ã—3, Câ‚’áµ¤â‚œ=256: ~9Ã— fewer params                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6. Mixed Precision Training

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FP16 + FP32 MIXED PRECISION                        â”‚
â”‚                                                     â”‚
â”‚  Forward: FP16 (faster, less memory)                â”‚
â”‚  Backward: FP16                                     â”‚
â”‚  Master weights: FP32 (for updates)                 â”‚
â”‚  Loss scaling: scale loss to avoid underflow        â”‚
â”‚                                                     â”‚
â”‚  LOSS SCALING                                       â”‚
â”‚  scaled_loss = loss Ã— scale_factor                  â”‚
â”‚  Update in FP32, then convert back                  â”‚
â”‚                                                     â”‚
â”‚  Speedup: ~2Ã— on modern GPUs (Tensor Cores)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Algorithms

### Algorithm 1: Post-Training Quantization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Trained FP32 model, calibration data        â”‚
â”‚  OUTPUT: INT8 model                                 â”‚
â”‚                                                     â”‚
â”‚  1. Run calibration data through model              â”‚
â”‚  2. FOR each layer:                                 â”‚
â”‚     3. Collect activation statistics (min, max)     â”‚
â”‚     4. Compute scale = (max - min) / 255            â”‚
â”‚     5. Compute zero_point = round(-min / scale)     â”‚
â”‚  6. Quantize weights:                               â”‚
â”‚     q_w = round(w / scale_w)                        â”‚
â”‚  7. Replace FP32 ops with INT8 ops                  â”‚
â”‚                                                     â”‚
â”‚  Calibration methods:                               â”‚
â”‚  - MinMax: use observed min/max                     â”‚
â”‚  - Histogram: percentile clipping                   â”‚
â”‚  - Entropy: minimize KL divergence                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 2: Iterative Pruning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Trained model, target sparsity s            â”‚
â”‚  OUTPUT: Pruned model                               â”‚
â”‚                                                     â”‚
â”‚  1. Train to convergence                            â”‚
â”‚  2. FOR each pruning step:                          â”‚
â”‚     3. Compute importance scores (magnitude)        â”‚
â”‚     4. Prune lowest p% of weights                   â”‚
â”‚     5. Fine-tune for k epochs                       â”‚
â”‚     6. IF sparsity >= s: break                      â”‚
â”‚                                                     â”‚
â”‚  GRADUAL PRUNING SCHEDULE                           â”‚
â”‚  sâ‚œ = sâ‚“ + (s - sâ‚“)(1 - (t-tâ‚€)/(T-tâ‚€))Â³             â”‚
â”‚                                                     â”‚
â”‚  Start sparse at tâ‚€, reach target at T              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 3: Knowledge Distillation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Teacher model T, student architecture S     â”‚
â”‚  OUTPUT: Trained student                            â”‚
â”‚                                                     â”‚
â”‚  1. Train or load teacher T                         â”‚
â”‚  2. Initialize student S randomly                   â”‚
â”‚  3. FOR each batch (x, y):                          â”‚
â”‚     4. z_t = T(x), z_s = S(x)                       â”‚
â”‚     5. p_t = softmax(z_t / T)                       â”‚
â”‚     6. p_s = softmax(z_s / T)                       â”‚
â”‚     7. L_hard = CE(z_s, y)                          â”‚
â”‚     8. L_soft = KL(p_s, p_t) Ã— TÂ²                   â”‚
â”‚     9. L = Î± Ã— L_hard + (1-Î±) Ã— L_soft              â”‚
â”‚    10. Update S using L                             â”‚
â”‚                                                     â”‚
â”‚  Temperature T typically 2-20                       â”‚
â”‚  Î± typically 0.5-0.9                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## â“ Interview Questions & Answers

<details>
<summary><b>Q1: What is the difference between PTQ and QAT?</b></summary>

**Answer:**

| Aspect | Post-Training (PTQ) | Quantization-Aware (QAT) |
| :--- | :--- | :--- |
| Training | No retraining | Retraining required |
| Time | Fast (minutes) | Slow (hours/days) |
| Accuracy | Lower | Higher |
| Use case | Quick deployment | Production quality |

**QAT** simulates quantization during training, allowing the model to adapt.

</details>

<details>
<summary><b>Q2: Why is structured pruning more practical than unstructured?</b></summary>

**Answer:**

**Unstructured:** Random zeros â†’ need sparse matrix libraries

**Structured:** Remove entire channels/filters â†’ standard dense ops

| Aspect | Unstructured | Structured |
| :--- | :--- | :--- |
| Granularity | Individual weights | Channels, filters |
| Sparsity | Very high (90%+) | Moderate (50-80%) |
| Speedup | Limited (sparse libs) | Direct (smaller matrix) |
| Hardware | Specialized | Standard |

</details>

<details>
<summary><b>Q3: How does knowledge distillation work?</b></summary>

**Answer:**

**Teacher:** Large, accurate model
**Student:** Small, efficient model

**Key insight:** Soft labels (teacher probabilities) contain more information than hard labels

**Temperature:** Higher T â†’ softer distribution â†’ more "dark knowledge"

**Loss:** Î± Ã— Hard_loss + (1-Î±) Ã— KL(student, teacher)

**Why it works:** Student learns class relationships, not just correct answer

</details>

<details>
<summary><b>Q4: Explain depthwise separable convolution.</b></summary>

**Answer:**

**Standard conv:** KÃ—KÃ—Cáµ¢â‚™Ã—Câ‚’áµ¤â‚œ operations

**Depthwise separable:**
1. **Depthwise:** KÃ—K conv per channel (KÃ—KÃ—Cáµ¢â‚™)
2. **Pointwise:** 1Ã—1 conv to mix channels (Cáµ¢â‚™Ã—Câ‚’áµ¤â‚œ)

**Savings:** (KÂ² + Câ‚’áµ¤â‚œ)/(KÂ²Ã—Câ‚’áµ¤â‚œ) â‰ˆ 1/Câ‚’áµ¤â‚œ + 1/KÂ²

For 3Ã—3, 256 channels: ~9Ã— reduction

</details>

<details>
<summary><b>Q5: What is loss scaling in mixed precision?</b></summary>

**Answer:**

**Problem:** FP16 has limited range â†’ small gradients underflow to 0

**Solution:** Scale loss before backward, unscale gradients after

1. loss_scaled = loss Ã— scale (e.g., 1024)
2. Compute gradients in FP16
3. Unscale: grad = grad_fp16 / scale
4. Update in FP32

**Dynamic scaling:** Increase scale until overflow, then reduce

</details>

<details>
<summary><b>Q6: How does TensorRT optimize inference?</b></summary>

**Answer:**

**Optimizations:**
1. **Layer fusion:** Conv+BN+ReLU â†’ single kernel
2. **Precision:** FP16/INT8 with calibration
3. **Kernel auto-tuning:** Select best CUDA kernels
4. **Memory:** Optimize tensor memory layout
5. **Batching:** Dynamic batching for throughput

**Speedup:** Typically 2-10Ã— over PyTorch

</details>

<details>
<summary><b>Q7: What is the lottery ticket hypothesis?</b></summary>

**Answer:**

**Claim:** Dense networks contain sparse subnetworks (winning tickets) that can train to same accuracy alone.

**Finding:** 
- Prune + reinitialize to original weights
- These sparse networks train as well as dense

**Implication:** Dense networks may be overparameterized for training, not just inference

**Limitation:** Finding tickets requires training dense network first

</details>

<details>
<summary><b>Q8: Compare ONNX, TensorRT, and CoreML.</b></summary>

**Answer:**

| Aspect | ONNX | TensorRT | CoreML |
| :--- | :--- | :--- | :--- |
| Purpose | Interchange format | NVIDIA inference | Apple inference |
| Hardware | Generic | NVIDIA GPU | Apple Neural Engine |
| Optimization | Minimal | Heavy | Heavy |
| Platform | Cross-platform | Linux, Windows | macOS, iOS |

**Typical pipeline:** PyTorch â†’ ONNX â†’ TensorRT/CoreML

</details>

---

## ğŸ“š Key Formulas Reference

| Formula | Description |
| :--- | :--- |
| q = round(x/scale) + zp | Quantization |
| x' = (q - zp) Ã— scale | Dequantization |
| W' = W âŠ™ M | Pruning (mask) |
| L = Î±Â·CE + (1-Î±)Â·KL | Distillation loss |
| p_soft = softmax(z/T) | Temperature softmax |


---

<br/>

<div align="center">

## ğŸ““ PRACTICE

### ğŸš€ *Ready to code? Let's get started!*

<br/>

### ğŸš€ Open in Google Colab

<br/>

<p align="center">
  <a href="https://colab.research.google.com/github/falkomeAI/Computer-Vision-Tutorial/blob/main/18_Deployment_Systems/colab_tutorial.ipynb">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" height="60"/>
  </a>
</p>

<br/>

<p align="center">
  <strong>âœ¨ Click the badge above to open this notebook directly in Google Colab!</strong>
</p>

<br/>


</div>

<br/>


---

<br/>

<div align="center">

| | | |
| :--- |:---:|---:|
| **[â—€ Photo](../17_Computational_Photography/README.md)** | **[ğŸ  HOME](../README.md)** | **[Ethics â–¶](../19_Ethics_Safety/README.md)** |

<br/>

---

ğŸŒ™ Part of **[Computer Vision Complete](../README.md)**

<p align="center">
  Made with â¤ï¸ by <a href="https://github.com/falkomeAI">falkomeAI</a>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/â­_Star_this_repo_if_helpful-60A5FA?style=for-the-badge&logo=github&logoColor=white" alt="Star"/>
</p>

<br/>

</div>
