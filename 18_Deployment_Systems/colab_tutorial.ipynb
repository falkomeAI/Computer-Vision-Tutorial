{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \u26a1 Deployment & Optimization\n",
        "\n",
        "**Topics:** Quantization, Pruning, ONNX Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "!pip install torch torchvision -q\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "print('\u2705 Setup complete!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quantization Demo\n",
        "def quantize_tensor(x, bits=8):\n",
        "    \"\"\"Simple uniform quantization\"\"\"\n",
        "    qmin, qmax = 0, 2**bits - 1\n",
        "    x_min, x_max = x.min(), x.max()\n",
        "    scale = (x_max - x_min) / (qmax - qmin)\n",
        "    zero_point = qmin - x_min / scale\n",
        "    \n",
        "    # Quantize\n",
        "    q = torch.round(x / scale + zero_point).clamp(qmin, qmax).to(torch.int8)\n",
        "    \n",
        "    # Dequantize\n",
        "    x_dequant = (q.float() - zero_point) * scale\n",
        "    \n",
        "    return q, x_dequant, scale, zero_point\n",
        "\n",
        "# Demo\n",
        "x = torch.randn(1000) * 2\n",
        "for bits in [8, 4, 2]:\n",
        "    q, dq, s, z = quantize_tensor(x, bits)\n",
        "    error = torch.mean((x - dq)**2).item()\n",
        "    print(f'INT{bits}: MSE={error:.6f}, Scale={s:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pruning Demo\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.fc2(torch.relu(self.fc1(x)))\n",
        "\n",
        "model = SimpleNet()\n",
        "\n",
        "def count_params(model):\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    nonzero = sum((p != 0).sum().item() for p in model.parameters())\n",
        "    return total, nonzero\n",
        "\n",
        "total, nonzero = count_params(model)\n",
        "print(f'Before pruning: {nonzero}/{total} params ({100*nonzero/total:.1f}%)')\n",
        "\n",
        "# Magnitude pruning\n",
        "def prune_by_magnitude(model, ratio=0.5):\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            threshold = torch.quantile(param.abs().flatten(), ratio)\n",
        "            mask = param.abs() > threshold\n",
        "            param.data *= mask\n",
        "\n",
        "prune_by_magnitude(model, 0.5)\n",
        "total, nonzero = count_params(model)\n",
        "print(f'After 50% pruning: {nonzero}/{total} params ({100*nonzero/total:.1f}%)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Knowledge Distillation Concept\n",
        "def distillation_loss(student_logits, teacher_logits, labels, temperature=4.0, alpha=0.5):\n",
        "    \"\"\"Combine soft and hard targets\"\"\"\n",
        "    soft_targets = nn.functional.softmax(teacher_logits / temperature, dim=-1)\n",
        "    soft_loss = nn.functional.kl_div(\n",
        "        nn.functional.log_softmax(student_logits / temperature, dim=-1),\n",
        "        soft_targets,\n",
        "        reduction='batchmean'\n",
        "    ) * (temperature ** 2)\n",
        "    \n",
        "    hard_loss = nn.functional.cross_entropy(student_logits, labels)\n",
        "    \n",
        "    return alpha * soft_loss + (1 - alpha) * hard_loss\n",
        "\n",
        "# Demo\n",
        "batch_size, num_classes = 32, 10\n",
        "teacher_logits = torch.randn(batch_size, num_classes)\n",
        "student_logits = torch.randn(batch_size, num_classes)\n",
        "labels = torch.randint(0, num_classes, (batch_size,))\n",
        "\n",
        "loss = distillation_loss(student_logits, teacher_logits, labels)\n",
        "print(f'Distillation Loss: {loss.item():.4f}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}