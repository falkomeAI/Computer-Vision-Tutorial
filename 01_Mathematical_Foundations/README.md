<div align="center">

<br/>

<img src="https://img.shields.io/badge/â—€__START-1e293b?style=for-the-badge" height="35"/>
&nbsp;&nbsp;&nbsp;&nbsp;
<a href="../README.md"><img src="https://img.shields.io/badge/ğŸ __HOME-60A5FA?style=for-the-badge&labelColor=0f172a" height="35"/></a>
&nbsp;&nbsp;&nbsp;&nbsp;
<a href="../02_Transform_Methods/README.md"><img src="https://img.shields.io/badge/Transform Methods__â–¶-0f172a?style=for-the-badge&labelColor=1e293b" height="35"/></a>

<br/><br/>

---

<br/>

# ğŸ“ MATHEMATICAL FOUNDATIONS

### ğŸŒ™ *The Language of Computer Vision*

<br/>

<img src="https://img.shields.io/badge/ğŸ“š__MODULE__01/20-60A5FA?style=for-the-badge&labelColor=0f172a" height="40"/>
&nbsp;&nbsp;
<img src="https://img.shields.io/badge/â±ï¸__2_HOURS-FBBF24?style=for-the-badge&labelColor=0f172a" height="40"/>
&nbsp;&nbsp;
<img src="https://img.shields.io/badge/ğŸ““__NOTEBOOK_READY-34D399?style=for-the-badge&labelColor=0f172a" height="40"/>

<br/><br/>

---

</div>

<br/>

## ğŸ“– Overview

> **Mathematics is the foundation of everything in computer vision.** This module covers the essential math you needâ€”linear algebra, probability, optimization, and convolutionâ€”explained visually with runnable code.

<br/>

---

## ğŸ¯ What You'll Learn

<table>
<tr>
<td width="50%">

### ğŸ“Š **Linear Algebra**
- Vectors & Matrices
- Matrix Operations
- Eigenvalues & SVD
- Applications in CV

</td>
<td width="50%">

### ğŸ“ˆ **Probability**
- Distributions
- Bayes' Theorem
- Maximum Likelihood
- Softmax & Cross-Entropy

</td>
</tr>
<tr>
<td width="50%">

### âš¡ **Optimization**
- Gradient Descent
- SGD & Adam
- Loss Landscapes
- Convergence

</td>
<td width="50%">

### ğŸ”„ **Convolution**
- 1D & 2D Convolution
- Kernels & Filters
- CNN Foundation
- Fourier Connection

</td>
</tr>
</table>

<br/>

---

## ğŸ“Š Key Concepts

| Concept | Formula | Use in CV |
| :--- | :--- | :--- |
| **Matrix Multiply** | `Y = WX` | Neural network layers |
| **SVD** | `A = UÎ£Váµ€` | Image compression, PCA |
| **Gradient** | `âˆ‡f = [âˆ‚f/âˆ‚xâ‚, ...]` | Backpropagation |
| **Softmax** | `Ïƒ(z)áµ¢ = eá¶»â±/Î£eá¶»Ê²` | Classification output |
| **Convolution** | `(f*g)[n] = Î£f[k]g[n-k]` | Filtering, CNNs |

<br/>

---

## ğŸ”¢ Essential Formulas

<table>
<tr>
<td>

### Matrix Operations

**Eigendecomposition:**
```
A = VÎ›Vâ»Â¹
Av = Î»v
```

**SVD:**
```
A = UÎ£Váµ€
```

</td>
<td>

### Probability

**Bayes' Theorem:**
```
P(A|B) = P(B|A)Â·P(A) / P(B)
```

**Gaussian:**
```
N(x|Î¼,ÏƒÂ²) = exp(-(x-Î¼)Â²/2ÏƒÂ²) / âˆš(2Ï€ÏƒÂ²)
```

</td>
</tr>
<tr>
<td>

### Optimization

**Gradient Descent:**
```
Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î·Â·âˆ‡L(Î¸â‚œ)
```

</td>
<td>

### Cross-Entropy Loss

```
L = -Î£ yáµ¢Â·log(Å·áµ¢)
```

</td>
</tr>
</table>

<br/>

---

## âš™ï¸ Algorithms

### Algorithm 1: Gradient Descent

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Loss function L(Î¸), learning rate Î·         â”‚
â”‚  OUTPUT: Optimal parameters Î¸*                      â”‚
â”‚                                                     â”‚
â”‚  1. Initialize Î¸ randomly                           â”‚
â”‚  2. REPEAT until convergence:                       â”‚
â”‚     a. Compute gradient: g = âˆ‡L(Î¸)                  â”‚
â”‚     b. Update: Î¸ = Î¸ - Î·Â·g                          â”‚
â”‚                                                     â”‚
â”‚  Variants:                                          â”‚
â”‚  - Momentum: v = Î²v + g; Î¸ = Î¸ - Î·Â·v                â”‚
â”‚  - Adam: Uses adaptive learning rates               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 2: SVD Computation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Matrix A (mÃ—n)                              â”‚
â”‚  OUTPUT: U, Î£, V such that A = UÎ£Váµ€                 â”‚
â”‚                                                     â”‚
â”‚  1. Compute Aáµ€A (nÃ—n symmetric)                     â”‚
â”‚  2. Find eigenvalues Î»áµ¢ and eigenvectors váµ¢         â”‚
â”‚  3. Ïƒáµ¢ = âˆšÎ»áµ¢ (singular values)                      â”‚
â”‚  4. V = [vâ‚, vâ‚‚, ..., vâ‚™]                           â”‚
â”‚  5. uáµ¢ = Aváµ¢/Ïƒáµ¢ (left singular vectors)             â”‚
â”‚  6. U = [uâ‚, uâ‚‚, ..., uâ‚˜]                           â”‚
â”‚                                                     â”‚
â”‚  Truncated SVD: Keep top k singular values          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 3: 2D Convolution

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Image I (HÃ—W), Kernel K (khÃ—kw)             â”‚
â”‚  OUTPUT: Convolved image O                          â”‚
â”‚                                                     â”‚
â”‚  FOR each output position (i, j):                   â”‚
â”‚    sum = 0                                          â”‚
â”‚    FOR m = 0 to kh-1:                               â”‚
â”‚      FOR n = 0 to kw-1:                             â”‚
â”‚        sum += I[i+m, j+n] Ã— K[m, n]                 â”‚
â”‚    O[i, j] = sum                                    â”‚
â”‚                                                     â”‚
â”‚  Output size: (H-kh+1) Ã— (W-kw+1)                   â”‚
â”‚  With padding P: (H+2P-kh+1) Ã— (W+2P-kw+1)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

<br/>

---

## ğŸ¨ Visual Diagrams

<div align="center">
<img src="./svg_figs/linear_algebra_overview.svg" alt="Linear Algebra" width="90%"/>
</div>

<br/>

<div align="center">
<img src="./svg_figs/convolution_operation.svg" alt="Convolution" width="90%"/>
</div>

<br/>

---

---

## âš ï¸ Common Pitfalls

| âŒ Pitfall | âœ… Solution |
| --- | --- |
| Numerical overflow in softmax | Subtract max: `softmax(z - max(z))` |
| Matrix inverse fails | Use SVD pseudo-inverse instead |
| Vanishing gradients | Proper initialization + batch norm |
| Wrong axis in NumPy | Always use explicit `axis=` parameter |
| Forgetting to normalize | Normalize features before distance metrics |

<br/>

---

## ğŸ› ï¸ Mini Projects

<details>
<summary><b>Project 1: PCA Image Compression</b></summary>

- Load a grayscale image
- Apply PCA with different numbers of components (10, 50, 100)
- Compare quality vs compression ratio
- Plot reconstruction error vs number of components

</details>

<details>
<summary><b>Project 2: Optimizer Comparison</b></summary>

- Implement GD, SGD, Momentum, and Adam from scratch
- Test on different loss surfaces (bowl, saddle, ravine)
- Visualize convergence paths
- Compare convergence speed

</details>

<details>
<summary><b>Project 3: Convolution from Scratch</b></summary>

- Implement 2D convolution without using libraries
- Apply different kernels (blur, edge detection, sharpen)
- Compare your implementation with OpenCV/NumPy
- Measure performance difference

</details>

<br/>

---

## â“ Interview Q&A

<details>
<summary><b>Q1: What's the difference between eigenvalues and singular values?</b></summary>

| Eigenvalues | Singular Values |
| --- | --- |
| Square matrices only | Any matrix shape |
| Can be negative/complex | Always â‰¥ 0 |
| `Av = Î»v` | `Av = Ïƒu` |

For symmetric positive semi-definite matrices, eigenvalues = singular values.
</details>

<details>
<summary><b>Q2: Why is convolution used in CNNs?</b></summary>

1. **Translation equivariance** - If input shifts, output shifts same amount
2. **Parameter sharing** - Same kernel everywhere â†’ fewer parameters
3. **Local connectivity** - Each output depends on local region
4. **Hierarchical** - Stacking builds receptive field
</details>

<details>
<summary><b>Q3: GD vs SGD?</b></summary>

| Full-batch GD | SGD |
| --- | --- |
| Uses all samples | Uses mini-batch |
| Exact gradient | Noisy estimate |
| Slow updates | Fast updates |
| Can get stuck | Noise helps escape |
</details>

<details>
<summary><b>Q4: What is softmax?</b></summary>

```
softmax(záµ¢) = exp(záµ¢) / Î£â±¼ exp(zâ±¼)
```

- Converts logits â†’ probabilities (sum = 1)
- Used in multi-class classification
- Temperature `T` controls sharpness
</details>

<details>
<summary><b>Q5: How does SVD relate to PCA?</b></summary>

PCA on centered data X:
1. Covariance: `C = Xáµ€X / (n-1)`
2. Eigendecompose C

OR: SVD on X directly: `X = UÎ£Váµ€`
- Principal components = columns of V
- Variance = Î£Â²/(n-1)
</details>

<br/>

---

## ğŸ“š Resources

**Textbooks:**
- *Mathematics for Machine Learning* - Deisenroth (free PDF)
- *Linear Algebra Done Right* - Axler
- *Pattern Recognition and ML* - Bishop (Ch. 1-2)

**Videos:**
- [3Blue1Brown - Linear Algebra](https://www.3blue1brown.com/topics/linear-algebra)
- [StatQuest - Statistics](https://www.youtube.com/c/joshstarmer)

<br/>

---

<br/>

<div align="center">

## ğŸ““ PRACTICE

<br/>

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                                                                               â”ƒ
â”ƒ   ğŸ“¥ Download .ipynb  â†’  ğŸŒ Open colab.google  â†’  ğŸ“¤ Upload  â†’  â–¶ï¸ Run All   â”ƒ
â”ƒ                                                                               â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
```

<br/>

<a href="./colab_tutorial.ipynb"><img src="https://img.shields.io/badge/ğŸ“¥__DOWNLOAD_NOTEBOOK-0f172a?style=for-the-badge&labelColor=1e293b" height="40"/></a>
&nbsp;&nbsp;&nbsp;&nbsp;
<a href="https://colab.research.google.com"><img src="https://img.shields.io/badge/ğŸŒ__OPEN_COLAB-0f172a?style=for-the-badge&labelColor=1e293b" height="40"/></a>

</div>

<br/>



---

<br/>

<div align="center">

| | | |
|:---|:---:|---:|
|  | **[ğŸ  HOME](../README.md)** | **[Transform Methods â–¶](../02_Transform_Methods/README.md)** |

<br/>

---

ğŸŒ™ Part of **[Computer Vision Complete](../README.md)** Â· Made with â¤ï¸

<br/>

</div>
