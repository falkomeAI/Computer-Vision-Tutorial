<div align="center">

# ğŸ“ Mathematical Foundations

### *The Language of Computer Vision*

<br/>

<p>
<img src="https://img.shields.io/badge/Level-Beginner-green?style=for-the-badge" alt="Level"/>
<img src="https://img.shields.io/badge/Time-1_week-blue?style=for-the-badge" alt="Time"/>
</p>

**ğŸ““ [Download Notebook](./colab_tutorial.ipynb) â†’ Upload to Colab â†’ Run!**

</div>

---

[ğŸ  Home](../README.md) Â· [Transform Methods â†’](../02_Transform_Methods/)

---

<br/>

## ğŸ“– Overview

> **Mathematics is the foundation of everything in computer vision.** This module covers the essential math you needâ€”linear algebra, probability, optimization, and convolutionâ€”explained visually with runnable code.

<br/>

---

## ğŸ¯ What You'll Learn

<table>
<tr>
<td width="50%">

### ğŸ“Š **Linear Algebra**
- Vectors & Matrices
- Matrix Operations
- Eigenvalues & SVD
- Applications in CV

</td>
<td width="50%">

### ğŸ“ˆ **Probability**
- Distributions
- Bayes' Theorem
- Maximum Likelihood
- Softmax & Cross-Entropy

</td>
</tr>
<tr>
<td width="50%">

### âš¡ **Optimization**
- Gradient Descent
- SGD & Adam
- Loss Landscapes
- Convergence

</td>
<td width="50%">

### ğŸ”„ **Convolution**
- 1D & 2D Convolution
- Kernels & Filters
- CNN Foundation
- Fourier Connection

</td>
</tr>
</table>

<br/>

---

## ğŸ“Š Key Concepts

| Concept | Formula | Use in CV |
|:--------|:--------|:----------|
| **Matrix Multiply** | `Y = WX` | Neural network layers |
| **SVD** | `A = UÎ£Váµ€` | Image compression, PCA |
| **Gradient** | `âˆ‡f = [âˆ‚f/âˆ‚xâ‚, ...]` | Backpropagation |
| **Softmax** | `Ïƒ(z)áµ¢ = eá¶»â±/Î£eá¶»Ê²` | Classification output |
| **Convolution** | `(f*g)[n] = Î£f[k]g[n-k]` | Filtering, CNNs |

<br/>

---

## ğŸ”¢ Essential Formulas

<table>
<tr>
<td>

### Matrix Operations

**Eigendecomposition:**
```
A = VÎ›Vâ»Â¹
Av = Î»v
```

**SVD:**
```
A = UÎ£Váµ€
```

</td>
<td>

### Probability

**Bayes' Theorem:**
```
P(A|B) = P(B|A)Â·P(A) / P(B)
```

**Gaussian:**
```
N(x|Î¼,ÏƒÂ²) = exp(-(x-Î¼)Â²/2ÏƒÂ²) / âˆš(2Ï€ÏƒÂ²)
```

</td>
</tr>
<tr>
<td>

### Optimization

**Gradient Descent:**
```
Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î·Â·âˆ‡L(Î¸â‚œ)
```

</td>
<td>

### Cross-Entropy Loss

```
L = -Î£ yáµ¢Â·log(Å·áµ¢)
```

</td>
</tr>
</table>

<br/>

---

## âš™ï¸ Algorithms

### Algorithm 1: Gradient Descent

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Loss function L(Î¸), learning rate Î·        â”‚
â”‚  OUTPUT: Optimal parameters Î¸*                     â”‚
â”‚                                                     â”‚
â”‚  1. Initialize Î¸ randomly                          â”‚
â”‚  2. REPEAT until convergence:                      â”‚
â”‚     a. Compute gradient: g = âˆ‡L(Î¸)                â”‚
â”‚     b. Update: Î¸ = Î¸ - Î·Â·g                        â”‚
â”‚                                                     â”‚
â”‚  Variants:                                          â”‚
â”‚  - Momentum: v = Î²v + g; Î¸ = Î¸ - Î·Â·v              â”‚
â”‚  - Adam: Uses adaptive learning rates              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 2: SVD Computation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Matrix A (mÃ—n)                             â”‚
â”‚  OUTPUT: U, Î£, V such that A = UÎ£Váµ€               â”‚
â”‚                                                     â”‚
â”‚  1. Compute Aáµ€A (nÃ—n symmetric)                   â”‚
â”‚  2. Find eigenvalues Î»áµ¢ and eigenvectors váµ¢       â”‚
â”‚  3. Ïƒáµ¢ = âˆšÎ»áµ¢ (singular values)                    â”‚
â”‚  4. V = [vâ‚, vâ‚‚, ..., vâ‚™]                         â”‚
â”‚  5. uáµ¢ = Aváµ¢/Ïƒáµ¢ (left singular vectors)          â”‚
â”‚  6. U = [uâ‚, uâ‚‚, ..., uâ‚˜]                         â”‚
â”‚                                                     â”‚
â”‚  Truncated SVD: Keep top k singular values        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 3: 2D Convolution

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Image I (HÃ—W), Kernel K (khÃ—kw)           â”‚
â”‚  OUTPUT: Convolved image O                         â”‚
â”‚                                                     â”‚
â”‚  FOR each output position (i, j):                  â”‚
â”‚    sum = 0                                         â”‚
â”‚    FOR m = 0 to kh-1:                             â”‚
â”‚      FOR n = 0 to kw-1:                           â”‚
â”‚        sum += I[i+m, j+n] Ã— K[m, n]               â”‚
â”‚    O[i, j] = sum                                  â”‚
â”‚                                                     â”‚
â”‚  Output size: (H-kh+1) Ã— (W-kw+1)                 â”‚
â”‚  With padding P: (H+2P-kh+1) Ã— (W+2P-kw+1)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

<br/>

---

## ğŸ¨ Visual Diagrams

<div align="center">
<img src="./svg_figs/linear_algebra_overview.svg" alt="Linear Algebra" width="90%"/>
</div>

<br/>

<div align="center">
<img src="./svg_figs/convolution_operation.svg" alt="Convolution" width="90%"/>
</div>

<br/>

---

## ğŸ““ Practice

See the Colab notebook for hands-on coding: [`colab_tutorial.ipynb`](./colab_tutorial.ipynb)

<br/>

---

## âš ï¸ Common Pitfalls

| âŒ Pitfall | âœ… Solution |
|-----------|------------|
| Numerical overflow in softmax | Subtract max: `softmax(z - max(z))` |
| Matrix inverse fails | Use SVD pseudo-inverse instead |
| Vanishing gradients | Proper initialization + batch norm |
| Wrong axis in NumPy | Always use explicit `axis=` parameter |
| Forgetting to normalize | Normalize features before distance metrics |

<br/>

---

## ğŸ› ï¸ Mini Projects

<details>
<summary><b>Project 1: PCA Image Compression</b></summary>

- Load a grayscale image
- Apply PCA with different numbers of components (10, 50, 100)
- Compare quality vs compression ratio
- Plot reconstruction error vs number of components

</details>

<details>
<summary><b>Project 2: Optimizer Comparison</b></summary>

- Implement GD, SGD, Momentum, and Adam from scratch
- Test on different loss surfaces (bowl, saddle, ravine)
- Visualize convergence paths
- Compare convergence speed

</details>

<details>
<summary><b>Project 3: Convolution from Scratch</b></summary>

- Implement 2D convolution without using libraries
- Apply different kernels (blur, edge detection, sharpen)
- Compare your implementation with OpenCV/NumPy
- Measure performance difference

</details>

<br/>

---

## â“ Interview Q&A

<details>
<summary><b>Q1: What's the difference between eigenvalues and singular values?</b></summary>

| Eigenvalues | Singular Values |
|-------------|-----------------|
| Square matrices only | Any matrix shape |
| Can be negative/complex | Always â‰¥ 0 |
| `Av = Î»v` | `Av = Ïƒu` |

For symmetric positive semi-definite matrices, eigenvalues = singular values.
</details>

<details>
<summary><b>Q2: Why is convolution used in CNNs?</b></summary>

1. **Translation equivariance** - If input shifts, output shifts same amount
2. **Parameter sharing** - Same kernel everywhere â†’ fewer parameters
3. **Local connectivity** - Each output depends on local region
4. **Hierarchical** - Stacking builds receptive field
</details>

<details>
<summary><b>Q3: GD vs SGD?</b></summary>

| Full-batch GD | SGD |
|---------------|-----|
| Uses all samples | Uses mini-batch |
| Exact gradient | Noisy estimate |
| Slow updates | Fast updates |
| Can get stuck | Noise helps escape |
</details>

<details>
<summary><b>Q4: What is softmax?</b></summary>

```
softmax(záµ¢) = exp(záµ¢) / Î£â±¼ exp(zâ±¼)
```

- Converts logits â†’ probabilities (sum = 1)
- Used in multi-class classification
- Temperature `T` controls sharpness
</details>

<details>
<summary><b>Q5: How does SVD relate to PCA?</b></summary>

PCA on centered data X:
1. Covariance: `C = Xáµ€X / (n-1)`
2. Eigendecompose C

OR: SVD on X directly: `X = UÎ£Váµ€`
- Principal components = columns of V
- Variance = Î£Â²/(n-1)
</details>

<br/>

---

## ğŸ“š Resources

**Textbooks:**
- *Mathematics for Machine Learning* - Deisenroth (free PDF)
- *Linear Algebra Done Right* - Axler
- *Pattern Recognition and ML* - Bishop (Ch. 1-2)

**Videos:**
- [3Blue1Brown - Linear Algebra](https://www.3blue1brown.com/topics/linear-algebra)
- [StatQuest - Statistics](https://www.youtube.com/c/joshstarmer)

<br/>

---

<div align="center">

### Next Up

# [Transform Methods â†’](../02_Transform_Methods/)

*Fourier, Wavelets, DCT & JPEG*

<br/>

[ğŸ  Back to Home](../README.md)

</div>
