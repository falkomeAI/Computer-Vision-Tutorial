<div align="center">

<br/>

<a href="../14_3D_Vision/README.md"><img src="https://img.shields.io/badge/â—€__3D Vision-0f172a?style=for-the-badge&labelColor=1e293b" height="35"/></a>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="../README.md"><img src="https://img.shields.io/badge/ğŸ __HOME-34D399?style=for-the-badge&labelColor=0f172a" height="35"/></a>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="../16_Vision_Language/README.md"><img src="https://img.shields.io/badge/Vision-Language__â–¶-0f172a?style=for-the-badge&labelColor=1e293b" height="35"/></a>

<br/><br/>

---

<br/>

# ğŸ­ GENERATIVE MODELS

### ğŸŒ™ *Creating Images*

<br/>

<img src="https://img.shields.io/badge/ğŸ“š__MODULE__15/20-34D399?style=for-the-badge&labelColor=0f172a" height="40"/>
&nbsp;&nbsp;
<img src="https://img.shields.io/badge/â±ï¸__2_HOURS-FBBF24?style=for-the-badge&labelColor=0f172a" height="40"/>
&nbsp;&nbsp;
<img src="https://img.shields.io/badge/ğŸ““__NOTEBOOK_READY-34D399?style=for-the-badge&labelColor=0f172a" height="40"/>

<br/><br/>

---

</div>

<br/>

## ğŸ¯ Key Concepts

| Model | Key Idea | Training | Sampling |
| :--- | :--- | :--- | :--- |
| **VAE** | Latent space + reconstruction | ELBO maximization | Decode z ~ N(0,I) |
| **GAN** | Adversarial game | Min-max | Decode z ~ N(0,I) |
| **Diffusion** | Iterative denoising | Predict noise | Iterative denoising |
| **Flow** | Invertible transforms | Exact likelihood | Invert transform |

---

## ğŸ¨ Visual Overview

<div align="center">
<img src="./svg_figs/generative_models.svg" alt="Generative Models" width="100%"/>
</div>

---

## ğŸ”¢ Mathematical Foundations

### 1. Variational Autoencoder (VAE)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GENERATIVE MODEL                                   â”‚
â”‚                                                     â”‚
â”‚  p(x) = âˆ« p(x|z) p(z) dz                            â”‚
â”‚                                                     â”‚
â”‚  p(z) = N(0, I)  (prior)                            â”‚
â”‚  p(x|z) = decoder (learned)                         â”‚
â”‚                                                     â”‚
â”‚  INFERENCE MODEL                                    â”‚
â”‚                                                     â”‚
â”‚  q(z|x) = N(Î¼(x), ÏƒÂ²(x))  (encoder output)          â”‚
â”‚                                                     â”‚
â”‚  ELBO (Evidence Lower Bound)                        â”‚
â”‚                                                     â”‚
â”‚  log p(x) â‰¥ E_q[log p(x|z)] - KL(q(z|x) || p(z))    â”‚
â”‚           = reconstruction   - regularization       â”‚
â”‚                                                     â”‚
â”‚  REPARAMETERIZATION TRICK                           â”‚
â”‚                                                     â”‚
â”‚  z = Î¼ + Ïƒ âŠ™ Îµ,  Îµ ~ N(0,I)                         â”‚
â”‚  (enables gradient through sampling)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. GAN (Generative Adversarial Network)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MIN-MAX GAME                                       â”‚
â”‚                                                     â”‚
â”‚  min_G max_D  E_x[log D(x)] + E_z[log(1-D(G(z)))]   â”‚
â”‚                                                     â”‚
â”‚  D: Discriminator (real vs fake)                    â”‚
â”‚  G: Generator (z â†’ image)                           â”‚
â”‚                                                     â”‚
â”‚  ALTERNATIVE LOSSES                                 â”‚
â”‚                                                     â”‚
â”‚  Non-saturating: min_G -E_z[log D(G(z))]            â”‚
â”‚  (Better gradients when D wins)                     â”‚
â”‚                                                     â”‚
â”‚  Wasserstein GAN:                                   â”‚
â”‚  min_G max_D  E_x[D(x)] - E_z[D(G(z))]              â”‚
â”‚  with Lipschitz constraint on D                     â”‚
â”‚                                                     â”‚
â”‚  CONDITIONAL GAN: G(z, c), D(x, c)                  â”‚
â”‚  c = class label or other condition                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. Diffusion Models

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FORWARD PROCESS (add noise)                        â”‚
â”‚                                                     â”‚
â”‚  q(xâ‚œ|xâ‚œâ‚‹â‚) = N(xâ‚œ; âˆš(1-Î²â‚œ)xâ‚œâ‚‹â‚, Î²â‚œI)               â”‚
â”‚                                                     â”‚
â”‚  Closed form:                                       â”‚
â”‚  q(xâ‚œ|xâ‚€) = N(xâ‚œ; âˆšá¾±â‚œ xâ‚€, (1-á¾±â‚œ)I)                  â”‚
â”‚  where á¾±â‚œ = âˆáµ¢â‚Œâ‚áµ— (1-Î²áµ¢)                            â”‚
â”‚                                                     â”‚
â”‚  REVERSE PROCESS (denoise)                          â”‚
â”‚                                                     â”‚
â”‚  p_Î¸(xâ‚œâ‚‹â‚|xâ‚œ) = N(xâ‚œâ‚‹â‚; Î¼_Î¸(xâ‚œ,t), Ïƒâ‚œÂ²I)            â”‚
â”‚                                                     â”‚
â”‚  TRAINING OBJECTIVE (simplified DDPM)               â”‚
â”‚                                                     â”‚
â”‚  L = E_t,xâ‚€,Îµ [||Îµ - Îµ_Î¸(xâ‚œ, t)||Â²]                 â”‚
â”‚                                                     â”‚
â”‚  Network predicts the noise Îµ added at step t       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. Latent Diffusion (Stable Diffusion)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KEY INSIGHT: Diffusion in latent space             â”‚
â”‚                                                     â”‚
â”‚  1. ENCODE: z = E(x)  (pretrained VAE encoder)      â”‚
â”‚  2. DIFFUSE: Apply diffusion to z (smaller!)        â”‚
â”‚  3. DECODE: x = D(z)  (pretrained VAE decoder)      â”‚
â”‚                                                     â”‚
â”‚  CONDITIONING (text-to-image)                       â”‚
â”‚                                                     â”‚
â”‚  Cross-attention in U-Net:                          â”‚
â”‚  Attention(Q, K, V) where:                          â”‚
â”‚  - Q from image features                            â”‚
â”‚  - K, V from text embeddings (CLIP)                 â”‚
â”‚                                                     â”‚
â”‚  CLASSIFIER-FREE GUIDANCE                           â”‚
â”‚                                                     â”‚
â”‚  ÎµÌƒ = Îµ_Î¸(z, âˆ…) + sÂ·(Îµ_Î¸(z, c) - Îµ_Î¸(z, âˆ…))         â”‚
â”‚                                                     â”‚
â”‚  s > 1: Stronger conditioning                       â”‚
â”‚  (interpolate between conditional and unconditional)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5. Score Matching

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCORE FUNCTION                                     â”‚
â”‚                                                     â”‚
â”‚  s(x) = âˆ‡_x log p(x)                                â”‚
â”‚                                                     â”‚
â”‚  SCORE MATCHING OBJECTIVE                           â”‚
â”‚                                                     â”‚
â”‚  L = E_x [Â½||s_Î¸(x) - âˆ‡_x log p(x)||Â²]              â”‚
â”‚                                                     â”‚
â”‚  DENOISING SCORE MATCHING                           â”‚
â”‚                                                     â”‚
â”‚  L = E_x,Îµ [||s_Î¸(x+ÏƒÎµ) - (-Îµ/Ïƒ)||Â²]                â”‚
â”‚                                                     â”‚
â”‚  Connection to diffusion:                           â”‚
â”‚  - Score = direction to denoise                     â”‚
â”‚  - Îµ_Î¸(xâ‚œ,t) âˆ -s_Î¸(xâ‚œ,t)                           â”‚
â”‚                                                     â”‚
â”‚  LANGEVIN DYNAMICS for sampling:                    â”‚
â”‚  x_{i+1} = x_i + (Î·/2)âˆ‡_x log p(x) + âˆšÎ· Îµ           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Algorithms

### Algorithm 1: VAE Training

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FOR each batch x:                                  â”‚
â”‚                                                     â”‚
â”‚  1. ENCODE:                                         â”‚
â”‚     Î¼, log ÏƒÂ² = Encoder(x)                          â”‚
â”‚                                                     â”‚
â”‚  2. REPARAMETERIZE:                                 â”‚
â”‚     Îµ ~ N(0, I)                                     â”‚
â”‚     z = Î¼ + Ïƒ âŠ™ Îµ                                   â”‚
â”‚                                                     â”‚
â”‚  3. DECODE:                                         â”‚
â”‚     xÌ‚ = Decoder(z)                                 â”‚
â”‚                                                     â”‚
â”‚  4. COMPUTE LOSS:                                   â”‚
â”‚     L_recon = ||x - xÌ‚||Â² or BCE(x, xÌ‚)             â”‚
â”‚     L_KL = -Â½ Î£(1 + log ÏƒÂ² - Î¼Â² - ÏƒÂ²)               â”‚
â”‚     L = L_recon + Î²Â·L_KL                            â”‚
â”‚                                                     â”‚
â”‚  5. BACKPROP and update                             â”‚
â”‚                                                     â”‚
â”‚  Î²-VAE: Î² > 1 for disentangled latents              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 2: GAN Training

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FOR each iteration:                                â”‚
â”‚                                                     â”‚
â”‚  1. TRAIN DISCRIMINATOR (k steps):                  â”‚
â”‚     FOR k steps:                                    â”‚
â”‚       Sample real x ~ data                          â”‚
â”‚       Sample fake G(z), z ~ N(0,I)                  â”‚
â”‚       L_D = -[log D(x) + log(1-D(G(z)))]            â”‚
â”‚       Update D                                      â”‚
â”‚                                                     â”‚
â”‚  2. TRAIN GENERATOR (1 step):                       â”‚
â”‚     Sample z ~ N(0,I)                               â”‚
â”‚     L_G = -log D(G(z))   (non-saturating)           â”‚
â”‚     Update G                                        â”‚
â”‚                                                     â”‚
â”‚  TIPS:                                              â”‚
â”‚  - Feature matching: match D features               â”‚
â”‚  - Spectral normalization for D                     â”‚
â”‚  - Progressive growing (StyleGAN)                   â”‚
â”‚  - R1 regularization                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 3: DDPM Sampling

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  START: x_T ~ N(0, I)                               â”‚
â”‚                                                     â”‚
â”‚  FOR t = T, T-1, ..., 1:                            â”‚
â”‚                                                     â”‚
â”‚  1. PREDICT NOISE:                                  â”‚
â”‚     ÎµÌ‚ = Îµ_Î¸(x_t, t)                                â”‚
â”‚                                                     â”‚
â”‚  2. COMPUTE MEAN:                                   â”‚
â”‚     Î¼ = (1/âˆšÎ±â‚œ)(x_t - (Î²â‚œ/âˆš(1-á¾±â‚œ))ÎµÌ‚)               â”‚
â”‚                                                     â”‚
â”‚  3. SAMPLE:                                         â”‚
â”‚     if t > 1: z ~ N(0, I)                           â”‚
â”‚     else: z = 0                                     â”‚
â”‚     x_{t-1} = Î¼ + Ïƒâ‚œz                               â”‚
â”‚                                                     â”‚
â”‚  RETURN x_0                                         â”‚
â”‚                                                     â”‚
â”‚  DDIM (deterministic, faster):                      â”‚
â”‚  Skip steps, use deterministic update               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

---

## â“ Interview Questions & Answers

<details>
<summary><b>Q1: VAE vs GAN - when to use which?</b></summary>

| VAE | GAN |
| :--- | :--- |
| Stable training | Training can be tricky |
| Blurry samples | Sharp samples |
| Meaningful latent | Latent less interpretable |
| Good for compression | Good for generation |
| Explicit likelihood | No likelihood |

**Use VAE:** When you need latent representations
**Use GAN:** When you need high-quality images

</details>

<details>
<summary><b>Q2: Why do diffusion models produce better images than GANs?</b></summary>

**Reasons:**
1. **Stable training:** No adversarial dynamics
2. **Mode coverage:** Don't suffer from mode collapse
3. **Iterative refinement:** Correct errors over many steps
4. **Strong theoretical foundation:** Score matching

**Trade-off:** Slower sampling (many denoising steps)

</details>

<details>
<summary><b>Q3: What is the reparameterization trick?</b></summary>

**Problem:** Can't backprop through random sampling

**Solution:** z = Î¼ + Ïƒ âŠ™ Îµ, where Îµ ~ N(0,I)

**Why it works:**
- Randomness moved to Îµ (external)
- z is deterministic function of Î¼, Ïƒ, Îµ
- Gradients flow through Î¼, Ïƒ

</details>

<details>
<summary><b>Q4: What is classifier-free guidance?</b></summary>

**Idea:** Amplify conditioning without a classifier

**Method:**
1. Train model with both conditional and unconditional (random drop c)
2. At inference: ÎµÌƒ = Îµ(z,âˆ…) + sÂ·(Îµ(z,c) - Îµ(z,âˆ…))

**s > 1:** Stronger adherence to condition (but may reduce diversity)

**Advantage:** No separate classifier needed

</details>

<details>
<summary><b>Q5: Why is latent diffusion more efficient?</b></summary>

**Problem:** Diffusion on pixels is expensive (e.g., 512Ã—512Ã—3)

**Solution:** Diffuse in latent space (e.g., 64Ã—64Ã—4)

**Steps:**
1. Encode x â†’ z (64x smaller)
2. Diffuse z (fast!)
3. Decode z â†’ x

**Result:** Same quality, 10x faster training

</details>

<details>
<summary><b>Q6: What is mode collapse in GANs?</b></summary>

**Problem:** Generator produces only a few modes (ignores diversity)

**Why:** G finds "safe" outputs that fool D

**Solutions:**
1. **Minibatch discrimination:** D sees batches
2. **Feature matching:** Match statistics, not just fool D
3. **Unrolled GAN:** G anticipates D updates
4. **Wasserstein GAN:** Better loss landscape

</details>

---

## ğŸ“š Key Formulas Reference

| Formula | Description |
| :--- | :--- |
| ELBO = E[log p(x\|z)] - KL(q\|\|p) | VAE objective |
| min_G max_D E[log D] + E[log(1-D(G))] | GAN objective |
| L = E[\|\|Îµ - Îµ_Î¸(xâ‚œ,t)\|\|Â²] | DDPM loss |
| z = Î¼ + Ïƒ âŠ™ Îµ | Reparameterization |
| q(xâ‚œ\|xâ‚€) = N(âˆšá¾±â‚œxâ‚€, (1-á¾±â‚œ)I) | Forward diffusion |


---

<br/>

<div align="center">

## ğŸ““ PRACTICE

<br/>

### ğŸš€ Open in Google Colab

<br/>

**Option 1: Direct Link (After pushing to GitHub)**
```
Replace YOUR_USERNAME with your GitHub username:
https://colab.research.google.com/github/YOUR_USERNAME/computer_vision_complete/blob/main/15_Generative_Vision/colab_tutorial.ipynb
```

**Option 2: Manual Upload (Works Immediately!)**
1. [ğŸ“¥ Download this notebook](./colab_tutorial.ipynb)
2. Go to [Google Colab](https://colab.research.google.com)
3. Click "Upload" â†’ Select the downloaded `.ipynb` file
4. Run all cells!

**Option 3: Open from GitHub (if already pushed)**
- Click the notebook file on GitHub
- Click "Open in Colab" button (if available)
- Or copy the GitHub URL and paste it into Colab's "File â†’ Open notebook â†’ GitHub" option

<br/>

<a href="https://colab.research.google.com/">
<img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" height="50"/>
</a>

</div>

<br/>


---

<br/>

<div align="center">

| | | |
| :--- |:---:|---:|
| **[â—€ 3D Vision](../14_3D_Vision/README.md)** | **[ğŸ  HOME](../README.md)** | **[Vision-Language â–¶](../16_Vision_Language/README.md)** |

<br/>

---

ğŸŒ™ Part of **[Computer Vision Complete](../README.md)** Â· Made with â¤ï¸

<br/>

</div>
