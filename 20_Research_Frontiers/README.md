<div align="center">

<br/>

<a href="../19_Ethics_Safety/README.md"><img src="https://img.shields.io/badge/â—€__Ethics-0f172a?style=for-the-badge&labelColor=1e293b" height="35"/></a>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="../README.md"><img src="https://img.shields.io/badge/ğŸ __HOME-A78BFA?style=for-the-badge&labelColor=0f172a" height="35"/></a>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<img src="https://img.shields.io/badge/âœ“__COMPLETE-34D399?style=for-the-badge&labelColor=0f172a" height="35"/>

<br/><br/>

---

<br/>

# ğŸ”¬ RESEARCH FRONTIERS

### ğŸŒ™ *The Cutting Edge*

<br/>

<img src="https://img.shields.io/badge/ğŸ“š__MODULE__20/20-A78BFA?style=for-the-badge&labelColor=0f172a" height="40"/>
&nbsp;&nbsp;
<img src="https://img.shields.io/badge/â±ï¸__2_HOURS-FBBF24?style=for-the-badge&labelColor=0f172a" height="40"/>
&nbsp;&nbsp;
<img src="https://img.shields.io/badge/ğŸ““__NOTEBOOK_READY-34D399?style=for-the-badge&labelColor=0f172a" height="40"/>

<br/><br/>

---

</div>

<br/>

## ğŸ¯ Key Concepts

| Topic | Description | Examples |
| :--- | :--- | :--- |
| **Foundation Models** | Large-scale pretrained models | SAM, CLIP, DINOv2 |
| **Zero-Shot** | Generalize without task-specific training | CLIP, GPT-4V |
| **Few-Shot** | Learn from few examples | Prototypical Networks |
| **World Models** | Learn environment dynamics | Dreamer, JEPA |
| **Neuro-Symbolic** | Combine neural + symbolic AI | CLEVR, VQA |

---

## ğŸ¨ Visual Overview

<div align="center">
<img src="./svg_figs/foundation_models.svg" alt="Foundation Models" width="100%"/>
</div>

---

## ğŸ”¢ Mathematical Foundations

### 1. Foundation Models

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DEFINITION                                         â”‚
â”‚                                                     â”‚
â”‚  Large models trained on broad data that can be     â”‚
â”‚  adapted to many downstream tasks                   â”‚
â”‚                                                     â”‚
â”‚  KEY PROPERTIES                                     â”‚
â”‚                                                     â”‚
â”‚  1. Scale: Billions of parameters                   â”‚
â”‚  2. Generality: Works across tasks                  â”‚
â”‚  3. Emergence: Capabilities emerge at scale         â”‚
â”‚  4. Transfer: Pretrain once, adapt many             â”‚
â”‚                                                     â”‚
â”‚  VISION FOUNDATION MODELS                           â”‚
â”‚                                                     â”‚
â”‚  - SAM: Segment Anything (promptable segmentation)  â”‚
â”‚  - DINOv2: Self-supervised features                 â”‚
â”‚  - CLIP: Vision-language alignment                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Zero-Shot Learning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DEFINITION                                         â”‚
â”‚                                                     â”‚
â”‚  Classify classes never seen during training        â”‚
â”‚                                                     â”‚
â”‚  ATTRIBUTE-BASED                                    â”‚
â”‚                                                     â”‚
â”‚  f(x) = argmax_c sim(Ï†(x), a_c)                     â”‚
â”‚                                                     â”‚
â”‚  Where a_c = attribute vector of class c            â”‚
â”‚  (e.g., "has stripes", "four legs")                 â”‚
â”‚                                                     â”‚
â”‚  EMBEDDING-BASED (CLIP)                             â”‚
â”‚                                                     â”‚
â”‚  f(x) = argmax_c sim(f_img(x), f_txt("a {c}"))      â”‚
â”‚                                                     â”‚
â”‚  Use text descriptions as class definitions         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. Few-Shot Learning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  N-WAY K-SHOT CLASSIFICATION                        â”‚
â”‚                                                     â”‚
â”‚  Support set: K examples per N classes              â”‚
â”‚  Query set: Samples to classify                     â”‚
â”‚                                                     â”‚
â”‚  PROTOTYPICAL NETWORKS                              â”‚
â”‚                                                     â”‚
â”‚  1. Compute prototype per class:                    â”‚
â”‚     c_n = (1/K) Î£â‚– f_Î¸(x_n,k)                       â”‚
â”‚                                                     â”‚
â”‚  2. Classify query by nearest prototype:            â”‚
â”‚     p(y=n|x) âˆ exp(-d(f_Î¸(x), c_n))                 â”‚
â”‚                                                     â”‚
â”‚  MAML (Model-Agnostic Meta-Learning)                â”‚
â”‚                                                     â”‚
â”‚  Î¸* = Î¸ - Î±âˆ‡Î¸ L(D_support)                          â”‚
â”‚  Meta-update: Î¸ â† Î¸ - Î²âˆ‡Î¸ L(D_query; Î¸*)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. Segment Anything (SAM)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PROMPTABLE SEGMENTATION                            â”‚
â”‚                                                     â”‚
â”‚  Input: Image I + Prompt P (point, box, text)       â”‚
â”‚  Output: Segmentation mask M                        â”‚
â”‚                                                     â”‚
â”‚  ARCHITECTURE                                       â”‚
â”‚                                                     â”‚
â”‚  1. Image Encoder (ViT-H): I â†’ features F           â”‚
â”‚  2. Prompt Encoder: P â†’ prompt embedding            â”‚
â”‚  3. Mask Decoder: F + prompt â†’ M                    â”‚
â”‚                                                     â”‚
â”‚  TRAINING (SA-1B dataset)                           â”‚
â”‚                                                     â”‚
â”‚  - 11M images, 1B+ masks                            â”‚
â”‚  - Interactive annotation with model in the loop    â”‚
â”‚  - Focal loss + dice loss for masks                 â”‚
â”‚                                                     â”‚
â”‚  Loss = Î»_focal Ã— L_focal + Î»_dice Ã— L_dice         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5. World Models

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LEARN ENVIRONMENT DYNAMICS                         â”‚
â”‚                                                     â”‚
â”‚  Components:                                        â”‚
â”‚  1. Encoder: o_t â†’ z_t (observation to latent)      â”‚
â”‚  2. Dynamics: z_t, a_t â†’ z_{t+1} (prediction)       â”‚
â”‚  3. Decoder: z_t â†’ Ã´_t (reconstruction)             â”‚
â”‚                                                     â”‚
â”‚  DREAMER                                            â”‚
â”‚                                                     â”‚
â”‚  Learn in imagination:                              â”‚
â”‚  - Train world model from real experience           â”‚
â”‚  - Train policy in imagined rollouts                â”‚
â”‚                                                     â”‚
â”‚  JEPA (Joint Embedding Predictive Architecture)     â”‚
â”‚                                                     â”‚
â”‚  Predict in embedding space, not pixel space        â”‚
â”‚  z_{y} = predictor(z_x, Î”)                          â”‚
â”‚  Loss = ||z_y - z_{y_true}||Â²                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6. Continual Learning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PROBLEM: Catastrophic Forgetting                   â”‚
â”‚                                                     â”‚
â”‚  Learning task B hurts performance on task A        â”‚
â”‚                                                     â”‚
â”‚  APPROACHES                                         â”‚
â”‚                                                     â”‚
â”‚  1. REPLAY: Store/generate old examples             â”‚
â”‚     L = L_new + L_replay                            â”‚
â”‚                                                     â”‚
â”‚  2. REGULARIZATION (EWC):                           â”‚
â”‚     L = L_new + Î»Î£áµ¢ Fáµ¢(Î¸áµ¢ - Î¸*áµ¢)Â²                   â”‚
â”‚     Fáµ¢ = Fisher information (importance)            â”‚
â”‚                                                     â”‚
â”‚  3. ARCHITECTURE:                                   â”‚
â”‚     Add task-specific modules                       â”‚
â”‚     Freeze old, add new                             â”‚
â”‚                                                     â”‚
â”‚  4. PARAMETER ISOLATION:                            â”‚
â”‚     Different subsets for different tasks           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7. Neuro-Symbolic AI

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  COMBINE NEURAL + SYMBOLIC                          â”‚
â”‚                                                     â”‚
â”‚  Neural: Pattern recognition, learning              â”‚
â”‚  Symbolic: Reasoning, compositionality              â”‚
â”‚                                                     â”‚
â”‚  NEURAL SCENE REPRESENTATIONS                       â”‚
â”‚                                                     â”‚
â”‚  Image â†’ Object detector â†’ Scene graph              â”‚
â”‚  Scene graph + Question â†’ Reasoning â†’ Answer        â”‚
â”‚                                                     â”‚
â”‚  PROGRAM SYNTHESIS                                  â”‚
â”‚                                                     â”‚
â”‚  Learn to generate programs from data               â”‚
â”‚  Neural network outputs symbolic program            â”‚
â”‚                                                     â”‚
â”‚  DIFFERENTIABLE REASONING                           â”‚
â”‚                                                     â”‚
â”‚  Soft logic: âˆ§ = min, âˆ¨ = max, Â¬ = 1-x              â”‚
â”‚  End-to-end trainable reasoning                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Algorithms

### Algorithm 1: Prototypical Networks (Few-Shot)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Support set S, Query set Q                  â”‚
â”‚  OUTPUT: Class predictions for Q                    â”‚
â”‚                                                     â”‚
â”‚  1. COMPUTE PROTOTYPES (per class c):               â”‚
â”‚     p_c = (1/|S_c|) Î£ f(x)  for x in S_c            â”‚
â”‚     f = embedding network                           â”‚
â”‚                                                     â”‚
â”‚  2. CLASSIFY QUERIES:                               â”‚
â”‚     FOR each query q:                               â”‚
â”‚       d_c = ||f(q) - p_c||Â²  (distance to proto)    â”‚
â”‚       P(y=c|q) = softmax(-d_c)                      â”‚
â”‚                                                     â”‚
â”‚  3. TRAIN with episodic learning                    â”‚
â”‚     Sample N-way K-shot episodes                    â”‚
â”‚     Minimize cross-entropy on queries               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 2: SAM (Segment Anything)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Image, prompts (points/boxes/text)          â”‚
â”‚  OUTPUT: Segmentation masks                         â”‚
â”‚                                                     â”‚
â”‚  1. IMAGE ENCODER (ViT-H):                          â”‚
â”‚     image_embedding = MAE_encoder(image)            â”‚
â”‚     Run once per image                              â”‚
â”‚                                                     â”‚
â”‚  2. PROMPT ENCODER:                                 â”‚
â”‚     prompt_embedding = encode(points/boxes/text)    â”‚
â”‚                                                     â”‚
â”‚  3. MASK DECODER (lightweight):                     â”‚
â”‚     masks = decode(image_emb, prompt_emb)           â”‚
â”‚     Output multiple mask candidates                 â”‚
â”‚                                                     â”‚
â”‚  Key: Promptable - any type of prompt works         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Algorithm 3: CLIP Zero-Shot Classification

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Image, class names                          â”‚
â”‚  OUTPUT: Class prediction                           â”‚
â”‚                                                     â”‚
â”‚  1. ENCODE IMAGE:                                   â”‚
â”‚     z_img = ImageEncoder(image)                     â”‚
â”‚     z_img = z_img / ||z_img||  (normalize)          â”‚
â”‚                                                     â”‚
â”‚  2. ENCODE TEXT (for each class):                   â”‚
â”‚     text_c = "a photo of a {class_name}"            â”‚
â”‚     z_text_c = TextEncoder(text_c)                  â”‚
â”‚     z_text_c = z_text_c / ||z_text_c||              â”‚
â”‚                                                     â”‚
â”‚  3. COMPUTE SIMILARITY:                             â”‚
â”‚     sim_c = z_img Â· z_text_c  (dot product)         â”‚
â”‚     probs = softmax(sim / temperature)              â”‚
â”‚                                                     â”‚
â”‚  No training on target classes needed!              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸŒŸ Emerging Paradigms

### 1. In-Context Learning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LEARN FROM EXAMPLES IN PROMPT                      â”‚
â”‚                                                     â”‚
â”‚  Prompt: [Example 1] [Example 2] ... [Query]        â”‚
â”‚                                                     â”‚
â”‚  No gradient updates, just conditioning             â”‚
â”‚  Emergent ability at scale                          â”‚
â”‚                                                     â”‚
â”‚  Vision: GPT-4V, Gemini with image examples         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Multimodal Reasoning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BEYOND PERCEPTION                                  â”‚
â”‚                                                     â”‚
â”‚  - Chain-of-thought for visual reasoning            â”‚
â”‚  - Tool use (code, search, calculators)             â”‚
â”‚  - Multi-step problem solving                       â”‚
â”‚                                                     â”‚
â”‚  Examples: GPT-4V math, diagram understanding       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. 3D Generation from Text/Images

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TEXT/IMAGE â†’ 3D                                    â”‚
â”‚                                                     â”‚
â”‚  - Score Distillation: Use 2D diffusion for 3D      â”‚
â”‚  - Multi-view generation + reconstruction           â”‚
â”‚  - NeRF/3D Gaussian from single image               â”‚
â”‚                                                     â”‚
â”‚  Methods: DreamFusion, Zero123, Magic3D             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## â“ Interview Questions & Answers

<details>
<summary><b>Q1: What makes a model a "foundation model"?</b></summary>

**Answer:**

**Characteristics:**
1. **Scale:** Very large (billions of parameters)
2. **Broad data:** Trained on diverse, large-scale data
3. **Generality:** Applicable to many downstream tasks
4. **Emergence:** Capabilities emerge that weren't explicitly trained

**Examples in Vision:**
- SAM: Promptable segmentation on any image
- DINOv2: General visual features
- CLIP: Vision-language alignment

**Key shift:** From task-specific to general-purpose

</details>

<details>
<summary><b>Q2: How does zero-shot learning differ from few-shot?</b></summary>

**Answer:**

| Aspect | Zero-Shot | Few-Shot |
| :--- | :--- | :--- |
| Training examples | 0 | 1-10 per class |
| Class definition | Attributes/text | Example images |
| Generalization | Semantic transfer | Example matching |
| Example | CLIP text prompts | Prototypical networks |

**Zero-shot** requires semantic knowledge of new classes
**Few-shot** adapts from small support set

</details>

<details>
<summary><b>Q3: What is SAM and why is it significant?</b></summary>

**Answer:**

**Segment Anything Model:**
- Promptable: Point, box, or text input
- Zero-shot: Works on any image without fine-tuning
- Dataset: SA-1B (1B+ masks, 11M images)

**Significance:**
1. Foundation model for segmentation
2. Enables interactive segmentation
3. Works across domains (medical, satellite, etc.)
4. Promptable interface â†’ flexible applications

</details>

<details>
<summary><b>Q4: Explain the idea of world models.</b></summary>

**Answer:**

**Concept:** Learn a model of environment dynamics

**Components:**
- Encoder: Observation â†’ latent state
- Dynamics: Predict next state from action
- (Optional) Decoder: Latent â†’ observation

**Uses:**
1. **Planning:** Simulate before acting
2. **Sample efficiency:** Learn in imagination
3. **Robustness:** Handle distribution shift

**JEPA innovation:** Predict in latent space (not pixels)

</details>

<details>
<summary><b>Q5: What is catastrophic forgetting?</b></summary>

**Answer:**

**Problem:** Training on new task degrades old task performance

**Why:** Neural networks overwrite old knowledge with new

**Solutions:**

| Method | Approach |
| :--- | :--- |
| Replay | Store/generate old examples |
| Regularization | Protect important weights (EWC) |
| Architecture | Task-specific modules |
| Isolation | Different params per task |

</details>

<details>
<summary><b>Q6: What is neuro-symbolic AI?</b></summary>

**Answer:**

**Combine:**
- Neural: Pattern recognition, learning from data
- Symbolic: Logic, reasoning, compositionality

**Approaches:**
1. Neural perception â†’ symbolic reasoning
2. Differentiable logic (soft constraints)
3. Neural program synthesis

**Advantages:**
- Interpretable reasoning
- Data efficient (structure)
- Compositional generalization

</details>

<details>
<summary><b>Q7: How does in-context learning work?</b></summary>

**Answer:**

**Mechanism:** Condition on examples in prompt, no weight updates

**Example prompt:**
```
Image1: cat â†’ "cat"
Image2: dog â†’ "dog"  
Image3: ? â†’ [model predicts]
```

**Why it works:** Large models learn to pattern match

**Requirements:** Very large model (emergent ability)

**Vision:** GPT-4V can do few-shot classification from examples in prompt

</details>

<details>
<summary><b>Q8: What are current frontiers in vision research?</b></summary>

**Answer:**

1. **Foundation models:** SAM, DINOv2 for general features
2. **Multimodal reasoning:** GPT-4V, Gemini for complex tasks
3. **3D generation:** Text/image to 3D (DreamFusion)
4. **Video understanding:** Long context, temporal reasoning
5. **Embodied AI:** Vision for robotics
6. **Efficient models:** On-device, real-time
7. **Robust/fair:** Distribution shift, fairness

</details>

---

## ğŸ“š Key Formulas Reference

| Formula | Description |
| :--- | :--- |
| c_n = (1/K)Î£f(x_k) | Prototype computation |
| Î¸* = Î¸ - Î±âˆ‡L_support | MAML inner loop |
| L = Î»_focal L_focal + Î»_dice L_dice | SAM loss |
| L = L_new + Î»Î£F(Î¸-Î¸*)Â² | EWC regularization |
| z' = predictor(z, Î”) | JEPA prediction |


---

<br/>

<div align="center">

## ğŸ““ PRACTICE

### ğŸš€ *Ready to code? Let's get started!*

<br/>

### ğŸš€ Open in Google Colab

<br/>

<p align="center">
  <a href="https://colab.research.google.com/github/falkomeAI/computer_vision_complete/blob/main/20_Research_Frontiers/colab_tutorial.ipynb">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" height="60"/>
  </a>
</p>

<br/>

<p align="center">
  <strong>âœ¨ Click the badge above to open this notebook directly in Google Colab!</strong>
</p>

<br/>


</div>

<br/>


---

<br/>

<div align="center">

| | | |
| :--- |:---:|---:|
| **[â—€ Ethics](../19_Ethics_Safety/README.md)** | **[ğŸ  HOME](../README.md)** |  |

<br/>

---

ğŸŒ™ Part of **[Computer Vision Complete](../README.md)**

<p align="center">
  Made with â¤ï¸ by <a href="https://github.com/falkomeAI">falkomeAI</a>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/â­_Star_this_repo_if_helpful-60A5FA?style=for-the-badge&logo=github&logoColor=white" alt="Star"/>
</p>

<br/>

</div>
